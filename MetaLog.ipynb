{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from CONSTANTS import DEVICE, LOG_ROOT, PROJECT_ROOT, SESSION\n",
    "from models.gru import AttGRUModel\n",
    "from module.Common import batch_variable_inst, data_iter, generate_tinsts_binary_label\n",
    "from module.Optimizer import Optimizer\n",
    "from preprocessing.AutoLabeling import Probabilistic_Labeling\n",
    "from preprocessing.datacutter.SimpleCutting import cut_by\n",
    "from preprocessing.Preprocess import Preprocessor\n",
    "from representations.sequences.statistics import Sequential_TF\n",
    "from representations.templates.statistics import (\n",
    "    Simple_template_TF_IDF,\n",
    "    Template_TF_IDF_without_clean,\n",
    ")\n",
    "from utils.Vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom params\n",
    "lstm_hiddens = 64\n",
    "num_layer = 4\n",
    "batch_size = 100\n",
    "drop_out = 0.2\n",
    "epochs = 10\n",
    "\n",
    "word2vec_file = \"glove.840B.300d.txt\"\n",
    "dim = 300\n",
    "alpha = 2e-3\n",
    "beta = 2\n",
    "gamma = 2e-3\n",
    "\n",
    "\n",
    "parser = \"IBM\"\n",
    "mode = \"train\"\n",
    "min_cluster_size = 100\n",
    "min_samples = 100\n",
    "reduce_dimension = 50\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function for updating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_network(old, new, lr, load=False):\n",
    "    updated_theta = {}\n",
    "    state_dicts = old.state_dict()\n",
    "    param_dicts = dict(old.named_parameters())\n",
    "\n",
    "    for i, (k, v) in enumerate(state_dicts.items()):\n",
    "        if k in param_dicts.keys() and param_dicts[k].grad is not None:\n",
    "            updated_theta[k] = param_dicts[k] - lr * param_dicts[k].grad\n",
    "        else:\n",
    "            updated_theta[k] = state_dicts[k]\n",
    "    if load:\n",
    "        new.load_state_dict(updated_theta)\n",
    "    else:\n",
    "        new = put_theta(new, updated_theta)\n",
    "    return new\n",
    "\n",
    "\n",
    "def put_theta(model, theta):\n",
    "    def k_param_fn(tmp_model, name=None):\n",
    "        if len(tmp_model._modules) != 0:\n",
    "            for k, v in tmp_model._modules.items():\n",
    "                if name is None:\n",
    "                    k_param_fn(v, name=str(k))\n",
    "                else:\n",
    "                    k_param_fn(v, name=str(name + \".\" + k))\n",
    "        else:\n",
    "            for k, v in tmp_model._parameters.items():\n",
    "                if not isinstance(v, torch.Tensor):\n",
    "                    continue\n",
    "                tmp_model._parameters[k] = theta[str(name + \".\" + k)]\n",
    "\n",
    "    k_param_fn(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MetaLog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLog:\n",
    "    def __init__(self, vocab, num_layer, hidden_size, drop_out, label2id):\n",
    "        self.label2id = label2id\n",
    "        self.vocab = vocab\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1024\n",
    "        self.drop_out = drop_out\n",
    "        self.model = AttGRUModel(vocab, self.num_layer, self.hidden_size, self.drop_out)\n",
    "        self.bk_model = AttGRUModel(\n",
    "            vocab, self.num_layer, self.hidden_size, self.drop_out, is_backup=True\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda(DEVICE)\n",
    "            self.bk_model = self.bk_model.cuda(DEVICE)\n",
    "        elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "            self.model = self.model.to(DEVICE)\n",
    "            self.bk_model = self.bk_model.to(DEVICE)\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        tag_logits = self.model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def bk_forward(self, inputs, targets):\n",
    "        tag_logits = self.bk_model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, threshold=None):\n",
    "        with torch.no_grad():\n",
    "            tag_logits = self.model(inputs)\n",
    "            tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        if threshold is not None:\n",
    "            probs = tag_logits.detach().cpu().numpy()\n",
    "            anomaly_id = self.label2id[\"Anomalous\"]\n",
    "            pred_tags = np.zeros(probs.shape[0])\n",
    "            for i, logits in enumerate(probs):\n",
    "                if logits[anomaly_id] >= threshold:\n",
    "                    pred_tags[i] = anomaly_id\n",
    "                else:\n",
    "                    pred_tags[i] = 1 - anomaly_id\n",
    "\n",
    "        else:\n",
    "            pred_tags = tag_logits.detach().max(1)[1].cpu()\n",
    "        return pred_tags, tag_logits\n",
    "\n",
    "    def evaluate(self, dataset, instances, threshold=0.5):\n",
    "        logger.info(f\"Start evaluating {dataset} by threshold {threshold}\")\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            globalBatchNum = 0\n",
    "            TP, TN, FP, FN = 0, 0, 0, 0\n",
    "            tag_correct, tag_total = 0, 0\n",
    "            for onebatch in data_iter(instances, self.test_batch_size, False):\n",
    "                tinst = generate_tinsts_binary_label(onebatch, vocab_BGL, False)\n",
    "                if torch.cuda.is_available():\n",
    "                    tinst.to_cuda(DEVICE)\n",
    "                elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                    tinst.to_mps(DEVICE)\n",
    "                self.model.eval()\n",
    "                pred_tags, tag_logits = self.predict(tinst.inputs, threshold)\n",
    "                for inst, bmatch in batch_variable_inst(\n",
    "                    onebatch, pred_tags, tag_logits, processor_BGL.id2tag\n",
    "                ):\n",
    "                    tag_total += 1\n",
    "                    if bmatch:\n",
    "                        tag_correct += 1\n",
    "                        if inst.label == \"Normal\":\n",
    "                            TN += 1\n",
    "                        else:\n",
    "                            TP += 1\n",
    "                    else:\n",
    "                        if inst.label == \"Normal\":\n",
    "                            FP += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                globalBatchNum += 1\n",
    "            if TP + FP != 0:\n",
    "                precision = 100 * TP / (TP + FP)\n",
    "                recall = 100 * TP / (TP + FN)\n",
    "                f1_score = 2 * precision * recall / (precision + recall)\n",
    "                # fpr = 100 * FP / (FP + TN)\n",
    "                logger.info(\n",
    "                    f\"{dataset}: F1 score = {f1_score} | Precision = {precision} | Recall = {recall})\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"{dataset}: F1 score = {0} | Precision = {0} | Recall = {0}\"\n",
    "                )\n",
    "                precision, recall, f1_score = 0, 0, 0\n",
    "        return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 05:49:53,055 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:49:53,055 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"MetaLog\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler(sys.stderr)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    "    )\n",
    ")\n",
    "file_handler = logging.FileHandler(os.path.join(LOG_ROOT, \"MetaLog.log\"))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    "    )\n",
    ")\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "logger.info(\n",
    "    f\"Construct logger for MetaLog succeeded, current working directory: {os.getcwd()}, logs will be written in {LOG_ROOT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Import BGL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 709/2196017 [00:42<05:20, 6842.57it/s]IOStream.flush timed out\n",
      "100%|██████████| 2196017/2196017 [02:20<00:00, 15604.01it/s]\n",
      "2024-09-14 05:53:28,112 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:28,112 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:28,130 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/BGL/raw_log_seqs.txt\n",
      "2024-09-14 05:53:28,130 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/BGL/raw_log_seqs.txt\n",
      "100%|██████████| 85577/85577 [00:04<00:00, 20443.93it/s]\n",
      "2024-09-14 05:53:34,822 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Extraction finished successfully.\n",
      "2024-09-14 05:53:34,822 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Extraction finished successfully.\n",
      "2024-09-14 05:53:34,824 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:34,824 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:34,824 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:34,827 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/BGL.ini\n",
      "2024-09-14 05:53:34,827 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/BGL.ini\n",
      "2024-09-14 05:53:34,827 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/BGL.ini\n",
      "2024-09-14 05:53:34,841 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/persistence\n",
      "2024-09-14 05:53:34,841 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/persistence\n",
      "2024-09-14 05:53:34,841 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/persistence\n",
      "2024-09-14 05:53:34,889 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Persistence file found, loading.\n",
      "2024-09-14 05:53:34,889 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Persistence file found, loading.\n",
      "2024-09-14 05:53:34,889 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Persistence file found, loading.\n",
      "2024-09-14 05:53:34,925 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded.\n",
      "2024-09-14 05:53:34,925 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded.\n",
      "2024-09-14 05:53:34,925 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded.\n",
      "2024-09-14 05:53:34,928 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_event_mapping.dict ... True\n",
      "2024-09-14 05:53:34,928 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_event_mapping.dict ... True\n",
      "2024-09-14 05:53:34,929 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_sequences.txt ... True\n",
      "2024-09-14 05:53:34,929 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_sequences.txt ... True\n",
      "2024-09-14 05:53:34,930 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start loading previous parsing results.\n",
      "2024-09-14 05:53:34,930 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start loading previous parsing results.\n",
      "2024-09-14 05:53:36,906 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded 4747963 log sequences and their mappings.\n",
      "2024-09-14 05:53:36,906 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded 4747963 log sequences and their mappings.\n",
      "2024-09-14 05:53:37,937 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded 85577 blocks\n",
      "2024-09-14 05:53:37,937 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded 85577 blocks\n",
      "2024-09-14 05:53:37,939 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Finished in 3.01\n",
      "2024-09-14 05:53:37,939 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Finished in 3.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciod ciodb ciod ciod fefff fefff ciod ciod ciod edram edra regctl scancom miscompare sernum mpgood fffa ccbc bglmaster ciod ciod ebda sernum ciod pgood pgood mpgood mpgood pgood pgood mpgood mpgood ciod ciod softheader ciod ciod fabd abdd fabdd fabda feaa feaa mmcs pgood pgood mpgood mpgood idoproxydb microloader ffffc idoproxydb idomarshalerio *& idoproxydb idomarshalerio x+ y+ z+ x+ y+ z+ x+ y+ z+ x+ y+ z+ x+ y+ z+ x+ softheader pixf softheader pixf softheader pixf mmcs mmcs feaa feaa feaa feaa feaa cdadc cdadc baae afcc feea feea feea eaca pgood pgood mpgood mpgood pgood pgood mpgood mpgood feaa feaa `personality >version bglpersonality version' `void efcd efcd efb softheader prxf pixf ecaf pgood fffc fpscr dbcr dbsr cnvt ciod bgldevices bgldevices \u0000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 05:53:38,368 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/templates.vec\n",
      "2024-09-14 05:53:38,368 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/templates.vec\n",
      "2024-09-14 05:53:38,371 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: All data preparation finished in 3.44\n",
      "2024-09-14 05:53:38,371 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: All data preparation finished in 3.44\n",
      "2024-09-14 05:53:38,373 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start preprocessing dataset BGL by parsing method IBM\n",
      "2024-09-14 05:53:38,376 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start generating instances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciod ciod softheader ciod `vaddr `int "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85577/85577 [00:00<00:00, 186226.03it/s]\n",
      "2024-09-14 05:53:38,873 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev: 4442 Normal, 4115 Anomalous instances.\n",
      "2024-09-14 05:53:38,882 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train: 13311 Normal, 106 Anomalous instances.\n",
      "2024-09-14 05:53:38,883 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test: 31521 Normal, 19826 Anomalous instances.\n",
      "2024-09-14 05:53:39,421 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-14 05:53:39,498 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Embed size: 112 in pre dataset.\n",
      "2024-09-14 05:53:39,499 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-14 05:53:39,617 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Embed size: 384 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 05:53:46,891 - Solitary_HDBSCAN - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for Solitary_HDBSCAN succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:46,891 - Solitary_HDBSCAN - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for Solitary_HDBSCAN succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:46,894 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for Probabilistic labeling succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:46,894 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for Probabilistic labeling succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 05:53:46,896 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Found previous labeled file, will load and continue to accelerate the process.\n",
      "2024-09-14 05:53:46,896 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Found previous labeled file, will load and continue to accelerate the process.\n",
      "2024-09-14 05:53:46,897 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - ERROR: Random state does not match, please check or re-train.\n",
      "2024-09-14 05:53:46,897 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - ERROR: Random state does not match, please check or re-train.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 1.11.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 72\u001b[0m\n\u001b[1;32m     65\u001b[0m normal_ids_BGL \u001b[38;5;241m=\u001b[39m train_normal_BGL[: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_normal_BGL))]\n\u001b[1;32m     66\u001b[0m label_generator_BGL \u001b[38;5;241m=\u001b[39m Probabilistic_Labeling(\n\u001b[1;32m     67\u001b[0m     min_samples\u001b[38;5;241m=\u001b[39mmin_samples,\n\u001b[1;32m     68\u001b[0m     min_clust_size\u001b[38;5;241m=\u001b[39mmin_cluster_size,\n\u001b[1;32m     69\u001b[0m     res_file\u001b[38;5;241m=\u001b[39mprob_label_res_file_BGL,\n\u001b[1;32m     70\u001b[0m     rand_state_file\u001b[38;5;241m=\u001b[39mrand_state_BGL,\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m labeled_train_BGL \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_generator_BGL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_BGL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal_ids_BGL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Load Embeddings\u001b[39;00m\n\u001b[1;32m     75\u001b[0m vocab_BGL \u001b[38;5;241m=\u001b[39m Vocab()\n",
      "File \u001b[0;32m~/research/MetaLog/preprocessing/AutoLabeling.py:74\u001b[0m, in \u001b[0;36mProbabilistic_Labeling.auto_label\u001b[0;34m(self, instances, normal_ids)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom state does not match, please check or re-train.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0m         \u001b[43mexit\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m     labeled_inst \u001b[38;5;241m=\u001b[39m instances\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = \"BGL\"\n",
    "\n",
    "# # Mark results saving directories.\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "prob_label_res_file_BGL = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\"\n",
    "    + dataset\n",
    "    + \"_\"\n",
    "    + parser\n",
    "    + \"/prob_label_res/mcs-\"\n",
    "    + str(min_cluster_size)\n",
    "    + \"_ms-\"\n",
    "    + str(min_samples),\n",
    ")\n",
    "rand_state_BGL = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\" + dataset + \"_\" + parser + \"/prob_label_res/random_state\",\n",
    ")\n",
    "\n",
    "output_model_dir = os.path.join(\n",
    "    save_dir, \"models/MetaLog/\" + dataset + \"_\" + parser + \"/model\"\n",
    ")\n",
    "output_res_dir = os.path.join(\n",
    "    save_dir, \"results/MetaLog/\" + dataset + \"_\" + parser + \"/detect_res\"\n",
    ")\n",
    "\n",
    "# Training, Validating and Testing instances.\n",
    "template_encoder_BGL = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    "    if dataset == \"NC\"\n",
    "    else Simple_template_TF_IDF(word2vec_file)\n",
    ")\n",
    "processor_BGL = Preprocessor()\n",
    "train_BGL, dev_BGL, test_BGL = processor_BGL.process(\n",
    "    dataset=dataset,\n",
    "    parsing=parser,\n",
    "    cut_func=cut_by(0.3, 0.1, 0.01),\n",
    "    template_encoding=template_encoder_BGL.present,\n",
    ")\n",
    "\n",
    "# Log sequence representation.\n",
    "sequential_encoder_BGL = Sequential_TF(processor_BGL.embedding)\n",
    "train_reprs_BGL = sequential_encoder_BGL.present(train_BGL)\n",
    "for index, inst in enumerate(train_BGL):\n",
    "    inst.repr = train_reprs_BGL[index]\n",
    "test_reprs_BGL = sequential_encoder_BGL.present(test_BGL)\n",
    "for index, inst in enumerate(test_BGL):\n",
    "    inst.repr = test_reprs_BGL[index]\n",
    "\n",
    "# Dimension reduction if specified.\n",
    "transformer_BGL = None\n",
    "if reduce_dimension != -1:\n",
    "    start_time = time.time()\n",
    "    print(f\"Start FastICA, target dimension: {reduce_dimension}.\")\n",
    "    transformer_BGL = FastICA(n_components=reduce_dimension)\n",
    "    train_reprs_BGL = transformer_BGL.fit_transform(train_reprs_BGL)\n",
    "    for idx, inst in enumerate(train_BGL):\n",
    "        inst.repr = train_reprs_BGL[idx]\n",
    "    print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "\n",
    "# Probabilistic labeling.\n",
    "# Sample normal instances.\n",
    "train_normal_BGL = [x for x, inst in enumerate(train_BGL) if inst.label == \"Normal\"]\n",
    "normal_ids_BGL = train_normal_BGL[: int(0.5 * len(train_normal_BGL))]\n",
    "label_generator_BGL = Probabilistic_Labeling(\n",
    "    min_samples=min_samples,\n",
    "    min_clust_size=min_cluster_size,\n",
    "    res_file=prob_label_res_file_BGL,\n",
    "    rand_state_file=rand_state_BGL,\n",
    ")\n",
    "labeled_train_BGL = label_generator_BGL.auto_label(train_BGL, normal_ids_BGL)\n",
    "\n",
    "# Load Embeddings\n",
    "vocab_BGL = Vocab()\n",
    "vocab_BGL.load_from_dict(processor_BGL.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Import HDFS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196017/2196017 [01:35<00:00, 22953.79it/s]\n",
      "2024-09-14 00:20:52,237 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 00:20:52,248 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/raw_log_seqs.txt\n",
      "100%|██████████| 575061/575061 [00:03<00:00, 188316.75it/s]\n",
      "2024-09-14 00:20:55,474 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Extraction finished successfully.\n",
      "2024-09-14 00:20:55,695 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 00:20:55,695 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 00:20:55,697 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-14 00:20:55,697 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-14 00:20:55,703 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-14 00:20:55,703 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-14 00:20:55,715 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Persistence file found, loading.\n",
      "2024-09-14 00:20:55,715 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Persistence file found, loading.\n",
      "2024-09-14 00:20:55,726 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded.\n",
      "2024-09-14 00:20:55,726 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded.\n",
      "2024-09-14 00:20:55,728 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_event_mapping.dict ... True\n",
      "2024-09-14 00:20:55,728 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_sequences.txt ... True\n",
      "2024-09-14 00:20:55,729 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start loading previous parsing results.\n",
      "2024-09-14 00:20:59,567 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded 11175629 log sequences and their mappings.\n",
      "2024-09-14 00:21:00,988 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded 575061 blocks\n",
      "2024-09-14 00:21:00,989 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Finished in 5.26\n",
      "2024-09-14 00:21:01,049 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/templates.vec\n",
      "2024-09-14 00:21:01,050 - HDFSLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: All data preparation finished in 5.32\n",
      "2024-09-14 00:21:01,050 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start preprocessing dataset HDFS by parsing method IBM\n",
      "2024-09-14 00:21:01,050 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start generating instances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipandport block* block* ipandport ipandport ipandport ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport ipandport ipandport ipandport ipandport ipandport block* ipandport block* ipandport ipandport ipandport ipandport ipandport ipandport block* "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575061/575061 [00:02<00:00, 265034.24it/s]\n",
      "2024-09-14 00:21:03,355 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev: 55283 Normal, 2223 Anomalous instances.\n",
      "2024-09-14 00:21:03,390 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train: 165811 Normal, 6707 Anomalous instances.\n",
      "2024-09-14 00:21:03,390 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test: 337129 Normal, 7908 Anomalous instances.\n",
      "2024-09-14 00:21:04,672 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-14 00:21:05,007 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Embed size: 43 in pre dataset.\n",
      "2024-09-14 00:21:05,008 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-14 00:21:05,360 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Embed size: 46 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n",
      "Finished at 19.13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = \"HDFS\"\n",
    "# Mark results saving directories.\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "prob_label_res_file_HDFS = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\"\n",
    "    + dataset\n",
    "    + \"_\"\n",
    "    + parser\n",
    "    + \"/prob_label_res/mcs-\"\n",
    "    + str(min_cluster_size)\n",
    "    + \"_ms-\"\n",
    "    + str(min_samples),\n",
    ")\n",
    "rand_state_HDFS = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\" + dataset + \"_\" + parser + \"/prob_label_res/random_state\",\n",
    ")\n",
    "\n",
    "# Training, Validating and Testing instances.\n",
    "template_encoder_HDFS = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    "    if dataset == \"NC\"\n",
    "    else Simple_template_TF_IDF(word2vec_file)\n",
    ")\n",
    "processor_HDFS = Preprocessor()\n",
    "train_HDFS, _, _ = processor_HDFS.process(\n",
    "    dataset=dataset,\n",
    "    parsing=parser,\n",
    "    cut_func=cut_by(0.3, 0.1),\n",
    "    template_encoding=template_encoder_HDFS.present,\n",
    ")\n",
    "\n",
    "# Log sequence representation.\n",
    "sequential_encoder_HDFS = Sequential_TF(processor_HDFS.embedding)\n",
    "train_reprs_HDFS = sequential_encoder_HDFS.present(train_HDFS)\n",
    "for index, inst in enumerate(train_HDFS):\n",
    "    inst.repr = train_reprs_HDFS[index]\n",
    "\n",
    "# Dimension reduction if specified.\n",
    "transformer_HDFS = None\n",
    "if reduce_dimension != -1:\n",
    "    start_time = time.time()\n",
    "    print(f\"Start FastICA, target dimension: {reduce_dimension}.\")\n",
    "    transformer_HDFS = FastICA(n_components=reduce_dimension)\n",
    "    train_reprs_HDFS = transformer_HDFS.fit_transform(train_reprs_HDFS)\n",
    "    for idx, inst in enumerate(train_HDFS):\n",
    "        inst.repr = train_reprs_HDFS[idx]\n",
    "    print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "\n",
    "labeled_train_HDFS = train_HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Aggregate vocab and label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 05:54:00,298 - Vocab - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Total words: 50\n",
      "2024-09-14 05:54:00,301 - Vocab - SESSION_7a123d0cc10c1408932518b309616721 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-14 05:54:00,302 - Vocab - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Total words: 482\n",
      "2024-09-14 05:54:00,303 - Vocab - SESSION_7a123d0cc10c1408932518b309616721 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-14 05:54:00,328 - AttGRU - SESSION_7a123d0cc10c1408932518b309616721 - INFO: ==== Model Parameters ====\n",
      "2024-09-14 05:54:00,329 - AttGRU - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Input Dimension: 300\n",
      "2024-09-14 05:54:00,330 - AttGRU - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Hidden Size: 64\n",
      "2024-09-14 05:54:00,330 - AttGRU - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Num Layers: 4\n",
      "2024-09-14 05:54:00,331 - AttGRU - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dropout: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 128\n",
      "Orthogonal pretrainer loss: 1.06e-01\n",
      "2 128\n",
      "Orthogonal pretrainer loss: 3.08e-03\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "new_embedding = {}\n",
    "for key in processor_BGL.embedding.keys():\n",
    "    new_embedding[key] = processor_BGL.embedding[key]\n",
    "for key in processor_HDFS.embedding.keys():\n",
    "    new_embedding[key + 432] = processor_HDFS.embedding[key]\n",
    "# Load Embeddings\n",
    "vocab_HDFS = Vocab()\n",
    "vocab_HDFS.load_from_dict(processor_HDFS.embedding)\n",
    "vocab.load_from_dict(new_embedding)\n",
    "\n",
    "metalog = MetaLog(\n",
    "    vocab=vocab,\n",
    "    num_layer=num_layer,\n",
    "    hidden_size=lstm_hiddens,\n",
    "    drop_out=drop_out,\n",
    "    label2id=processor_BGL.label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 05:54:02,824 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Custom params: alpha = 0.002 | beta = 2 | gamma = 0.002 | word2vec_file = glove.840B.300d.txt.\n",
      "2024-09-14 05:54:02,824 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Custom params: alpha = 0.002 | beta = 2 | gamma = 0.002 | word2vec_file = glove.840B.300d.txt.\n",
      "2024-09-14 05:54:03,620 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 0 | phase: train | start time: 05:54:03 | learning rate: [0.002].\n",
      "2024-09-14 05:54:03,620 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 0 | phase: train | start time: 05:54:03 | learning rate: [0.002].\n",
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:935: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n",
      "2024-09-14 06:06:40,966 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 500 | Epoch: 0 | Meta-train loss: 0.0029490117449313402 | Meta-test loss: 0.39354342222213745.\n",
      "2024-09-14 06:06:40,966 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 500 | Epoch: 0 | Meta-train loss: 0.0029490117449313402 | Meta-test loss: 0.39354342222213745.\n",
      "2024-09-14 06:19:10,170 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 1000 | Epoch: 0 | Meta-train loss: 0.003614878049120307 | Meta-test loss: 0.3427410125732422.\n",
      "2024-09-14 06:19:10,170 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 1000 | Epoch: 0 | Meta-train loss: 0.003614878049120307 | Meta-test loss: 0.3427410125732422.\n",
      "2024-09-14 06:31:35,293 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 1500 | Epoch: 0 | Meta-train loss: 0.12601201236248016 | Meta-test loss: 0.3190651535987854.\n",
      "2024-09-14 06:31:35,293 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 1500 | Epoch: 0 | Meta-train loss: 0.12601201236248016 | Meta-test loss: 0.3190651535987854.\n",
      "2024-09-14 06:37:09,592 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Train BGL by threshold 0.5\n",
      "2024-09-14 06:37:09,592 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Train BGL by threshold 0.5\n",
      "2024-09-14 06:37:22,233 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train BGL: F1 score = 12.70820481184454 | Precision = 6.798679867986799 | Recall = 97.16981132075472)\n",
      "2024-09-14 06:37:22,233 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train BGL: F1 score = 12.70820481184454 | Precision = 6.798679867986799 | Recall = 97.16981132075472)\n",
      "2024-09-14 06:37:22,234 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Dev BGL by threshold 0.5\n",
      "2024-09-14 06:37:22,234 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Dev BGL by threshold 0.5\n",
      "2024-09-14 06:37:30,795 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev BGL: F1 score = 92.73729907309635 | Precision = 89.65517241379311 | Recall = 96.03888213851762)\n",
      "2024-09-14 06:37:30,795 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev BGL: F1 score = 92.73729907309635 | Precision = 89.65517241379311 | Recall = 96.03888213851762)\n",
      "2024-09-14 06:37:30,796 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Test BGL by threshold 0.5\n",
      "2024-09-14 06:37:30,796 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Test BGL by threshold 0.5\n",
      "2024-09-14 06:38:02,829 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test BGL: F1 score = 84.23361534938655 | Precision = 80.6657125709801 | Recall = 88.13174619186927)\n",
      "2024-09-14 06:38:02,829 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test BGL: F1 score = 84.23361534938655 | Precision = 80.6657125709801 | Recall = 88.13174619186927)\n",
      "2024-09-14 06:38:03,221 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Exceed best F1 score: history = 0, current = 84.23361534938655.\n",
      "2024-09-14 06:38:03,221 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Exceed best F1 score: history = 0, current = 84.23361534938655.\n",
      "2024-09-14 06:38:03,254 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Training epoch 0 finished.\n",
      "2024-09-14 06:38:03,254 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Training epoch 0 finished.\n",
      "2024-09-14 06:38:03,269 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 1 | phase: train | start time: 06:38:03 | learning rate: [0.0015].\n",
      "2024-09-14 06:38:03,269 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 1 | phase: train | start time: 06:38:03 | learning rate: [0.0015].\n",
      "2024-09-14 06:44:37,770 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 2000 | Epoch: 1 | Meta-train loss: 0.0024414295330643654 | Meta-test loss: 0.26928624510765076.\n",
      "2024-09-14 06:44:37,770 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 2000 | Epoch: 1 | Meta-train loss: 0.0024414295330643654 | Meta-test loss: 0.26928624510765076.\n",
      "2024-09-14 06:57:08,011 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 2500 | Epoch: 1 | Meta-train loss: 0.003026246093213558 | Meta-test loss: 0.3392288386821747.\n",
      "2024-09-14 06:57:08,011 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 2500 | Epoch: 1 | Meta-train loss: 0.003026246093213558 | Meta-test loss: 0.3392288386821747.\n",
      "2024-09-14 07:09:03,797 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 3000 | Epoch: 1 | Meta-train loss: 0.025915108621120453 | Meta-test loss: 0.32647642493247986.\n",
      "2024-09-14 07:09:03,797 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 3000 | Epoch: 1 | Meta-train loss: 0.025915108621120453 | Meta-test loss: 0.32647642493247986.\n",
      "2024-09-14 07:19:53,781 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Train BGL by threshold 0.5\n",
      "2024-09-14 07:19:53,781 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Train BGL by threshold 0.5\n",
      "2024-09-14 07:20:06,800 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train BGL: F1 score = 19.788664745437078 | Precision = 11.016042780748663 | Recall = 97.16981132075472)\n",
      "2024-09-14 07:20:06,800 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train BGL: F1 score = 19.788664745437078 | Precision = 11.016042780748663 | Recall = 97.16981132075472)\n",
      "2024-09-14 07:20:06,802 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Dev BGL by threshold 0.5\n",
      "2024-09-14 07:20:06,802 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Dev BGL by threshold 0.5\n",
      "2024-09-14 07:20:15,123 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev BGL: F1 score = 95.18474374255067 | Precision = 93.40350877192982 | Recall = 97.03523693803159)\n",
      "2024-09-14 07:20:15,123 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev BGL: F1 score = 95.18474374255067 | Precision = 93.40350877192982 | Recall = 97.03523693803159)\n",
      "2024-09-14 07:20:15,124 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Test BGL by threshold 0.5\n",
      "2024-09-14 07:20:15,124 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Test BGL by threshold 0.5\n",
      "2024-09-14 07:20:46,193 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test BGL: F1 score = 88.04717623043774 | Precision = 87.98287584991186 | Recall = 88.11157066478361)\n",
      "2024-09-14 07:20:46,193 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test BGL: F1 score = 88.04717623043774 | Precision = 87.98287584991186 | Recall = 88.11157066478361)\n",
      "2024-09-14 07:20:46,195 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Exceed best F1 score: history = 84.23361534938655, current = 88.04717623043774.\n",
      "2024-09-14 07:20:46,195 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Exceed best F1 score: history = 84.23361534938655, current = 88.04717623043774.\n",
      "2024-09-14 07:20:46,233 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Training epoch 1 finished.\n",
      "2024-09-14 07:20:46,233 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Training epoch 1 finished.\n",
      "2024-09-14 07:20:46,251 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 2 | phase: train | start time: 07:20:46 | learning rate: [0.00084375].\n",
      "2024-09-14 07:20:46,251 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 2 | phase: train | start time: 07:20:46 | learning rate: [0.00084375].\n",
      "2024-09-14 07:21:55,036 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 3500 | Epoch: 2 | Meta-train loss: 0.001163623877801001 | Meta-test loss: 0.27328675985336304.\n",
      "2024-09-14 07:21:55,036 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 3500 | Epoch: 2 | Meta-train loss: 0.001163623877801001 | Meta-test loss: 0.27328675985336304.\n",
      "2024-09-14 07:33:51,201 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 4000 | Epoch: 2 | Meta-train loss: 0.0019208341836929321 | Meta-test loss: 0.2668667137622833.\n",
      "2024-09-14 07:33:51,201 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 4000 | Epoch: 2 | Meta-train loss: 0.0019208341836929321 | Meta-test loss: 0.2668667137622833.\n",
      "2024-09-14 07:45:55,214 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 4500 | Epoch: 2 | Meta-train loss: 0.006027411203831434 | Meta-test loss: 0.3117803931236267.\n",
      "2024-09-14 07:45:55,214 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 4500 | Epoch: 2 | Meta-train loss: 0.006027411203831434 | Meta-test loss: 0.3117803931236267.\n",
      "2024-09-14 07:57:59,598 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 5000 | Epoch: 2 | Meta-train loss: 0.0009560710750520229 | Meta-test loss: 0.34296080470085144.\n",
      "2024-09-14 07:57:59,598 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 5000 | Epoch: 2 | Meta-train loss: 0.0009560710750520229 | Meta-test loss: 0.34296080470085144.\n",
      "2024-09-14 08:02:16,341 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Train BGL by threshold 0.5\n",
      "2024-09-14 08:02:16,341 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Train BGL by threshold 0.5\n",
      "2024-09-14 08:02:28,715 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train BGL: F1 score = 18.04255319148936 | Precision = 9.915809167446211 | Recall = 100.0)\n",
      "2024-09-14 08:02:28,715 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train BGL: F1 score = 18.04255319148936 | Precision = 9.915809167446211 | Recall = 100.0)\n",
      "2024-09-14 08:02:28,719 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Dev BGL by threshold 0.5\n",
      "2024-09-14 08:02:28,719 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Dev BGL by threshold 0.5\n",
      "2024-09-14 08:02:36,826 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev BGL: F1 score = 95.44166273027871 | Precision = 92.83252929014473 | Recall = 98.20170109356015)\n",
      "2024-09-14 08:02:36,826 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev BGL: F1 score = 95.44166273027871 | Precision = 92.83252929014473 | Recall = 98.20170109356015)\n",
      "2024-09-14 08:02:36,827 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Test BGL by threshold 0.5\n",
      "2024-09-14 08:02:36,827 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Test BGL by threshold 0.5\n",
      "2024-09-14 08:03:07,209 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test BGL: F1 score = 90.62028752387656 | Precision = 90.31159202484722 | Recall = 90.93110057500252)\n",
      "2024-09-14 08:03:07,209 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test BGL: F1 score = 90.62028752387656 | Precision = 90.31159202484722 | Recall = 90.93110057500252)\n",
      "2024-09-14 08:03:07,212 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Exceed best F1 score: history = 88.04717623043774, current = 90.62028752387656.\n",
      "2024-09-14 08:03:07,212 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Exceed best F1 score: history = 88.04717623043774, current = 90.62028752387656.\n",
      "2024-09-14 08:03:07,232 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Training epoch 2 finished.\n",
      "2024-09-14 08:03:07,232 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Training epoch 2 finished.\n",
      "2024-09-14 08:03:07,244 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 3 | phase: train | start time: 08:03:07 | learning rate: [0.000474609375].\n",
      "2024-09-14 08:03:07,244 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 3 | phase: train | start time: 08:03:07 | learning rate: [0.000474609375].\n",
      "2024-09-14 08:10:52,471 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 5500 | Epoch: 3 | Meta-train loss: 0.003547090105712414 | Meta-test loss: 0.24349907040596008.\n",
      "2024-09-14 08:10:52,471 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 5500 | Epoch: 3 | Meta-train loss: 0.003547090105712414 | Meta-test loss: 0.24349907040596008.\n",
      "2024-09-14 08:22:36,990 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 6000 | Epoch: 3 | Meta-train loss: 0.008813786320388317 | Meta-test loss: 0.2726645767688751.\n",
      "2024-09-14 08:22:36,990 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 6000 | Epoch: 3 | Meta-train loss: 0.008813786320388317 | Meta-test loss: 0.2726645767688751.\n",
      "2024-09-14 08:34:21,635 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 6500 | Epoch: 3 | Meta-train loss: 0.0163801908493042 | Meta-test loss: 0.32238951325416565.\n",
      "2024-09-14 08:34:21,635 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 6500 | Epoch: 3 | Meta-train loss: 0.0163801908493042 | Meta-test loss: 0.32238951325416565.\n",
      "2024-09-14 08:44:07,900 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Train BGL by threshold 0.5\n",
      "2024-09-14 08:44:07,900 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Train BGL by threshold 0.5\n",
      "2024-09-14 08:44:20,220 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train BGL: F1 score = 17.362817362817363 | Precision = 9.506726457399104 | Recall = 100.0)\n",
      "2024-09-14 08:44:20,220 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train BGL: F1 score = 17.362817362817363 | Precision = 9.506726457399104 | Recall = 100.0)\n",
      "2024-09-14 08:44:20,223 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Dev BGL by threshold 0.5\n",
      "2024-09-14 08:44:20,223 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Dev BGL by threshold 0.5\n",
      "2024-09-14 08:44:28,397 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev BGL: F1 score = 95.16072059342989 | Precision = 92.30242119689356 | Recall = 98.20170109356015)\n",
      "2024-09-14 08:44:28,397 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev BGL: F1 score = 95.16072059342989 | Precision = 92.30242119689356 | Recall = 98.20170109356015)\n",
      "2024-09-14 08:44:28,401 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Test BGL by threshold 0.5\n",
      "2024-09-14 08:44:28,401 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start evaluating Test BGL by threshold 0.5\n",
      "2024-09-14 08:44:59,319 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test BGL: F1 score = 90.79457122147518 | Precision = 89.99157706981123 | Recall = 91.61202461414304)\n",
      "2024-09-14 08:44:59,319 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test BGL: F1 score = 90.79457122147518 | Precision = 89.99157706981123 | Recall = 91.61202461414304)\n",
      "2024-09-14 08:44:59,324 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Exceed best F1 score: history = 90.62028752387656, current = 90.79457122147518.\n",
      "2024-09-14 08:44:59,324 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Exceed best F1 score: history = 90.62028752387656, current = 90.79457122147518.\n",
      "2024-09-14 08:44:59,345 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Training epoch 3 finished.\n",
      "2024-09-14 08:44:59,345 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Training epoch 3 finished.\n",
      "2024-09-14 08:44:59,360 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 4 | phase: train | start time: 08:44:59 | learning rate: [0.00035595703125].\n",
      "2024-09-14 08:44:59,360 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Starting epoch: 4 | phase: train | start time: 08:44:59 | learning rate: [0.00035595703125].\n",
      "2024-09-14 08:47:18,095 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 7000 | Epoch: 4 | Meta-train loss: 0.0030391314066946507 | Meta-test loss: 0.29856884479522705.\n",
      "2024-09-14 08:47:18,095 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 7000 | Epoch: 4 | Meta-train loss: 0.0030391314066946507 | Meta-test loss: 0.29856884479522705.\n",
      "2024-09-14 08:59:26,572 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 7500 | Epoch: 4 | Meta-train loss: 0.0015192872378975153 | Meta-test loss: 0.2609044909477234.\n",
      "2024-09-14 08:59:26,572 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 7500 | Epoch: 4 | Meta-train loss: 0.0015192872378975153 | Meta-test loss: 0.2609044909477234.\n",
      "2024-09-14 09:11:24,463 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 8000 | Epoch: 4 | Meta-train loss: 0.005556792952120304 | Meta-test loss: 0.3336265981197357.\n",
      "2024-09-14 09:11:24,463 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Step: 8000 | Epoch: 4 | Meta-train loss: 0.005556792952120304 | Meta-test loss: 0.3336265981197357.\n"
     ]
    }
   ],
   "source": [
    "# Log custom params\n",
    "logger.info(\n",
    "    f\"Custom params: alpha = {alpha} | beta = {beta} | gamma = {gamma} | word2vec_file = {word2vec_file}.\"\n",
    ")\n",
    "\n",
    "# meta learning\n",
    "log = \"layer={}_hidden={}_epoch={}\".format(num_layer, lstm_hiddens, epochs)\n",
    "best_model_file = os.path.join(output_model_dir, log + \"_best.pt\")\n",
    "last_model_file = os.path.join(output_model_dir, log + \"_last.pt\")\n",
    "if not os.path.exists(output_model_dir):\n",
    "    os.makedirs(output_model_dir)\n",
    "if mode == \"train\":\n",
    "    # Train\n",
    "    optimizer = Optimizer(\n",
    "        filter(lambda p: p.requires_grad, metalog.model.parameters()), lr=gamma\n",
    "    )\n",
    "    global_step = 0\n",
    "    best_f1_score = 0\n",
    "    for epoch in range(epochs):\n",
    "        metalog.model.train()\n",
    "        metalog.bk_model.train()\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        logger.info(\n",
    "            f\"Starting epoch: {epoch} | phase: train | start time: {start} | learning rate: {optimizer.lr}.\"\n",
    "        )\n",
    "\n",
    "        batch_num = int(np.ceil(len(labeled_train_HDFS) / float(batch_size)))\n",
    "        batch_iter = 0\n",
    "        batch_num_test = int(np.ceil(len(labeled_train_BGL) / float(batch_size)))\n",
    "        batch_iter_test = 0\n",
    "        total_bn = max(batch_num, batch_num_test)\n",
    "        meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "        meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "\n",
    "        for i in range(total_bn):\n",
    "            optimizer.zero_grad()\n",
    "            # meta train\n",
    "            meta_train_batch = meta_train_loader.__next__()\n",
    "            meta_test_batch = meta_test_loader.__next__()\n",
    "            tinst_tr = generate_tinsts_binary_label(meta_train_batch, vocab_HDFS)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_tr.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_tr.to_mps(DEVICE)\n",
    "            loss = metalog.forward(tinst_tr.inputs, tinst_tr.targets)\n",
    "            loss_value = loss.data.cpu().numpy()\n",
    "            loss.backward(retain_graph=True)\n",
    "            batch_iter += 1\n",
    "            if torch.cuda.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .cuda()\n",
    "                )\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .to(DEVICE)\n",
    "                )\n",
    "            else:\n",
    "                metalog.bk_model = get_updated_network(\n",
    "                    metalog.model, metalog.bk_model, alpha\n",
    "                ).train()\n",
    "            # meta test\n",
    "            tinst_test = generate_tinsts_binary_label(meta_test_batch, vocab_BGL)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_test.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_test.to_mps(DEVICE)\n",
    "            loss_te = beta * metalog.bk_forward(\n",
    "                tinst_test.inputs, tinst_test.targets\n",
    "            )\n",
    "            loss_value_te = loss_te.data.cpu().numpy() / beta\n",
    "            loss_te.backward()\n",
    "            batch_iter_test += 1\n",
    "            # aggregate\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            if global_step % 500 == 0:\n",
    "                logger.info(\n",
    "                    f\"Step: {global_step} | Epoch: {epoch} | Meta-train loss: {loss_value} | Meta-test loss: {loss_value_te}.\"\n",
    "                )\n",
    "            if batch_iter == batch_num:\n",
    "                meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "                batch_iter = 0\n",
    "            if batch_iter_test == batch_num_test:\n",
    "                meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "                batch_iter_test = 0\n",
    "\n",
    "        if train_BGL:\n",
    "            metalog.evaluate(\"Train BGL\", train_BGL)\n",
    "\n",
    "        if dev_BGL:\n",
    "            metalog.evaluate(\"Dev BGL\", dev_BGL)\n",
    "\n",
    "        if test_BGL:\n",
    "            _, _, f1_score = metalog.evaluate(\"Test BGL\", test_BGL)\n",
    "\n",
    "            if f1_score > best_f1_score:\n",
    "                logger.info(\n",
    "                    f\"Exceed best F1 score: history = {best_f1_score}, current = {f1_score}.\"\n",
    "                )\n",
    "                torch.save(metalog.model.state_dict(), best_model_file)\n",
    "                best_f1_score = f1_score\n",
    "\n",
    "        logger.info(f\"Training epoch {epoch} finished.\")\n",
    "        torch.save(metalog.model.state_dict(), last_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(last_model_file):\n",
    "    logger.info(\"=== Final Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(last_model_file))\n",
    "    metalog.evaluate(\"Test BGL\", test_BGL)\n",
    "if os.path.exists(best_model_file):\n",
    "    logger.info(\"=== Best Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(best_model_file))\n",
    "    metalog.evaluate(\"Test BGL\", test_BGL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
