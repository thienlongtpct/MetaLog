{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:08:24,112 - AttGRU - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct logger for Attention-Based GRU succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:08:24,167 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:08:24,168 - StatisticsRepresentation. - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct logger for Statistics Representation succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:08:24,170 - Statistics_Template_Encoder - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct logger for Statistics Template Encoder succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:08:24,171 - Vocab - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct logger for Vocab succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "# External libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import FastICA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project specific imports\n",
    "from CONSTANTS import DEVICE, LOG_ROOT, PROJECT_ROOT, SESSION\n",
    "from models.gru import AttGRUModel\n",
    "from module.Common import batch_variable_inst, data_iter, generate_tinsts_binary_label\n",
    "from module.Optimizer import Optimizer\n",
    "from preprocessing.AutoLabeling import Probabilistic_Labeling\n",
    "from preprocessing.datacutter.SimpleCutting import cut_by\n",
    "from preprocessing.Preprocess import Preprocessor\n",
    "from representations.sequences.statistics import Sequential_TF\n",
    "from representations.templates.statistics import (\n",
    "    Simple_template_TF_IDF,\n",
    "    Template_TF_IDF_without_clean,\n",
    ")\n",
    "from utils.Vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom default params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the word2vec file and its dimensions\n",
    "word2vec_file = \"glove.840B.300d.txt\"\n",
    "dim = 300\n",
    "\n",
    "# Set the hyperparameters\n",
    "alpha = 0.002  # Learning rate for meta-train\n",
    "beta = 1       # Scaling factor for meta-test loss\n",
    "gamma = 0.002  # Learning rate for optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Network model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM hidden units\n",
    "lstm_hiddens = 64\n",
    "\n",
    "# Number of layers in the network\n",
    "num_layer = 3\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 100\n",
    "\n",
    "# Dropout rate\n",
    "drop_out = 0.3\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 1\n",
    "\n",
    "# Threshold for prediction\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Dataset params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parser type\n",
    "parser = \"IBM\"\n",
    "\n",
    "# Set the mode to 'train'\n",
    "mode = \"train\"\n",
    "\n",
    "# Parameters for clustering\n",
    "min_cluster_size = 100\n",
    "min_samples = 100\n",
    "\n",
    "# Dimension reduction target\n",
    "reduce_dimensions = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(dataset, parser, min_cluster_size, min_samples, project_root):\n",
    "    \"\"\"Generate file paths for probabilistic labeling, random state, model, and result directories.\"\"\"\n",
    "    save_dir = os.path.join(project_root, \"outputs\")\n",
    "    \n",
    "    prob_label_res_file = os.path.join(\n",
    "        save_dir,\n",
    "        f\"results/MetaLog/{dataset}_{parser}/prob_label_res/mcs-{min_cluster_size}_ms-{min_samples}\"\n",
    "    )\n",
    "    rand_state_file = os.path.join(\n",
    "        save_dir,\n",
    "        f\"results/MetaLog/{dataset}_{parser}/prob_label_res/random_state\"\n",
    "    )\n",
    "    \n",
    "    return prob_label_res_file, rand_state_file\n",
    "\n",
    "\n",
    "# Generate file paths for HDFS (source dataset)\n",
    "prob_label_res_file_HDFS, rand_state_file_HDFS = get_file_paths(\"HDFS\", parser, min_cluster_size, min_samples, PROJECT_ROOT)\n",
    "\n",
    "# Generate file paths for BGL (target dataset)\n",
    "prob_label_res_file_BGL, rand_state_file_BGL = get_file_paths(\"BGL\", parser, min_cluster_size, min_samples, PROJECT_ROOT)\n",
    "\n",
    "# Shared model and result directories\n",
    "output_model_dir = os.path.join(PROJECT_ROOT, f\"outputs/models/MetaLog/{parser}/model\")\n",
    "output_res_dir = os.path.join(PROJECT_ROOT, f\"outputs/results/MetaLog/{parser}/detect_res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Function for updating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_network(old, new, lr, load=False):\n",
    "    updated_theta = {}\n",
    "    state_dicts = old.state_dict()\n",
    "    param_dicts = dict(old.named_parameters())\n",
    "\n",
    "    for i, (k, v) in enumerate(state_dicts.items()):\n",
    "        if k in param_dicts.keys() and param_dicts[k].grad is not None:\n",
    "            updated_theta[k] = param_dicts[k] - lr * param_dicts[k].grad\n",
    "        else:\n",
    "            updated_theta[k] = state_dicts[k]\n",
    "    if load:\n",
    "        new.load_state_dict(updated_theta)\n",
    "    else:\n",
    "        new = put_theta(new, updated_theta)\n",
    "    return new\n",
    "\n",
    "\n",
    "def put_theta(model, theta):\n",
    "    def k_param_fn(tmp_model, name=None):\n",
    "        if len(tmp_model._modules) != 0:\n",
    "            for k, v in tmp_model._modules.items():\n",
    "                if name is None:\n",
    "                    k_param_fn(v, name=str(k))\n",
    "                else:\n",
    "                    k_param_fn(v, name=str(name + \".\" + k))\n",
    "        else:\n",
    "            for k, v in tmp_model._parameters.items():\n",
    "                if not isinstance(v, torch.Tensor):\n",
    "                    continue\n",
    "                tmp_model._parameters[k] = theta[str(name + \".\" + k)]\n",
    "\n",
    "    k_param_fn(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logging config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:08:24,220 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Logger for MetaLog constructed successfully. Current working directory: /Users/minhthienlongvo/research/MetaLog. Logs will be written in /Users/minhthienlongvo/research/MetaLog/logs.\n"
     ]
    }
   ],
   "source": [
    "# Log setup\n",
    "logger = logging.getLogger(\"MetaLog\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Formatter\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    ")\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler(sys.stderr)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# File handler\n",
    "file_handler = logging.FileHandler(os.path.join(LOG_ROOT, \"MetaLog.log\"))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Successfully constructed\n",
    "logger.info(\n",
    "    f\"Logger for MetaLog constructed successfully. Current working directory: {os.getcwd()}. Logs will be written in {LOG_ROOT}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Log custom params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:08:24,225 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Network parameters:\n",
      "2024-09-19 16:08:24,226 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO:   - LSTM hidden units: 64\n",
      "2024-09-19 16:08:24,227 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO:   - Number of layers: 3\n",
      "2024-09-19 16:08:24,227 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO:   - Dropout rate: 0.3\n",
      "2024-09-19 16:08:24,229 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Hyperparameters:\n",
      "2024-09-19 16:08:24,230 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO:   - Alpha: 0.002\n",
      "2024-09-19 16:08:24,230 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO:   - Beta: 1\n",
      "2024-09-19 16:08:24,231 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO:   - Gamma: 0.002\n",
      "2024-09-19 16:08:24,231 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO:   - Word2Vec file: glove.840B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Network parameters:\")\n",
    "logger.info(f\"  - LSTM hidden units: {lstm_hiddens}\")\n",
    "logger.info(f\"  - Number of layers: {num_layer}\")\n",
    "logger.info(f\"  - Dropout rate: {drop_out}\")\n",
    "\n",
    "logger.info(f\"Hyperparameters:\")\n",
    "logger.info(f\"  - Alpha: {alpha}\")\n",
    "logger.info(f\"  - Beta: {beta}\")\n",
    "logger.info(f\"  - Gamma: {gamma}\")\n",
    "logger.info(f\"  - Word2Vec file: {word2vec_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:08:24,238 - Statistics_Template_Encoder - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loading word2vec dict from glove.840B.300d.txt.\n",
      "2024-09-19 16:08:24,239 - Statistics_Template_Encoder - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loading word2vec dict.\n",
      "100%|██████████| 2196017/2196017 [01:16<00:00, 28754.77it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(dataset, parser, cut_func, template_encoder):\n",
    "    \"\"\"Preprocess the data and return train, validation, and test sets.\"\"\"\n",
    "    processor = Preprocessor()\n",
    "    train_data, valid_data, test_data = processor.process(\n",
    "        dataset=dataset,\n",
    "        parsing=parser,\n",
    "        cut_func=cut_func,\n",
    "        template_encoding=template_encoder.present,\n",
    "    )\n",
    "    return train_data, valid_data, test_data, processor\n",
    "\n",
    "\n",
    "def encode_log_sequences(processor, train_data, test_data=None):\n",
    "    \"\"\"Encode log sequences using a sequential encoder and return updated instances.\"\"\"\n",
    "    sequential_encoder = Sequential_TF(processor.embedding)\n",
    "    \n",
    "    train_reprs = sequential_encoder.present(train_data)\n",
    "    for index, inst in enumerate(train_data):\n",
    "        inst.repr = train_reprs[index]\n",
    "    \n",
    "    if (test_data is not None):\n",
    "        test_reprs = sequential_encoder.present(test_data)\n",
    "        for index, inst in enumerate(test_data):\n",
    "            inst.repr = test_reprs[index]\n",
    "            \n",
    "        return train_data, test_data\n",
    "    \n",
    "    return train_data, None\n",
    "\n",
    "\n",
    "def reduce_dimension(train_data, reduce_dimensions):\n",
    "    \"\"\"Perform dimension reduction using FastICA and update the train data.\"\"\"\n",
    "    if reduce_dimensions != -1:\n",
    "        start_time = time.time()\n",
    "        print(f\"Start FastICA, target dimension: {reduce_dimensions}.\")\n",
    "        transformer = FastICA(n_components=reduce_dimensions)\n",
    "        train_reprs = np.array([inst.repr for inst in train_data])\n",
    "        train_reprs_transform = transformer.fit_transform(train_reprs)\n",
    "        for idx, inst in enumerate(train_data):\n",
    "            inst.repr = train_reprs_transform[idx]\n",
    "        print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def probabilistic_labeling(train_data, min_samples, min_cluster_size, prob_label_res_file, rand_state_file):\n",
    "    \"\"\"Perform probabilistic labeling and return labeled training data.\"\"\"\n",
    "    train_normal = [x for x, inst in enumerate(train_data) if inst.label == \"Normal\"]\n",
    "    normal_ids = train_normal[: int(0.5 * len(train_normal))]\n",
    "    \n",
    "    label_generator = Probabilistic_Labeling(\n",
    "        min_samples=min_samples,\n",
    "        min_clust_size=min_cluster_size,\n",
    "        res_file=prob_label_res_file,\n",
    "        rand_state_file=rand_state_file,\n",
    "    )\n",
    "    labeled_train_data = label_generator.auto_label(train_data, normal_ids)\n",
    "    return labeled_train_data\n",
    "\n",
    "datasets = [\"HDFS\", \"BGL\"]\n",
    "template_encoder = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    ")\n",
    "\n",
    "template_encoder_NC = None\n",
    "if \"NC\" in datasets:\n",
    "    template_encoder_NC = (\n",
    "        Simple_template_TF_IDF(word2vec_file)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Import BGL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:10:08,954 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:10:08,956 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/BGL/raw_log_seqs.txt\n",
      "100%|██████████| 85577/85577 [00:02<00:00, 39296.00it/s]\n",
      "2024-09-19 16:10:12,985 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Extraction finished successfully.\n",
      "2024-09-19 16:10:12,986 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:10:12,987 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/BGL.ini\n",
      "2024-09-19 16:10:12,990 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/persistence\n",
      "2024-09-19 16:10:13,014 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Persistence file found, loading.\n",
      "2024-09-19 16:10:13,044 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loaded.\n",
      "2024-09-19 16:10:13,046 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_event_mapping.dict ... True\n",
      "2024-09-19 16:10:13,046 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_sequences.txt ... True\n",
      "2024-09-19 16:10:13,046 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start loading previous parsing results.\n",
      "2024-09-19 16:10:14,589 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loaded 4747963 log sequences and their mappings.\n",
      "2024-09-19 16:10:15,201 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loaded 85577 blocks\n",
      "2024-09-19 16:10:15,202 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Finished in 2.16\n",
      "2024-09-19 16:10:15,468 - Statistics_Template_Encoder - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Total 0 OOV 0\n",
      "2024-09-19 16:10:15,541 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/templates.vec\n",
      "2024-09-19 16:10:15,541 - BGLLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: All data preparation finished in 2.50\n",
      "2024-09-19 16:10:15,542 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start preprocessing dataset BGL by parsing method IBM\n",
      "2024-09-19 16:10:15,543 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start generating instances.\n",
      "100%|██████████| 85577/85577 [00:00<00:00, 205948.53it/s]\n",
      "2024-09-19 16:10:15,990 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Train: 16352 Normal, 110 Anomalous instances.\n",
      "2024-09-19 16:10:15,990 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Test: 32922 Normal, 26982 Anomalous instances.\n",
      "2024-09-19 16:10:16,411 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-19 16:10:16,495 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Embed size: 120 in pre dataset.\n",
      "2024-09-19 16:10:16,496 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-19 16:10:16,638 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Embed size: 401 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:10:19,510 - Solitary_HDBSCAN - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct logger for Solitary_HDBSCAN succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:10:19,511 - Prob_Label - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct logger for Probabilistic labeling succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:10:19,512 - Prob_Label - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Found previous labeled file, will load and continue to accelerate the process.\n",
      "2024-09-19 16:10:19,513 - Prob_Label - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start load previous clustered results from /Users/minhthienlongvo/research/MetaLog/outputs/results/MetaLog/BGL_IBM/prob_label_res/mcs-100_ms-100\n",
      "2024-09-19 16:10:19,514 - Prob_Label - SESSION_eeeae78028ec72ad624cb44432770212 - WARNING: Please NOTE that this may cause some problem due to incomplete cluster settings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 1.07.\n"
     ]
    }
   ],
   "source": [
    "# Define the anomaly rate for BGL dataset\n",
    "BGL_ANOMALY_RATE = 0.01  # 1% anomaly in BGL train dataset\n",
    "\n",
    "# Preprocess BGL data with the specified anomaly rate in the train set\n",
    "cut_func = cut_by(0.3, 0.0, BGL_ANOMALY_RATE)\n",
    "train_BGL, _, test_BGL, processor_BGL = preprocess_data(\"BGL\", parser, cut_func, template_encoder)\n",
    "\n",
    "# Encode log sequences for the train and test sets\n",
    "encoded_train_BGL, encoded_test_BGL = encode_log_sequences(processor_BGL, train_BGL, test_BGL)\n",
    "\n",
    "# Reduce dimensions of the encoded train set if necessary\n",
    "reduced_train_BGL = reduce_dimension(encoded_train_BGL, reduce_dimensions)\n",
    "\n",
    "# Perform probabilistic labeling on the reduced train set\n",
    "labeled_train_BGL = probabilistic_labeling(reduced_train_BGL, min_samples, min_cluster_size, prob_label_res_file_BGL, rand_state_file_BGL)\n",
    "\n",
    "# Assign the final train and test sets\n",
    "final_train_BGL = reduced_train_BGL\n",
    "final_test_BGL = encoded_test_BGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Import HDFS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:10:19,537 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:10:19,538 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/raw_log_seqs.txt\n",
      "100%|██████████| 575061/575061 [00:02<00:00, 285400.56it/s]\n",
      "2024-09-19 16:10:21,657 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Extraction finished successfully.\n",
      "2024-09-19 16:10:21,903 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:10:21,903 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-19 16:10:21,904 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-19 16:10:21,904 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-19 16:10:21,906 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-19 16:10:21,906 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-19 16:10:21,909 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Persistence file found, loading.\n",
      "2024-09-19 16:10:21,909 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Persistence file found, loading.\n",
      "2024-09-19 16:10:21,915 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loaded.\n",
      "2024-09-19 16:10:21,915 - drain - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loaded.\n",
      "2024-09-19 16:10:21,916 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_event_mapping.dict ... True\n",
      "2024-09-19 16:10:21,916 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_sequences.txt ... True\n",
      "2024-09-19 16:10:21,917 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start loading previous parsing results.\n",
      "2024-09-19 16:10:25,832 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loaded 11175629 log sequences and their mappings.\n",
      "2024-09-19 16:10:27,314 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Loaded 575061 blocks\n",
      "2024-09-19 16:10:27,315 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Finished in 5.40\n",
      "2024-09-19 16:10:27,342 - Statistics_Template_Encoder - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Total 0 OOV 0\n",
      "2024-09-19 16:10:27,350 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/templates.vec\n",
      "2024-09-19 16:10:27,350 - HDFSLoader - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: All data preparation finished in 5.43\n",
      "2024-09-19 16:10:27,351 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start preprocessing dataset HDFS by parsing method IBM\n",
      "2024-09-19 16:10:27,351 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start generating instances.\n",
      "100%|██████████| 575061/575061 [00:02<00:00, 227298.00it/s]\n",
      "2024-09-19 16:10:30,088 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Train: 165180 Normal, 7338 Anomalous instances.\n",
      "2024-09-19 16:10:30,089 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Test: 393043 Normal, 9500 Anomalous instances.\n",
      "2024-09-19 16:10:31,473 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-19 16:10:31,821 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Embed size: 41 in pre dataset.\n",
      "2024-09-19 16:10:31,821 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-19 16:10:32,239 - Preprocessor - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Embed size: 46 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 23.39.\n"
     ]
    }
   ],
   "source": [
    "# Define the anomaly rate for HDFS dataset\n",
    "HDFS_ANOMALY_RATE = 1.0  # 100% anomaly in HDFS train set\n",
    "\n",
    "# Preprocess HDFS data with the specified anomaly rate in the train set\n",
    "cut_func = cut_by(0.3, 0.0, HDFS_ANOMALY_RATE)\n",
    "train_HDFS, _, _, processor_HDFS = preprocess_data(\"HDFS\", parser, cut_func, template_encoder)\n",
    "\n",
    "# Encode log sequences for the train set\n",
    "encoded_train_HDFS, _ = encode_log_sequences(processor_HDFS, train_HDFS)\n",
    "\n",
    "# Reduce dimensions of the encoded train set if necessary\n",
    "reduced_train_HDFS = reduce_dimension(encoded_train_HDFS, reduce_dimensions)\n",
    "\n",
    "# No probabilistic labeling for HDFS, use the reduced train set directly\n",
    "labeled_train_HDFS = reduced_train_HDFS\n",
    "\n",
    "# Assign the final train set for HDFS\n",
    "final_train_HDFS = reduced_train_HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Aggregate vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:11:00,693 - Vocab - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Total words: 436\n",
      "2024-09-19 16:11:00,696 - Vocab - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-19 16:11:00,698 - Vocab - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Total words: 50\n",
      "2024-09-19 16:11:00,699 - Vocab - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-19 16:11:00,699 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Padding offset: 432\n",
      "2024-09-19 16:11:00,701 - Vocab - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Total words: 482\n",
      "2024-09-19 16:11:00,702 - Vocab - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: The dim of pretrained embeddings: 300\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(processor):\n",
    "    \"\"\"Load embeddings from the given processor and return a Vocab object.\"\"\"\n",
    "    vocab = Vocab()\n",
    "    vocab.load_from_dict(processor.embedding)\n",
    "    return vocab\n",
    "\n",
    "def merge_embeddings(processor_BGL, processor_HDFS):\n",
    "    \"\"\"Merge embeddings from BGL and HDFS processors, handling padding.\"\"\"\n",
    "    new_embedding = processor_BGL.embedding.copy()\n",
    "    padding_offset = len(processor_BGL.embedding)  # Use the length of BGL embedding as padding offset\n",
    "    logger.info(f\"Padding offset: {padding_offset}\")\n",
    "    for key, value in processor_HDFS.embedding.items():\n",
    "        new_key = key + padding_offset  # Append padding offset to key to avoid conflicts\n",
    "        new_embedding[new_key] = value\n",
    "    \n",
    "    return new_embedding\n",
    "\n",
    "# Load embeddings for BGL and HDFS\n",
    "vocab_BGL = load_embeddings(processor_BGL)\n",
    "vocab_HDFS = load_embeddings(processor_HDFS)\n",
    "\n",
    "# Merge embeddings from BGL and HDFS\n",
    "merged_embedding = merge_embeddings(processor_BGL, processor_HDFS)\n",
    "\n",
    "# Load merged embeddings into a new vocab object\n",
    "vocab = Vocab()\n",
    "vocab.load_from_dict(merged_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. MetaLog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:11:00,743 - AttGRU - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: ==== Model Parameters ====\n",
      "2024-09-19 16:11:00,759 - AttGRU - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Input Dimension: 300\n",
      "2024-09-19 16:11:00,760 - AttGRU - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Hidden Size: 64\n",
      "2024-09-19 16:11:00,761 - AttGRU - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Num Layers: 3\n",
      "2024-09-19 16:11:00,762 - AttGRU - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Dropout: 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 128\n",
      "Orthogonal pretrainer loss: 2.99e-03\n",
      "2 128\n",
      "Orthogonal pretrainer loss: 2.13e+01\n"
     ]
    }
   ],
   "source": [
    "class MetaLog:\n",
    "    def __init__(self, vocab, num_layer, hidden_size, drop_out, label2id):\n",
    "        # Initialize MetaLog with given parameters\n",
    "        self.label2id = label2id\n",
    "        self.vocab = vocab\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1024\n",
    "        self.drop_out = drop_out\n",
    "        \n",
    "        # Initialize the main and backup models\n",
    "        self.model = AttGRUModel(vocab, num_layer, hidden_size, drop_out)\n",
    "        self.bk_model = AttGRUModel(vocab, num_layer, hidden_size, drop_out, is_backup=True)\n",
    "        \n",
    "        # Move models to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda(DEVICE)\n",
    "            self.bk_model = self.bk_model.cuda(DEVICE)\n",
    "        elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "            self.model = self.model.to(DEVICE)\n",
    "            self.bk_model = self.bk_model.to(DEVICE)\n",
    "        \n",
    "        # Define the loss function\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Forward pass through the main model\n",
    "        tag_logits = self.model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def bk_forward(self, inputs, targets):\n",
    "        # Forward pass through the backup model\n",
    "        tag_logits = self.bk_model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, threshold=None):\n",
    "        # Predict tags for given inputs\n",
    "        with torch.no_grad():\n",
    "            tag_logits = self.model(inputs)\n",
    "            tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        \n",
    "        if threshold is not None:\n",
    "            # Apply threshold to determine anomalous tags\n",
    "            probs = tag_logits.detach().cpu().numpy()\n",
    "            anomaly_id = self.label2id[\"Anomalous\"]\n",
    "            pred_tags = np.zeros(probs.shape[0])\n",
    "            \n",
    "            for i, logits in enumerate(probs):\n",
    "                if logits[anomaly_id] >= threshold:\n",
    "                    pred_tags[i] = anomaly_id\n",
    "                else:\n",
    "                    pred_tags[i] = 1 - anomaly_id\n",
    "        else:\n",
    "            # Use max value to determine tags\n",
    "            pred_tags = tag_logits.detach().max(1)[1].cpu()\n",
    "        \n",
    "        return pred_tags, tag_logits\n",
    "\n",
    "    def evaluate(self, dataset, instances, threshold=0.5):\n",
    "        # Evaluate the model on the given dataset\n",
    "        logger.info(f\"Start evaluating {dataset} by threshold {threshold}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            globalBatchNum = 0\n",
    "            TP, TN, FP, FN = 0, 0, 0, 0\n",
    "            tag_correct, tag_total = 0, 0\n",
    "            \n",
    "            for onebatch in data_iter(instances, self.test_batch_size, False):\n",
    "                tinst = generate_tinsts_binary_label(onebatch, vocab_BGL, False)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    tinst.to_cuda(DEVICE)\n",
    "                elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                    tinst.to_mps(DEVICE)\n",
    "                \n",
    "                pred_tags, tag_logits = self.predict(tinst.inputs, threshold)\n",
    "                \n",
    "                for inst, bmatch in batch_variable_inst(onebatch, pred_tags, tag_logits, processor_BGL.id2tag):\n",
    "                    tag_total += 1\n",
    "                    if bmatch:\n",
    "                        tag_correct += 1\n",
    "                        if inst.label == \"Normal\":\n",
    "                            TN += 1\n",
    "                        else:\n",
    "                            TP += 1\n",
    "                    else:\n",
    "                        if inst.label == \"Normal\":\n",
    "                            FP += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                \n",
    "                globalBatchNum += 1\n",
    "            \n",
    "            if TP + FP != 0:\n",
    "                # Calculate precision, recall, and F1 score\n",
    "                precision = 100 * TP / (TP + FP)\n",
    "                recall = 100 * TP / (TP + FN)\n",
    "                f1_score = 2 * precision * recall / (precision + recall)\n",
    "                logger.info(f\"{dataset}: F1 score = {f1_score} | Precision = {precision} | Recall = {recall}\")\n",
    "            else:\n",
    "                logger.info(f\"{dataset}: F1 score = {0} | Precision = {0} | Recall = {0}\")\n",
    "                precision, recall, f1_score = 0, 0, 0\n",
    "        \n",
    "        return precision, recall, f1_score\n",
    "\n",
    "# Instantiate the MetaLog class with given parameters\n",
    "metalog = MetaLog(\n",
    "    vocab=vocab,\n",
    "    num_layer=num_layer,\n",
    "    hidden_size=lstm_hiddens,\n",
    "    drop_out=drop_out,\n",
    "    label2id=processor_BGL.label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2. Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the model output directory if it doesn't exist.\"\"\"\n",
    "if not os.path.exists(output_model_dir):\n",
    "    os.makedirs(output_model_dir)\n",
    "\n",
    "\"\"\"Construct and return the paths for the best and last model files.\"\"\"\n",
    "info = f\"layer={num_layer}_hidden={lstm_hiddens}_dropout_{drop_out}_epoch={epochs}\"\n",
    "\n",
    "best_model_file = os.path.join(output_model_dir, info + \"_best.pt\")\n",
    "last_model_file = os.path.join(output_model_dir, info + \"_last.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:11:01,459 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Starting epoch: 0 | phase: train | start time: 16:11:01 | learning rate: [0.002].\n",
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:935: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n",
      "2024-09-19 16:11:15,011 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 10 | Epoch: 0 | Meta-train loss: 0.23330549895763397 | Meta-test loss: 0.4399636685848236.\n",
      "2024-09-19 16:11:27,905 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 20 | Epoch: 0 | Meta-train loss: 0.10860926657915115 | Meta-test loss: 0.3510626256465912.\n",
      "2024-09-19 16:11:39,443 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 30 | Epoch: 0 | Meta-train loss: 0.14599670469760895 | Meta-test loss: 0.43469542264938354.\n",
      "2024-09-19 16:11:51,288 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 40 | Epoch: 0 | Meta-train loss: 0.10028029978275299 | Meta-test loss: 0.40106499195098877.\n",
      "2024-09-19 16:12:02,393 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 50 | Epoch: 0 | Meta-train loss: 0.07651865482330322 | Meta-test loss: 0.3702140748500824.\n",
      "2024-09-19 16:12:13,275 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 60 | Epoch: 0 | Meta-train loss: 0.09858693927526474 | Meta-test loss: 0.41126373410224915.\n",
      "2024-09-19 16:12:24,754 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 70 | Epoch: 0 | Meta-train loss: 0.11160609871149063 | Meta-test loss: 0.5224874019622803.\n",
      "2024-09-19 16:12:36,359 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 80 | Epoch: 0 | Meta-train loss: 0.08570640534162521 | Meta-test loss: 0.3713271915912628.\n",
      "2024-09-19 16:12:47,558 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 90 | Epoch: 0 | Meta-train loss: 0.061398234218358994 | Meta-test loss: 0.3417256772518158.\n",
      "2024-09-19 16:12:58,328 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 100 | Epoch: 0 | Meta-train loss: 0.0907331258058548 | Meta-test loss: 0.3385162353515625.\n",
      "2024-09-19 16:13:09,249 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 110 | Epoch: 0 | Meta-train loss: 0.16888271272182465 | Meta-test loss: 0.36365482211112976.\n",
      "2024-09-19 16:13:20,064 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 120 | Epoch: 0 | Meta-train loss: 0.009112597443163395 | Meta-test loss: 0.444664865732193.\n",
      "2024-09-19 16:13:32,073 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 130 | Epoch: 0 | Meta-train loss: 0.06411384046077728 | Meta-test loss: 0.3877127766609192.\n",
      "2024-09-19 16:13:42,931 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 140 | Epoch: 0 | Meta-train loss: 0.027446385473012924 | Meta-test loss: 0.3900195360183716.\n",
      "2024-09-19 16:13:53,768 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 150 | Epoch: 0 | Meta-train loss: 0.02595810778439045 | Meta-test loss: 0.3680022060871124.\n",
      "2024-09-19 16:14:04,292 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 160 | Epoch: 0 | Meta-train loss: 0.10762063413858414 | Meta-test loss: 0.3557257652282715.\n",
      "2024-09-19 16:14:16,680 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 170 | Epoch: 0 | Meta-train loss: 0.01678781397640705 | Meta-test loss: 0.3776642084121704.\n",
      "2024-09-19 16:14:27,051 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 180 | Epoch: 0 | Meta-train loss: 0.0629238486289978 | Meta-test loss: 0.39614805579185486.\n",
      "2024-09-19 16:14:38,868 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 190 | Epoch: 0 | Meta-train loss: 0.06777750700712204 | Meta-test loss: 0.2938832640647888.\n",
      "2024-09-19 16:14:49,919 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 200 | Epoch: 0 | Meta-train loss: 0.08319693058729172 | Meta-test loss: 0.391985148191452.\n",
      "2024-09-19 16:15:00,559 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 210 | Epoch: 0 | Meta-train loss: 0.18607501685619354 | Meta-test loss: 0.304739385843277.\n",
      "2024-09-19 16:15:11,505 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 220 | Epoch: 0 | Meta-train loss: 0.022978588938713074 | Meta-test loss: 0.4377087652683258.\n",
      "2024-09-19 16:15:22,427 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 230 | Epoch: 0 | Meta-train loss: 0.032189469784498215 | Meta-test loss: 0.2863183915615082.\n",
      "2024-09-19 16:15:32,681 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 240 | Epoch: 0 | Meta-train loss: 0.0049495697021484375 | Meta-test loss: 0.36222103238105774.\n",
      "2024-09-19 16:15:42,839 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 250 | Epoch: 0 | Meta-train loss: 0.005848168395459652 | Meta-test loss: 0.28764933347702026.\n",
      "2024-09-19 16:15:53,411 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 260 | Epoch: 0 | Meta-train loss: 0.009041070938110352 | Meta-test loss: 0.2686670422554016.\n",
      "2024-09-19 16:16:03,619 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 270 | Epoch: 0 | Meta-train loss: 0.03901689499616623 | Meta-test loss: 0.28413957357406616.\n",
      "2024-09-19 16:16:13,972 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 280 | Epoch: 0 | Meta-train loss: 0.010526803322136402 | Meta-test loss: 0.4285397231578827.\n",
      "2024-09-19 16:16:24,122 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 290 | Epoch: 0 | Meta-train loss: 0.13103464245796204 | Meta-test loss: 0.46937277913093567.\n",
      "2024-09-19 16:16:34,353 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 300 | Epoch: 0 | Meta-train loss: 0.05254179239273071 | Meta-test loss: 0.365691214799881.\n",
      "2024-09-19 16:16:44,546 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 310 | Epoch: 0 | Meta-train loss: 0.0626029297709465 | Meta-test loss: 0.34538283944129944.\n",
      "2024-09-19 16:16:54,929 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 320 | Epoch: 0 | Meta-train loss: 0.00498819537460804 | Meta-test loss: 0.4117792546749115.\n",
      "2024-09-19 16:17:05,404 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 330 | Epoch: 0 | Meta-train loss: 0.10870955884456635 | Meta-test loss: 0.2654939889907837.\n",
      "2024-09-19 16:17:15,978 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 340 | Epoch: 0 | Meta-train loss: 0.011343032121658325 | Meta-test loss: 0.2967277467250824.\n",
      "2024-09-19 16:17:26,480 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 350 | Epoch: 0 | Meta-train loss: 0.12451764941215515 | Meta-test loss: 0.30732065439224243.\n",
      "2024-09-19 16:17:36,776 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 360 | Epoch: 0 | Meta-train loss: 0.008646881207823753 | Meta-test loss: 0.31421756744384766.\n",
      "2024-09-19 16:17:46,943 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 370 | Epoch: 0 | Meta-train loss: 0.012314755469560623 | Meta-test loss: 0.3677230775356293.\n",
      "2024-09-19 16:17:57,105 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 380 | Epoch: 0 | Meta-train loss: 0.009229900315403938 | Meta-test loss: 0.429067462682724.\n",
      "2024-09-19 16:18:07,289 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 390 | Epoch: 0 | Meta-train loss: 0.07429373264312744 | Meta-test loss: 0.41719770431518555.\n",
      "2024-09-19 16:18:18,040 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 400 | Epoch: 0 | Meta-train loss: 0.008387884125113487 | Meta-test loss: 0.3322766423225403.\n",
      "2024-09-19 16:18:28,283 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 410 | Epoch: 0 | Meta-train loss: 0.09642301499843597 | Meta-test loss: 0.3772839307785034.\n",
      "2024-09-19 16:18:38,931 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 420 | Epoch: 0 | Meta-train loss: 0.10585121065378189 | Meta-test loss: 0.33567509055137634.\n",
      "2024-09-19 16:18:49,489 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 430 | Epoch: 0 | Meta-train loss: 0.004260178189724684 | Meta-test loss: 0.3050534129142761.\n",
      "2024-09-19 16:18:59,856 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 440 | Epoch: 0 | Meta-train loss: 0.07560484856367111 | Meta-test loss: 0.3063447177410126.\n",
      "2024-09-19 16:19:10,129 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 450 | Epoch: 0 | Meta-train loss: 0.061076413840055466 | Meta-test loss: 0.2961379885673523.\n",
      "2024-09-19 16:19:20,528 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 460 | Epoch: 0 | Meta-train loss: 0.02796177566051483 | Meta-test loss: 0.22309023141860962.\n",
      "2024-09-19 16:19:30,729 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 470 | Epoch: 0 | Meta-train loss: 0.06427566707134247 | Meta-test loss: 0.2763562798500061.\n",
      "2024-09-19 16:19:40,890 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 480 | Epoch: 0 | Meta-train loss: 0.051164135336875916 | Meta-test loss: 0.23909200727939606.\n",
      "2024-09-19 16:19:50,964 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 490 | Epoch: 0 | Meta-train loss: 0.0771213099360466 | Meta-test loss: 0.3453643023967743.\n",
      "2024-09-19 16:20:01,401 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 500 | Epoch: 0 | Meta-train loss: 0.002966296626254916 | Meta-test loss: 0.39087504148483276.\n",
      "2024-09-19 16:20:11,578 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 510 | Epoch: 0 | Meta-train loss: 0.06811308860778809 | Meta-test loss: 0.42482832074165344.\n",
      "2024-09-19 16:20:22,003 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 520 | Epoch: 0 | Meta-train loss: 0.00500095821917057 | Meta-test loss: 0.31835997104644775.\n",
      "2024-09-19 16:20:33,239 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 530 | Epoch: 0 | Meta-train loss: 0.006549531128257513 | Meta-test loss: 0.3211512565612793.\n",
      "2024-09-19 16:20:47,363 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 540 | Epoch: 0 | Meta-train loss: 0.059017423540353775 | Meta-test loss: 0.369292289018631.\n",
      "2024-09-19 16:20:57,844 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 550 | Epoch: 0 | Meta-train loss: 0.0036683084908872843 | Meta-test loss: 0.31631287932395935.\n",
      "2024-09-19 16:21:08,099 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 560 | Epoch: 0 | Meta-train loss: 0.004733046051114798 | Meta-test loss: 0.3227074444293976.\n",
      "2024-09-19 16:21:18,354 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 570 | Epoch: 0 | Meta-train loss: 0.011040974408388138 | Meta-test loss: 0.2879504859447479.\n",
      "2024-09-19 16:21:28,566 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 580 | Epoch: 0 | Meta-train loss: 0.003859310643747449 | Meta-test loss: 0.3246980309486389.\n",
      "2024-09-19 16:21:38,700 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 590 | Epoch: 0 | Meta-train loss: 0.0072590564377605915 | Meta-test loss: 0.3214505612850189.\n",
      "2024-09-19 16:21:48,885 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 600 | Epoch: 0 | Meta-train loss: 0.04025926813483238 | Meta-test loss: 0.3254908323287964.\n",
      "2024-09-19 16:21:59,070 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 610 | Epoch: 0 | Meta-train loss: 0.00886880699545145 | Meta-test loss: 0.35132288932800293.\n",
      "2024-09-19 16:22:09,467 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 620 | Epoch: 0 | Meta-train loss: 0.06113465502858162 | Meta-test loss: 0.4043388366699219.\n",
      "2024-09-19 16:22:19,709 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 630 | Epoch: 0 | Meta-train loss: 0.0042808507569134235 | Meta-test loss: 0.34944990277290344.\n",
      "2024-09-19 16:22:29,858 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 640 | Epoch: 0 | Meta-train loss: 0.06639968603849411 | Meta-test loss: 0.24076707661151886.\n",
      "2024-09-19 16:22:40,035 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 650 | Epoch: 0 | Meta-train loss: 0.01482748705893755 | Meta-test loss: 0.3158662021160126.\n",
      "2024-09-19 16:22:50,217 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 660 | Epoch: 0 | Meta-train loss: 0.008264439180493355 | Meta-test loss: 0.3041820824146271.\n",
      "2024-09-19 16:23:00,477 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 670 | Epoch: 0 | Meta-train loss: 0.00659744068980217 | Meta-test loss: 0.3473028540611267.\n",
      "2024-09-19 16:23:10,780 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 680 | Epoch: 0 | Meta-train loss: 0.004684329964220524 | Meta-test loss: 0.29641708731651306.\n",
      "2024-09-19 16:23:20,983 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 690 | Epoch: 0 | Meta-train loss: 0.004543440416455269 | Meta-test loss: 0.304126113653183.\n",
      "2024-09-19 16:23:30,997 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 700 | Epoch: 0 | Meta-train loss: 0.008084671571850777 | Meta-test loss: 0.333849161863327.\n",
      "2024-09-19 16:23:41,191 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 710 | Epoch: 0 | Meta-train loss: 0.005081419367343187 | Meta-test loss: 0.3732084631919861.\n",
      "2024-09-19 16:23:51,301 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 720 | Epoch: 0 | Meta-train loss: 0.04560525715351105 | Meta-test loss: 0.302334189414978.\n",
      "2024-09-19 16:24:01,502 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 730 | Epoch: 0 | Meta-train loss: 0.05911918357014656 | Meta-test loss: 0.325509250164032.\n",
      "2024-09-19 16:24:11,945 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 740 | Epoch: 0 | Meta-train loss: 0.006021595094352961 | Meta-test loss: 0.2763088345527649.\n",
      "2024-09-19 16:24:22,404 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 750 | Epoch: 0 | Meta-train loss: 0.006045203190296888 | Meta-test loss: 0.3201987147331238.\n",
      "2024-09-19 16:24:33,008 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 760 | Epoch: 0 | Meta-train loss: 0.0022756566759198904 | Meta-test loss: 0.3326488733291626.\n",
      "2024-09-19 16:24:43,685 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 770 | Epoch: 0 | Meta-train loss: 0.003427010728046298 | Meta-test loss: 0.35967040061950684.\n",
      "2024-09-19 16:24:55,613 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 780 | Epoch: 0 | Meta-train loss: 0.09513306617736816 | Meta-test loss: 0.36320242285728455.\n",
      "2024-09-19 16:25:05,881 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 790 | Epoch: 0 | Meta-train loss: 0.003260679543018341 | Meta-test loss: 0.260193794965744.\n",
      "2024-09-19 16:25:16,309 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 800 | Epoch: 0 | Meta-train loss: 0.007995118387043476 | Meta-test loss: 0.35097211599349976.\n",
      "2024-09-19 16:25:26,543 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 810 | Epoch: 0 | Meta-train loss: 0.005624954588711262 | Meta-test loss: 0.3062414526939392.\n",
      "2024-09-19 16:25:36,772 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 820 | Epoch: 0 | Meta-train loss: 0.10174506157636642 | Meta-test loss: 0.33622345328330994.\n",
      "2024-09-19 16:25:46,932 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 830 | Epoch: 0 | Meta-train loss: 0.007946958765387535 | Meta-test loss: 0.3205581605434418.\n",
      "2024-09-19 16:25:57,122 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 840 | Epoch: 0 | Meta-train loss: 0.003650556318461895 | Meta-test loss: 0.3310772776603699.\n",
      "2024-09-19 16:26:07,213 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 850 | Epoch: 0 | Meta-train loss: 0.05677877366542816 | Meta-test loss: 0.3596543073654175.\n",
      "2024-09-19 16:26:17,214 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 860 | Epoch: 0 | Meta-train loss: 0.03302001953125 | Meta-test loss: 0.3147335946559906.\n",
      "2024-09-19 16:26:27,313 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 870 | Epoch: 0 | Meta-train loss: 0.06122339889407158 | Meta-test loss: 0.4630325138568878.\n",
      "2024-09-19 16:26:37,611 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 880 | Epoch: 0 | Meta-train loss: 0.0032562308479100466 | Meta-test loss: 0.36695554852485657.\n",
      "2024-09-19 16:26:47,840 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 890 | Epoch: 0 | Meta-train loss: 0.06756636500358582 | Meta-test loss: 0.27516263723373413.\n",
      "2024-09-19 16:26:58,365 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 900 | Epoch: 0 | Meta-train loss: 0.0034222991671413183 | Meta-test loss: 0.3763454854488373.\n",
      "2024-09-19 16:27:08,746 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 910 | Epoch: 0 | Meta-train loss: 0.04544459655880928 | Meta-test loss: 0.30448025465011597.\n",
      "2024-09-19 16:27:19,472 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 920 | Epoch: 0 | Meta-train loss: 0.009761105291545391 | Meta-test loss: 0.3573528230190277.\n",
      "2024-09-19 16:27:29,866 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 930 | Epoch: 0 | Meta-train loss: 0.04515155032277107 | Meta-test loss: 0.4329725205898285.\n",
      "2024-09-19 16:27:40,359 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 940 | Epoch: 0 | Meta-train loss: 0.058551013469696045 | Meta-test loss: 0.3947769105434418.\n",
      "2024-09-19 16:27:50,951 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 950 | Epoch: 0 | Meta-train loss: 0.0026525186840444803 | Meta-test loss: 0.29437488317489624.\n",
      "2024-09-19 16:28:02,634 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 960 | Epoch: 0 | Meta-train loss: 0.028027156367897987 | Meta-test loss: 0.3163439929485321.\n",
      "2024-09-19 16:28:12,847 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 970 | Epoch: 0 | Meta-train loss: 0.056596606969833374 | Meta-test loss: 0.3489975035190582.\n",
      "2024-09-19 16:28:24,523 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 980 | Epoch: 0 | Meta-train loss: 0.01138052437454462 | Meta-test loss: 0.4030364155769348.\n",
      "2024-09-19 16:28:34,738 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 990 | Epoch: 0 | Meta-train loss: 0.012793979607522488 | Meta-test loss: 0.3302103281021118.\n",
      "2024-09-19 16:28:44,994 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1000 | Epoch: 0 | Meta-train loss: 0.004228847101330757 | Meta-test loss: 0.31945478916168213.\n",
      "2024-09-19 16:28:55,176 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1010 | Epoch: 0 | Meta-train loss: 0.022052137181162834 | Meta-test loss: 0.3019016683101654.\n",
      "2024-09-19 16:29:05,541 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1020 | Epoch: 0 | Meta-train loss: 0.023042360320687294 | Meta-test loss: 0.361046701669693.\n",
      "2024-09-19 16:29:15,816 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1030 | Epoch: 0 | Meta-train loss: 0.049931447952985764 | Meta-test loss: 0.36516791582107544.\n",
      "2024-09-19 16:29:25,913 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1040 | Epoch: 0 | Meta-train loss: 0.07971922308206558 | Meta-test loss: 0.26618680357933044.\n",
      "2024-09-19 16:29:36,128 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1050 | Epoch: 0 | Meta-train loss: 0.004449855536222458 | Meta-test loss: 0.38436904549598694.\n",
      "2024-09-19 16:29:47,662 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1060 | Epoch: 0 | Meta-train loss: 0.02846900001168251 | Meta-test loss: 0.3412838280200958.\n",
      "2024-09-19 16:29:59,417 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1070 | Epoch: 0 | Meta-train loss: 0.06144682690501213 | Meta-test loss: 0.3125222325325012.\n",
      "2024-09-19 16:30:09,718 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1080 | Epoch: 0 | Meta-train loss: 0.050456225872039795 | Meta-test loss: 0.2985037863254547.\n",
      "2024-09-19 16:30:19,958 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1090 | Epoch: 0 | Meta-train loss: 0.016369711607694626 | Meta-test loss: 0.38458606600761414.\n",
      "2024-09-19 16:30:30,116 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1100 | Epoch: 0 | Meta-train loss: 0.002779573667794466 | Meta-test loss: 0.32188597321510315.\n",
      "2024-09-19 16:30:40,280 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1110 | Epoch: 0 | Meta-train loss: 0.0033502208534628153 | Meta-test loss: 0.35397735238075256.\n",
      "2024-09-19 16:30:50,556 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1120 | Epoch: 0 | Meta-train loss: 0.005456071812659502 | Meta-test loss: 0.3290959596633911.\n",
      "2024-09-19 16:31:00,750 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1130 | Epoch: 0 | Meta-train loss: 0.0055114105343818665 | Meta-test loss: 0.34220144152641296.\n",
      "2024-09-19 16:31:10,978 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1140 | Epoch: 0 | Meta-train loss: 0.01894267462193966 | Meta-test loss: 0.40866538882255554.\n",
      "2024-09-19 16:31:21,348 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1150 | Epoch: 0 | Meta-train loss: 0.035877861082553864 | Meta-test loss: 0.4184092581272125.\n",
      "2024-09-19 16:31:31,962 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1160 | Epoch: 0 | Meta-train loss: 0.048120398074388504 | Meta-test loss: 0.3028958737850189.\n",
      "2024-09-19 16:31:42,507 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1170 | Epoch: 0 | Meta-train loss: 0.0035523255355656147 | Meta-test loss: 0.33979666233062744.\n",
      "2024-09-19 16:31:53,131 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1180 | Epoch: 0 | Meta-train loss: 0.02840975858271122 | Meta-test loss: 0.32039183378219604.\n",
      "2024-09-19 16:32:03,681 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1190 | Epoch: 0 | Meta-train loss: 0.002458017785102129 | Meta-test loss: 0.4340690076351166.\n",
      "2024-09-19 16:32:14,297 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1200 | Epoch: 0 | Meta-train loss: 0.06294838339090347 | Meta-test loss: 0.3152490258216858.\n",
      "2024-09-19 16:32:25,284 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1210 | Epoch: 0 | Meta-train loss: 0.004586176946759224 | Meta-test loss: 0.3419840931892395.\n",
      "2024-09-19 16:32:35,553 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1220 | Epoch: 0 | Meta-train loss: 0.020626595243811607 | Meta-test loss: 0.3839316964149475.\n",
      "2024-09-19 16:32:47,133 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1230 | Epoch: 0 | Meta-train loss: 0.08414510637521744 | Meta-test loss: 0.32854264974594116.\n",
      "2024-09-19 16:32:57,328 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1240 | Epoch: 0 | Meta-train loss: 0.12429763376712799 | Meta-test loss: 0.2709468901157379.\n",
      "2024-09-19 16:33:07,534 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1250 | Epoch: 0 | Meta-train loss: 0.007337043061852455 | Meta-test loss: 0.35491248965263367.\n",
      "2024-09-19 16:33:17,854 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1260 | Epoch: 0 | Meta-train loss: 0.057767972350120544 | Meta-test loss: 0.36528676748275757.\n",
      "2024-09-19 16:33:28,171 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1270 | Epoch: 0 | Meta-train loss: 0.11484947055578232 | Meta-test loss: 0.30304422974586487.\n",
      "2024-09-19 16:33:38,373 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1280 | Epoch: 0 | Meta-train loss: 0.009955978952348232 | Meta-test loss: 0.24115633964538574.\n",
      "2024-09-19 16:33:48,676 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1290 | Epoch: 0 | Meta-train loss: 0.0019931343849748373 | Meta-test loss: 0.3085777163505554.\n",
      "2024-09-19 16:34:01,157 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1300 | Epoch: 0 | Meta-train loss: 0.009944315068423748 | Meta-test loss: 0.3111957311630249.\n",
      "2024-09-19 16:34:11,885 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1310 | Epoch: 0 | Meta-train loss: 0.0025982772931456566 | Meta-test loss: 0.3276441991329193.\n",
      "2024-09-19 16:34:23,398 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1320 | Epoch: 0 | Meta-train loss: 0.12117399275302887 | Meta-test loss: 0.2820481061935425.\n",
      "2024-09-19 16:34:34,218 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1330 | Epoch: 0 | Meta-train loss: 0.060902535915374756 | Meta-test loss: 0.42555513978004456.\n",
      "2024-09-19 16:34:44,499 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1340 | Epoch: 0 | Meta-train loss: 0.003502959618344903 | Meta-test loss: 0.31610116362571716.\n",
      "2024-09-19 16:34:54,721 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1350 | Epoch: 0 | Meta-train loss: 0.0018371572950854897 | Meta-test loss: 0.2251613587141037.\n",
      "2024-09-19 16:35:04,971 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1360 | Epoch: 0 | Meta-train loss: 0.031549785286188126 | Meta-test loss: 0.33955904841423035.\n",
      "2024-09-19 16:35:15,316 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1370 | Epoch: 0 | Meta-train loss: 0.12171456217765808 | Meta-test loss: 0.3546670079231262.\n",
      "2024-09-19 16:35:25,523 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1380 | Epoch: 0 | Meta-train loss: 0.0030032326467335224 | Meta-test loss: 0.33436107635498047.\n",
      "2024-09-19 16:35:35,661 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1390 | Epoch: 0 | Meta-train loss: 0.003926124889403582 | Meta-test loss: 0.3652554750442505.\n",
      "2024-09-19 16:35:45,815 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1400 | Epoch: 0 | Meta-train loss: 0.11813872307538986 | Meta-test loss: 0.36826738715171814.\n",
      "2024-09-19 16:35:56,081 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1410 | Epoch: 0 | Meta-train loss: 0.07854873687028885 | Meta-test loss: 0.2892358601093292.\n",
      "2024-09-19 16:36:06,212 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1420 | Epoch: 0 | Meta-train loss: 0.004401912447065115 | Meta-test loss: 0.2585713565349579.\n",
      "2024-09-19 16:36:16,460 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1430 | Epoch: 0 | Meta-train loss: 0.0023079710081219673 | Meta-test loss: 0.2694532573223114.\n",
      "2024-09-19 16:36:26,794 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1440 | Epoch: 0 | Meta-train loss: 0.06816007941961288 | Meta-test loss: 0.27042660117149353.\n",
      "2024-09-19 16:36:36,989 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1450 | Epoch: 0 | Meta-train loss: 0.0791049599647522 | Meta-test loss: 0.31385454535484314.\n",
      "2024-09-19 16:36:47,187 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1460 | Epoch: 0 | Meta-train loss: 0.04670059680938721 | Meta-test loss: 0.3376866579055786.\n",
      "2024-09-19 16:36:58,742 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1470 | Epoch: 0 | Meta-train loss: 0.004096973687410355 | Meta-test loss: 0.3308519721031189.\n",
      "2024-09-19 16:37:09,037 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1480 | Epoch: 0 | Meta-train loss: 0.003582992358133197 | Meta-test loss: 0.3136056363582611.\n",
      "2024-09-19 16:37:19,238 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1490 | Epoch: 0 | Meta-train loss: 0.08922591805458069 | Meta-test loss: 0.29728779196739197.\n",
      "2024-09-19 16:37:31,082 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1500 | Epoch: 0 | Meta-train loss: 0.060798149555921555 | Meta-test loss: 0.34160488843917847.\n",
      "2024-09-19 16:37:41,350 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1510 | Epoch: 0 | Meta-train loss: 0.05815764144062996 | Meta-test loss: 0.2500142753124237.\n",
      "2024-09-19 16:37:52,737 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1520 | Epoch: 0 | Meta-train loss: 0.018519937992095947 | Meta-test loss: 0.3362876772880554.\n",
      "2024-09-19 16:38:02,873 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1530 | Epoch: 0 | Meta-train loss: 0.0025298905093222857 | Meta-test loss: 0.2880849540233612.\n",
      "2024-09-19 16:38:13,166 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1540 | Epoch: 0 | Meta-train loss: 0.0009911495726555586 | Meta-test loss: 0.2748248875141144.\n",
      "2024-09-19 16:38:23,239 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1550 | Epoch: 0 | Meta-train loss: 0.05535256862640381 | Meta-test loss: 0.2815549075603485.\n",
      "2024-09-19 16:38:33,515 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1560 | Epoch: 0 | Meta-train loss: 0.040313370525836945 | Meta-test loss: 0.2247147113084793.\n",
      "2024-09-19 16:38:43,685 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1570 | Epoch: 0 | Meta-train loss: 0.06175971031188965 | Meta-test loss: 0.4613438546657562.\n",
      "2024-09-19 16:38:54,889 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1580 | Epoch: 0 | Meta-train loss: 0.004645877983421087 | Meta-test loss: 0.2507990300655365.\n",
      "2024-09-19 16:39:05,157 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1590 | Epoch: 0 | Meta-train loss: 0.028602464124560356 | Meta-test loss: 0.2921018600463867.\n",
      "2024-09-19 16:39:15,194 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1600 | Epoch: 0 | Meta-train loss: 0.0030594738200306892 | Meta-test loss: 0.3577483296394348.\n",
      "2024-09-19 16:39:25,369 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1610 | Epoch: 0 | Meta-train loss: 0.002503689145669341 | Meta-test loss: 0.42084625363349915.\n",
      "2024-09-19 16:39:35,452 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1620 | Epoch: 0 | Meta-train loss: 0.0031025302596390247 | Meta-test loss: 0.3365797996520996.\n",
      "2024-09-19 16:39:45,560 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1630 | Epoch: 0 | Meta-train loss: 0.005035612266510725 | Meta-test loss: 0.3014703392982483.\n",
      "2024-09-19 16:39:55,764 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1640 | Epoch: 0 | Meta-train loss: 0.04939588904380798 | Meta-test loss: 0.3643648028373718.\n",
      "2024-09-19 16:40:05,957 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1650 | Epoch: 0 | Meta-train loss: 0.04732824116945267 | Meta-test loss: 0.2737874984741211.\n",
      "2024-09-19 16:40:16,125 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1660 | Epoch: 0 | Meta-train loss: 0.004365174565464258 | Meta-test loss: 0.24993455410003662.\n",
      "2024-09-19 16:40:26,215 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1670 | Epoch: 0 | Meta-train loss: 0.05308232828974724 | Meta-test loss: 0.3603898882865906.\n",
      "2024-09-19 16:40:36,259 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1680 | Epoch: 0 | Meta-train loss: 0.004632369615137577 | Meta-test loss: 0.32999369502067566.\n",
      "2024-09-19 16:40:46,594 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1690 | Epoch: 0 | Meta-train loss: 0.004004180897027254 | Meta-test loss: 0.34609973430633545.\n",
      "2024-09-19 16:40:59,339 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1700 | Epoch: 0 | Meta-train loss: 0.003800230799242854 | Meta-test loss: 0.3229890465736389.\n",
      "2024-09-19 16:41:09,549 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1710 | Epoch: 0 | Meta-train loss: 0.0020531988702714443 | Meta-test loss: 0.360430508852005.\n",
      "2024-09-19 16:41:19,761 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Step: 1720 | Epoch: 0 | Meta-train loss: 0.012950422242283821 | Meta-test loss: 0.32008060812950134.\n",
      "2024-09-19 16:41:25,892 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-19 16:42:28,209 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Train: F1 score = 87.23767691556857 | Precision = 79.94186046511628 | Recall = 95.99892588614394\n",
      "2024-09-19 16:42:28,213 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-19 16:42:57,435 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Test: F1 score = 78.34932060451712 | Precision = 66.5647430748102 | Recall = 95.20421021421689\n",
      "2024-09-19 16:42:57,436 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Exceed best F1 score: history = 0, current = 78.34932060451712.\n",
      "2024-09-19 16:42:57,479 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Training epoch 0 finished.\n"
     ]
    }
   ],
   "source": [
    "if mode == \"train\":\n",
    "    # Initialize optimizer\n",
    "    optimizer = Optimizer(\n",
    "        filter(lambda p: p.requires_grad, metalog.model.parameters()), lr=gamma\n",
    "    )\n",
    "    global_step = 0\n",
    "    best_f1_score = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        metalog.model.train()\n",
    "        metalog.bk_model.train()\n",
    "        start_time = time.strftime(\"%H:%M:%S\")\n",
    "        logger.info(\n",
    "            f\"Starting epoch: {epoch} | phase: train | start time: {start_time} | learning rate: {optimizer.lr}.\"\n",
    "        )\n",
    "\n",
    "        batch_num_train = int(np.ceil(len(labeled_train_HDFS) / float(batch_size)))\n",
    "        batch_num_test = int(np.ceil(len(labeled_train_BGL) / float(batch_size)))\n",
    "        total_batches = max(batch_num_train, batch_num_test)\n",
    "\n",
    "        meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "        meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "\n",
    "        for _ in range(total_batches):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Meta-train on HDFS\n",
    "            try:\n",
    "                meta_train_batch = next(meta_train_loader)\n",
    "            except StopIteration:\n",
    "                meta_train_batch = data_iter(labeled_train_BGL, batch_size, True)\n",
    "                meta_train_batch = next(meta_train_loader)\n",
    "                \n",
    "            tinst_train = generate_tinsts_binary_label(meta_train_batch, vocab)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_train.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_train.to_mps(DEVICE)\n",
    "            train_loss = metalog.forward(tinst_train.inputs, tinst_train.targets)\n",
    "            train_loss_value = train_loss.data.cpu().numpy()\n",
    "            train_loss.backward(retain_graph=True)\n",
    "\n",
    "            # Update backup model\n",
    "            if torch.cuda.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .cuda()\n",
    "                )\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .to(DEVICE)\n",
    "                )\n",
    "            else:\n",
    "                metalog.bk_model = get_updated_network(\n",
    "                    metalog.model, metalog.bk_model, alpha\n",
    "                ).train()\n",
    "\n",
    "            # Meta-test on BGL\n",
    "            try:\n",
    "                meta_test_batch = next(meta_test_loader)\n",
    "            except StopIteration:\n",
    "                meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "                meta_test_batch = next(meta_test_loader)\n",
    "                \n",
    "            tinst_test = generate_tinsts_binary_label(meta_test_batch, vocab_BGL)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_test.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_test.to_mps(DEVICE)\n",
    "            test_loss = beta * metalog.bk_forward(tinst_test.inputs, tinst_test.targets)\n",
    "            test_loss_value = test_loss.data.cpu().numpy() / beta\n",
    "            test_loss.backward()\n",
    "\n",
    "            # Update the model\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                logger.info(\n",
    "                    f\"Step: {global_step} | Epoch: {epoch} | Meta-train loss: {train_loss_value} | Meta-test loss: {test_loss_value}.\"\n",
    "                )\n",
    "\n",
    "        # Evaluate and save the model\n",
    "        if final_train_BGL and final_train_HDFS:\n",
    "            metalog.evaluate(\"Train\", final_train_BGL + final_train_HDFS)\n",
    "\n",
    "        if final_test_BGL:\n",
    "            _, _, f1_score = metalog.evaluate(\"Test\", final_test_BGL)\n",
    "            if f1_score > best_f1_score:\n",
    "                logger.info(\n",
    "                    f\"Exceed best F1 score: history = {best_f1_score}, current = {f1_score}.\"\n",
    "                )\n",
    "                torch.save(metalog.model.state_dict(), best_model_file)\n",
    "                best_f1_score = f1_score\n",
    "\n",
    "        logger.info(f\"Training epoch {epoch} finished.\")\n",
    "        torch.save(metalog.model.state_dict(), last_model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 16:42:57,566 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: === Evaluating Final Model ===\n",
      "/var/folders/45/0lfy6jf14g3cptljv3x_x53r0000gn/T/ipykernel_61424/658566474.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metalog.model.load_state_dict(torch.load(last_model_file))\n",
      "2024-09-19 16:42:57,599 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start evaluating Final Model on Test BGL by threshold 0.5\n",
      "2024-09-19 16:43:25,702 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Final Model on Test BGL: F1 score = 78.34932060451712 | Precision = 66.5647430748102 | Recall = 95.20421021421689\n",
      "2024-09-19 16:43:25,703 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: === Evaluating Best Model ===\n",
      "/var/folders/45/0lfy6jf14g3cptljv3x_x53r0000gn/T/ipykernel_61424/658566474.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metalog.model.load_state_dict(torch.load(best_model_file))\n",
      "2024-09-19 16:43:25,716 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Start evaluating Best Model on Test BGL by threshold 0.5\n",
      "2024-09-19 16:43:54,560 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: Best Model on Test BGL: F1 score = 78.34932060451712 | Precision = 66.5647430748102 | Recall = 95.20421021421689\n",
      "2024-09-19 16:43:54,562 - MetaLog - SESSION_eeeae78028ec72ad624cb44432770212 - INFO: All evaluations finished!\n"
     ]
    }
   ],
   "source": [
    "# Check if the last model file exists and evaluate it\n",
    "if os.path.exists(last_model_file):\n",
    "    logger.info(\"=== Evaluating Final Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(last_model_file))\n",
    "    metalog.evaluate(\"Final Model on Test BGL\", final_test_BGL)\n",
    "\n",
    "# Check if the best model file exists and evaluate it\n",
    "if os.path.exists(best_model_file):\n",
    "    logger.info(\"=== Evaluating Best Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(best_model_file))\n",
    "    metalog.evaluate(\"Best Model on Test BGL\", final_test_BGL)\n",
    "\n",
    "logger.info(\"All evaluations finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export to graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Constanst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATISTICS_TEMPLATE_LOG_PATH = \"logs/Statistics_Template.log\"\n",
    "METALOG_LOG_PATH = \"logs/MetaLog.log\"\n",
    "LOSS_EPS = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '87.23767691556857 | Precision = 79.94186046511628'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBILATERAL GENERALIZATION TRANSFERRING HDFS TO BGL\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword2vec_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Extract F1 scores and losses\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m train_f1_scores, test_f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mextract_f1_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMETALOG_LOG_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSESSION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m meta_train_losses, meta_test_losses \u001b[38;5;241m=\u001b[39m extract_meta_losses(METALOG_LOG_PATH, SESSION)\n",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m, in \u001b[0;36mextract_f1_scores\u001b[0;34m(log_path, session)\u001b[0m\n\u001b[1;32m     16\u001b[0m test_match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^.+ - MetaLog - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msession\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - INFO: Test: F1 score = (.+) \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, line)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_match:\n\u001b[0;32m---> 19\u001b[0m     train_f1_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_match\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_match:\n\u001b[1;32m     21\u001b[0m     test_f1_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(test_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '87.23767691556857 | Precision = 79.94186046511628'"
     ]
    }
   ],
   "source": [
    "def extract_word2vec_file(log_path, session):\n",
    "    \"\"\"Extract the word2vec file path from the statistics template log.\"\"\"\n",
    "    with open(log_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            match = re.search(rf\"^.+ - Statistics_Template_Encoder - {session} - INFO: Loading word2vec dict from (.+)\\.$\", line)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "def extract_f1_scores(log_path, session):\n",
    "    \"\"\"Extract train and test F1 scores from the MetaLog.\"\"\"\n",
    "    train_f1_scores, test_f1_scores = [], []\n",
    "    with open(log_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            train_match = re.search(rf\"^.+ - MetaLog - {session} - INFO: Train: F1 score = (.+) \\|\", line)\n",
    "            test_match = re.search(rf\"^.+ - MetaLog - {session} - INFO: Test: F1 score = (.+) \\|\", line)\n",
    "\n",
    "            if train_match:\n",
    "                train_f1_scores.append(float(train_match.group(1)))\n",
    "            if test_match:\n",
    "                test_f1_scores.append(float(test_match.group(1)))\n",
    "    return train_f1_scores, test_f1_scores\n",
    "\n",
    "def extract_meta_losses(log_path, session):\n",
    "    \"\"\"Extract meta-train and meta-test losses from the MetaLog.\"\"\"\n",
    "    meta_train_losses, meta_test_losses = [], []\n",
    "    with open(log_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            match = re.search(rf\"^.* - MetaLog - {session} - INFO: Step: .+ \\| Meta-train loss: (.+) \\| Meta-test loss: (.+)\\.$\", line)\n",
    "            if match:\n",
    "                meta_train_losses.append(float(match.group(1)))\n",
    "                meta_test_losses.append(float(match.group(2)))\n",
    "\n",
    "    return meta_train_losses, meta_test_losses\n",
    "\n",
    "# Extract the word2vec file path\n",
    "word2vec_file = extract_word2vec_file(STATISTICS_TEMPLATE_LOG_PATH, SESSION)\n",
    "title = f\"BILATERAL GENERALIZATION TRANSFERRING HDFS TO BGL\\n(using {word2vec_file})\\n\"\n",
    "\n",
    "# Extract F1 scores and losses\n",
    "train_f1_scores, test_f1_scores = extract_f1_scores(METALOG_LOG_PATH, SESSION)\n",
    "meta_train_losses, meta_test_losses = extract_meta_losses(METALOG_LOG_PATH, SESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_scores(ax, num_epochs, train_f1, test_f1):\n",
    "    \"\"\"Plot train and test F1 scores on the provided axis.\"\"\"\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.plot(num_epochs, train_f1, color=\"tab:blue\")\n",
    "    ax.plot(num_epochs, test_f1, color=\"tab:orange\")\n",
    "    ax.legend([\"Train\", \"Test\"])\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"F1 Score\")\n",
    "\n",
    "    for i in range(len(num_epochs)):\n",
    "        ax.plot(num_epochs[i], train_f1[i], \"o\", color=\"tab:blue\", zorder=10)\n",
    "        ax.text(num_epochs[i], train_f1[i] + 5, round(train_f1[i], 2), ha=\"center\")\n",
    "\n",
    "        ax.plot(num_epochs[i], test_f1[i], \"o\", color=\"tab:orange\", zorder=10)\n",
    "        ax.text(num_epochs[i], test_f1[i] - 10, round(test_f1[i], 2), ha=\"center\")\n",
    "\n",
    "def plot_meta_losses(ax, num_steps, meta_train_losses, meta_test_losses):\n",
    "    \"\"\"Plot meta-train and meta-test losses on the provided axis.\"\"\"\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.plot(num_steps, meta_train_losses, color=\"tab:blue\")\n",
    "    ax.plot(num_steps, meta_test_losses, color=\"tab:orange\")\n",
    "    ax.legend([\"Meta-train loss\", \"Meta-test loss\"])\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    # min_test_loss = min(meta_test_losses)\n",
    "\n",
    "    # for i in range(0, len(num_steps), 2):\n",
    "    #     ax.plot(num_steps[i], meta_train_losses[i], \"o\", color=\"tab:blue\", zorder=10)\n",
    "    #     ax.text(num_steps[i], meta_train_losses[i] + LOSS_EPS, round(meta_train_losses[i], 4), ha=\"center\")\n",
    "\n",
    "    #     ax.plot(num_steps[i], meta_test_losses[i], \"o\", color=\"tab:orange\", zorder=10)\n",
    "    #     ax.text(num_steps[i], meta_test_losses[i] + LOSS_EPS, round(meta_test_losses[i], 4), color=\"red\" if meta_test_losses[i] == min_test_loss else \"black\", ha=\"center\")\n",
    "\n",
    "# Plot F1 scores and losses\n",
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "num_epochs = list(range(len(train_f1_scores)))\n",
    "num_steps = [i * 10 for i in range(len(meta_train_losses))]\n",
    "\n",
    "plot_f1_scores(axs[0], num_epochs, train_f1_scores, test_f1_scores)\n",
    "plot_meta_losses(axs[1], num_steps, meta_train_losses, meta_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. Saving and exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the title for the plot\n",
    "best_test_f1_score = max(test_f1_scores)\n",
    "fig_title = f\"{title}\\nBest model F1 Score = {best_test_f1_score}\"\n",
    "fig.suptitle(fig_title)\n",
    "\n",
    "# Define the path to save the plot\n",
    "plot_dir = os.path.join(\"visualization\", \"graphs\")\n",
    "plot_filename = f\"{word2vec_file}-{SESSION}.png\"\n",
    "plot_path = os.path.join(plot_dir, plot_filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# Save the plot\n",
    "fig.savefig(plot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
