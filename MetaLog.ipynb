{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from CONSTANTS import DEVICE, LOG_ROOT, PROJECT_ROOT, SESSION\n",
    "from models.gru import AttGRUModel\n",
    "from module.Common import batch_variable_inst, data_iter, generate_tinsts_binary_label\n",
    "from module.Optimizer import Optimizer\n",
    "from preprocessing.AutoLabeling import Probabilistic_Labeling\n",
    "from preprocessing.datacutter.SimpleCutting import cut_by\n",
    "from preprocessing.Preprocess import Preprocessor\n",
    "from representations.sequences.statistics import Sequential_TF\n",
    "from representations.templates.statistics import (\n",
    "    Simple_template_TF_IDF,\n",
    "    Template_TF_IDF_without_clean,\n",
    ")\n",
    "from utils.Vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom params\n",
    "lstm_hiddens = 64\n",
    "num_layer = 4\n",
    "batch_size = 100\n",
    "drop_out = 0.1\n",
    "epochs = 10\n",
    "\n",
    "word2vec_file = \"glove.840B.300d.txt\"\n",
    "dim = 300\n",
    "alpha = 2e-3\n",
    "beta = 1\n",
    "gamma = 2e-3\n",
    "\n",
    "\n",
    "parser = \"IBM\"\n",
    "mode = \"train\"\n",
    "min_cluster_size = 100\n",
    "min_samples = 100\n",
    "reduce_dimension = 50\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for updating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_network(old, new, lr, load=False):\n",
    "    updated_theta = {}\n",
    "    state_dicts = old.state_dict()\n",
    "    param_dicts = dict(old.named_parameters())\n",
    "\n",
    "    for i, (k, v) in enumerate(state_dicts.items()):\n",
    "        if k in param_dicts.keys() and param_dicts[k].grad is not None:\n",
    "            updated_theta[k] = param_dicts[k] - lr * param_dicts[k].grad\n",
    "        else:\n",
    "            updated_theta[k] = state_dicts[k]\n",
    "    if load:\n",
    "        new.load_state_dict(updated_theta)\n",
    "    else:\n",
    "        new = put_theta(new, updated_theta)\n",
    "    return new\n",
    "\n",
    "\n",
    "def put_theta(model, theta):\n",
    "    def k_param_fn(tmp_model, name=None):\n",
    "        if len(tmp_model._modules) != 0:\n",
    "            for k, v in tmp_model._modules.items():\n",
    "                if name is None:\n",
    "                    k_param_fn(v, name=str(k))\n",
    "                else:\n",
    "                    k_param_fn(v, name=str(name + \".\" + k))\n",
    "        else:\n",
    "            for k, v in tmp_model._parameters.items():\n",
    "                if not isinstance(v, torch.Tensor):\n",
    "                    continue\n",
    "                tmp_model._parameters[k] = theta[str(name + \".\" + k)]\n",
    "\n",
    "    k_param_fn(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaLog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLog:\n",
    "    def __init__(self, vocab, num_layer, hidden_size, drop_out, label2id):\n",
    "        self.label2id = label2id\n",
    "        self.vocab = vocab\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1024\n",
    "        self.drop_out = drop_out\n",
    "        self.model = AttGRUModel(vocab, self.num_layer, self.hidden_size, self.drop_out)\n",
    "        self.bk_model = AttGRUModel(\n",
    "            vocab, self.num_layer, self.hidden_size, self.drop_out, is_backup=True\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda(DEVICE)\n",
    "            self.bk_model = self.bk_model.cuda(DEVICE)\n",
    "        elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "            self.model = self.model.to(DEVICE)\n",
    "            self.bk_model = self.bk_model.to(DEVICE)\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        tag_logits = self.model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def bk_forward(self, inputs, targets):\n",
    "        tag_logits = self.bk_model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, threshold=None):\n",
    "        with torch.no_grad():\n",
    "            tag_logits = self.model(inputs)\n",
    "            tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        if threshold is not None:\n",
    "            probs = tag_logits.detach().cpu().numpy()\n",
    "            anomaly_id = self.label2id[\"Anomalous\"]\n",
    "            pred_tags = np.zeros(probs.shape[0])\n",
    "            for i, logits in enumerate(probs):\n",
    "                if logits[anomaly_id] >= threshold:\n",
    "                    pred_tags[i] = anomaly_id\n",
    "                else:\n",
    "                    pred_tags[i] = 1 - anomaly_id\n",
    "\n",
    "        else:\n",
    "            pred_tags = tag_logits.detach().max(1)[1].cpu()\n",
    "        return pred_tags, tag_logits\n",
    "\n",
    "    def evaluate(self, dataset, instances, threshold=0.5):\n",
    "        logger.info(f\"Start evaluating {dataset} by threshold {threshold}\")\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            globalBatchNum = 0\n",
    "            TP, TN, FP, FN = 0, 0, 0, 0\n",
    "            tag_correct, tag_total = 0, 0\n",
    "            for onebatch in data_iter(instances, self.test_batch_size, False):\n",
    "                tinst = generate_tinsts_binary_label(onebatch, vocab_BGL, False)\n",
    "                if torch.cuda.is_available():\n",
    "                    tinst.to_cuda(DEVICE)\n",
    "                elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                    tinst.to_mps(DEVICE)\n",
    "                self.model.eval()\n",
    "                pred_tags, tag_logits = self.predict(tinst.inputs, threshold)\n",
    "                for inst, bmatch in batch_variable_inst(\n",
    "                    onebatch, pred_tags, tag_logits, processor_BGL.id2tag\n",
    "                ):\n",
    "                    tag_total += 1\n",
    "                    if bmatch:\n",
    "                        tag_correct += 1\n",
    "                        if inst.label == \"Normal\":\n",
    "                            TN += 1\n",
    "                        else:\n",
    "                            TP += 1\n",
    "                    else:\n",
    "                        if inst.label == \"Normal\":\n",
    "                            FP += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                globalBatchNum += 1\n",
    "            if TP + FP != 0:\n",
    "                precision = 100 * TP / (TP + FP)\n",
    "                recall = 100 * TP / (TP + FN)\n",
    "                f1_score = 2 * precision * recall / (precision + recall)\n",
    "                # fpr = 100 * FP / (FP + TN)\n",
    "                logger.info(\n",
    "                    f\"{dataset}: F1 score = {f1_score} | Precision = {precision} | Recall = {recall})\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"{dataset}: F1 score = {0} | Precision = {0} | Recall = {0}\"\n",
    "                )\n",
    "                precision, recall, f1_score = 0, 0, 0\n",
    "        return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 00:12:47,181 - MetaLog - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"MetaLog\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler(sys.stderr)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    "    )\n",
    ")\n",
    "file_handler = logging.FileHandler(os.path.join(LOG_ROOT, \"MetaLog.log\"))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    "    )\n",
    ")\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "logger.info(\n",
    "    f\"Construct logger for MetaLog succeeded, current working directory: {os.getcwd()}, logs will be written in {LOG_ROOT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import BGL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196017/2196017 [01:14<00:00, 29524.33it/s]\n",
      "2024-09-14 00:18:40,768 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 00:18:40,774 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/BGL/raw_log_seqs.txt\n",
      "100%|██████████| 85577/85577 [00:01<00:00, 43702.64it/s] \n",
      "2024-09-14 00:18:43,436 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Extraction finished successfully.\n",
      "2024-09-14 00:18:43,437 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 00:18:43,438 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/BGL.ini\n",
      "2024-09-14 00:18:43,447 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/persistence\n",
      "2024-09-14 00:18:43,470 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Persistence file found, loading.\n",
      "2024-09-14 00:18:43,794 - drain - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded.\n",
      "2024-09-14 00:18:43,795 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_event_mapping.dict ... True\n",
      "2024-09-14 00:18:43,795 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_sequences.txt ... True\n",
      "2024-09-14 00:18:43,796 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start loading previous parsing results.\n",
      "2024-09-14 00:18:45,428 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded 4747963 log sequences and their mappings.\n",
      "2024-09-14 00:18:46,053 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Loaded 85577 blocks\n",
      "2024-09-14 00:18:46,054 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Finished in 2.26\n",
      "2024-09-14 00:18:46,255 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/templates.vec\n",
      "2024-09-14 00:18:46,255 - BGLLoader - SESSION_7a123d0cc10c1408932518b309616721 - INFO: All data preparation finished in 2.46\n",
      "2024-09-14 00:18:46,256 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start preprocessing dataset BGL by parsing method IBM\n",
      "2024-09-14 00:18:46,258 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start generating instances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciod ciodb ciod ciod fefff fefff ciod ciod ciod edram edra regctl scancom miscompare sernum mpgood fffa ccbc bglmaster ciod ciod ebda sernum ciod pgood pgood mpgood mpgood pgood pgood mpgood mpgood ciod ciod softheader ciod ciod fabd abdd fabdd fabda feaa feaa mmcs pgood pgood mpgood mpgood idoproxydb microloader ffffc idoproxydb idomarshalerio *& idoproxydb idomarshalerio x+ y+ z+ x+ y+ z+ x+ y+ z+ x+ y+ z+ x+ y+ z+ x+ softheader pixf softheader pixf softheader pixf mmcs mmcs feaa feaa feaa feaa feaa cdadc cdadc baae afcc feea feea feea eaca pgood pgood mpgood mpgood pgood pgood mpgood mpgood feaa feaa `personality >version bglpersonality version' `void efcd efcd efb softheader prxf pixf ecaf pgood fffc fpscr dbcr dbsr cnvt ciod bgldevices bgldevices \u0000 ciod ciod softheader ciod `vaddr `int "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85577/85577 [00:00<00:00, 153320.60it/s]\n",
      "2024-09-14 00:18:46,838 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Dev: 4371 Normal, 4186 Anomalous instances.\n",
      "2024-09-14 00:18:46,846 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Train: 13382 Normal, 132 Anomalous instances.\n",
      "2024-09-14 00:18:46,846 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Test: 31521 Normal, 19826 Anomalous instances.\n",
      "2024-09-14 00:18:47,200 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-14 00:18:47,269 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Embed size: 178 in pre dataset.\n",
      "2024-09-14 00:18:47,269 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-14 00:18:47,378 - Preprocessor - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Embed size: 383 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 00:18:49,956 - Solitary_HDBSCAN - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for Solitary_HDBSCAN succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 00:18:49,957 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Construct logger for Probabilistic labeling succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 00:18:49,957 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Found previous labeled file, will load and continue to accelerate the process.\n",
      "2024-09-14 00:18:49,958 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Start load previous clustered results from /Users/minhthienlongvo/research/MetaLog/outputs/results/MetaLog/BGL_IBM/prob_label_res/mcs-100_ms-100\n",
      "2024-09-14 00:18:49,959 - Prob_Label - SESSION_7a123d0cc10c1408932518b309616721 - WARNING: Please NOTE that this may cause some problem due to incomplete cluster settings.\n",
      "2024-09-14 00:18:49,973 - Vocab - SESSION_7a123d0cc10c1408932518b309616721 - INFO: Total words: 436\n",
      "2024-09-14 00:18:49,973 - Vocab - SESSION_7a123d0cc10c1408932518b309616721 - INFO: The dim of pretrained embeddings: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 1.26.\n"
     ]
    }
   ],
   "source": [
    "dataset = \"BGL\"\n",
    "\n",
    "# # Mark results saving directories.\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "prob_label_res_file_BGL = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\"\n",
    "    + dataset\n",
    "    + \"_\"\n",
    "    + parser\n",
    "    + \"/prob_label_res/mcs-\"\n",
    "    + str(min_cluster_size)\n",
    "    + \"_ms-\"\n",
    "    + str(min_samples),\n",
    ")\n",
    "rand_state_BGL = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\" + dataset + \"_\" + parser + \"/prob_label_res/random_state\",\n",
    ")\n",
    "\n",
    "output_model_dir = os.path.join(\n",
    "    save_dir, \"models/MetaLog/\" + dataset + \"_\" + parser + \"/model\"\n",
    ")\n",
    "output_res_dir = os.path.join(\n",
    "    save_dir, \"results/MetaLog/\" + dataset + \"_\" + parser + \"/detect_res\"\n",
    ")\n",
    "\n",
    "# Training, Validating and Testing instances.\n",
    "template_encoder_BGL = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    "    if dataset == \"NC\"\n",
    "    else Simple_template_TF_IDF(word2vec_file)\n",
    ")\n",
    "processor_BGL = Preprocessor()\n",
    "train_BGL, dev_BGL, test_BGL = processor_BGL.process(\n",
    "    dataset=dataset,\n",
    "    parsing=parser,\n",
    "    cut_func=cut_by(0.3, 0.1, 0.01),\n",
    "    template_encoding=template_encoder_BGL.present,\n",
    ")\n",
    "\n",
    "# Log sequence representation.\n",
    "sequential_encoder_BGL = Sequential_TF(processor_BGL.embedding)\n",
    "train_reprs_BGL = sequential_encoder_BGL.present(train_BGL)\n",
    "for index, inst in enumerate(train_BGL):\n",
    "    inst.repr = train_reprs_BGL[index]\n",
    "test_reprs_BGL = sequential_encoder_BGL.present(test_BGL)\n",
    "for index, inst in enumerate(test_BGL):\n",
    "    inst.repr = test_reprs_BGL[index]\n",
    "\n",
    "# Dimension reduction if specified.\n",
    "transformer_BGL = None\n",
    "if reduce_dimension != -1:\n",
    "    start_time = time.time()\n",
    "    print(f\"Start FastICA, target dimension: {reduce_dimension}.\")\n",
    "    transformer_BGL = FastICA(n_components=reduce_dimension)\n",
    "    train_reprs_BGL = transformer_BGL.fit_transform(train_reprs_BGL)\n",
    "    for idx, inst in enumerate(train_BGL):\n",
    "        inst.repr = train_reprs_BGL[idx]\n",
    "    print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "\n",
    "# Probabilistic labeling.\n",
    "# Sample normal instances.\n",
    "train_normal_BGL = [x for x, inst in enumerate(train_BGL) if inst.label == \"Normal\"]\n",
    "normal_ids_BGL = train_normal_BGL[: int(0.5 * len(train_normal_BGL))]\n",
    "label_generator_BGL = Probabilistic_Labeling(\n",
    "    min_samples=min_samples,\n",
    "    min_clust_size=min_cluster_size,\n",
    "    res_file=prob_label_res_file_BGL,\n",
    "    rand_state_file=rand_state_BGL,\n",
    ")\n",
    "labeled_train_BGL = label_generator_BGL.auto_label(train_BGL, normal_ids_BGL)\n",
    "\n",
    "# Load Embeddings\n",
    "vocab_BGL = Vocab()\n",
    "vocab_BGL.load_from_dict(processor_BGL.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import HDFS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 921404/2196017 [00:39<00:51, 24681.57it/s]"
     ]
    }
   ],
   "source": [
    "dataset = \"HDFS\"\n",
    "# Mark results saving directories.\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "prob_label_res_file_HDFS = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\"\n",
    "    + dataset\n",
    "    + \"_\"\n",
    "    + parser\n",
    "    + \"/prob_label_res/mcs-\"\n",
    "    + str(min_cluster_size)\n",
    "    + \"_ms-\"\n",
    "    + str(min_samples),\n",
    ")\n",
    "rand_state_HDFS = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\" + dataset + \"_\" + parser + \"/prob_label_res/random_state\",\n",
    ")\n",
    "\n",
    "# Training, Validating and Testing instances.\n",
    "template_encoder_HDFS = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    "    if dataset == \"NC\"\n",
    "    else Simple_template_TF_IDF(word2vec_file)\n",
    ")\n",
    "processor_HDFS = Preprocessor()\n",
    "train_HDFS, _, _ = processor_HDFS.process(\n",
    "    dataset=dataset,\n",
    "    parsing=parser,\n",
    "    cut_func=cut_by(0.3, 0.1),\n",
    "    template_encoding=template_encoder_HDFS.present,\n",
    ")\n",
    "\n",
    "# Log sequence representation.\n",
    "sequential_encoder_HDFS = Sequential_TF(processor_HDFS.embedding)\n",
    "train_reprs_HDFS = sequential_encoder_HDFS.present(train_HDFS)\n",
    "for index, inst in enumerate(train_HDFS):\n",
    "    inst.repr = train_reprs_HDFS[index]\n",
    "\n",
    "# Dimension reduction if specified.\n",
    "transformer_HDFS = None\n",
    "if reduce_dimension != -1:\n",
    "    start_time = time.time()\n",
    "    print(f\"Start FastICA, target dimension: {reduce_dimension}.\")\n",
    "    transformer_HDFS = FastICA(n_components=reduce_dimension)\n",
    "    train_reprs_HDFS = transformer_HDFS.fit_transform(train_reprs_HDFS)\n",
    "    for idx, inst in enumerate(train_HDFS):\n",
    "        inst.repr = train_reprs_HDFS[idx]\n",
    "    print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "\n",
    "labeled_train_HDFS = train_HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate vocab and label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab()\n",
    "new_embedding = {}\n",
    "for key in processor_BGL.embedding.keys():\n",
    "    new_embedding[key] = processor_BGL.embedding[key]\n",
    "for key in processor_HDFS.embedding.keys():\n",
    "    new_embedding[key + 432] = processor_HDFS.embedding[key]\n",
    "# Load Embeddings\n",
    "vocab_HDFS = Vocab()\n",
    "vocab_HDFS.load_from_dict(processor_HDFS.embedding)\n",
    "vocab.load_from_dict(new_embedding)\n",
    "\n",
    "metalog = MetaLog(\n",
    "    vocab=vocab,\n",
    "    num_layer=num_layer,\n",
    "    hidden_size=lstm_hiddens,\n",
    "    drop_out=drop_out,\n",
    "    label2id=processor_BGL.label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log custom params\n",
    "logger.info(\n",
    "    f\"Custom params: alpha = {alpha} | beta = {beta} | gamma = {gamma} | word2vec_file = {word2vec_file}.\"\n",
    ")\n",
    "\n",
    "# meta learning\n",
    "log = \"layer={}_hidden={}_epoch={}\".format(num_layer, lstm_hiddens, epochs)\n",
    "best_model_file = os.path.join(output_model_dir, log + \"_best.pt\")\n",
    "last_model_file = os.path.join(output_model_dir, log + \"_last.pt\")\n",
    "if not os.path.exists(output_model_dir):\n",
    "    os.makedirs(output_model_dir)\n",
    "if mode == \"train\":\n",
    "    # Train\n",
    "    optimizer = Optimizer(\n",
    "        filter(lambda p: p.requires_grad, metalog.model.parameters()), lr=gamma\n",
    "    )\n",
    "    global_step = 0\n",
    "    best_f1_score = 0\n",
    "    for epoch in range(epochs):\n",
    "        metalog.model.train()\n",
    "        metalog.bk_model.train()\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        logger.info(\n",
    "            f\"Starting epoch: {epoch} | phase: train | start time: {start} | learning rate: {optimizer.lr}.\"\n",
    "        )\n",
    "\n",
    "        batch_num = int(np.ceil(len(labeled_train_HDFS) / float(batch_size)))\n",
    "        batch_iter = 0\n",
    "        batch_num_test = int(np.ceil(len(labeled_train_BGL) / float(batch_size)))\n",
    "        batch_iter_test = 0\n",
    "        total_bn = max(batch_num, batch_num_test)\n",
    "        meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "        meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "\n",
    "        for i in range(total_bn):\n",
    "            optimizer.zero_grad()\n",
    "            # meta train\n",
    "            meta_train_batch = meta_train_loader.__next__()\n",
    "            meta_test_batch = meta_test_loader.__next__()\n",
    "            tinst_tr = generate_tinsts_binary_label(meta_train_batch, vocab_HDFS)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_tr.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_tr.to_mps(DEVICE)\n",
    "            loss = metalog.forward(tinst_tr.inputs, tinst_tr.targets)\n",
    "            loss_value = loss.data.cpu().numpy()\n",
    "            loss.backward(retain_graph=True)\n",
    "            batch_iter += 1\n",
    "            if torch.cuda.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .cuda()\n",
    "                )\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .to(DEVICE)\n",
    "                )\n",
    "            else:\n",
    "                metalog.bk_model = get_updated_network(\n",
    "                    metalog.model, metalog.bk_model, alpha\n",
    "                ).train()\n",
    "            # meta test\n",
    "            tinst_test = generate_tinsts_binary_label(meta_test_batch, vocab_BGL)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_test.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_test.to_mps(DEVICE)\n",
    "            loss_te = beta * metalog.bk_forward(\n",
    "                tinst_test.inputs, tinst_test.targets\n",
    "            )\n",
    "            loss_value_te = loss_te.data.cpu().numpy() / beta\n",
    "            loss_te.backward()\n",
    "            batch_iter_test += 1\n",
    "            # aggregate\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            if global_step % 500 == 0:\n",
    "                logger.info(\n",
    "                    f\"Step: {global_step} | Epoch: {epoch} | Meta-train loss: {loss_value} | Meta-test loss: {loss_value_te}.\"\n",
    "                )\n",
    "            if batch_iter == batch_num:\n",
    "                meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "                batch_iter = 0\n",
    "            if batch_iter_test == batch_num_test:\n",
    "                meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "                batch_iter_test = 0\n",
    "\n",
    "        if train_BGL:\n",
    "            metalog.evaluate(\"Train BGL\", train_BGL)\n",
    "\n",
    "        if dev_BGL:\n",
    "            metalog.evaluate(\"Dev BGL\", dev_BGL)\n",
    "\n",
    "        if test_BGL:\n",
    "            _, _, f1_score = metalog.evaluate(\"Test BGL\", test_BGL)\n",
    "\n",
    "            if f1_score > best_f1_score:\n",
    "                logger.info(\n",
    "                    f\"Exceed best F1 score: history = {best_f1_score}, current = {f1_score}.\"\n",
    "                )\n",
    "                torch.save(metalog.model.state_dict(), best_model_file)\n",
    "                best_f1_score = f1_score\n",
    "\n",
    "        logger.info(f\"Training epoch {epoch} finished.\")\n",
    "        torch.save(metalog.model.state_dict(), last_model_file)\n",
    "\n",
    "if os.path.exists(last_model_file):\n",
    "    logger.info(\"=== Final Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(last_model_file))\n",
    "    metalog.evaluate(test_BGL, threshold)\n",
    "if os.path.exists(best_model_file):\n",
    "    logger.info(\"=== Best Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(best_model_file))\n",
    "    metalog.evaluate(test_BGL, threshold)\n",
    "logger.info(\"All Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
