{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:56:32,631 - AttGRU - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Attention-Based GRU succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:56:33,003 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:56:33,005 - StatisticsRepresentation. - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Statistics Representation succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:56:33,006 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Statistics Template Encoder succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:56:33,011 - Vocab - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Vocab succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from CONSTANTS import DEVICE, LOG_ROOT, PROJECT_ROOT, SESSION\n",
    "from models.gru import AttGRUModel\n",
    "from module.Common import batch_variable_inst, data_iter, generate_tinsts_binary_label\n",
    "from module.Optimizer import Optimizer\n",
    "from preprocessing.AutoLabeling import Probabilistic_Labeling\n",
    "from preprocessing.datacutter.SimpleCutting import cut_by\n",
    "from preprocessing.Preprocess import Preprocessor\n",
    "from representations.sequences.statistics import Sequential_TF\n",
    "from representations.templates.statistics import (\n",
    "    Simple_template_TF_IDF,\n",
    "    Template_TF_IDF_without_clean,\n",
    ")\n",
    "from utils.Vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom params\n",
    "lstm_hiddens = 64\n",
    "num_layer = 4\n",
    "batch_size = 100\n",
    "drop_out = 0.2\n",
    "epochs = 10\n",
    "\n",
    "word2vec_file = \"glove.42B.300d.txt\"\n",
    "dim = 300\n",
    "alpha = 2e-3\n",
    "beta = 2\n",
    "gamma = 2e-3\n",
    "\n",
    "\n",
    "parser = \"IBM\"\n",
    "mode = \"train\"\n",
    "min_cluster_size = 100\n",
    "min_samples = 100\n",
    "reduce_dimension = 50\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function for updating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_network(old, new, lr, load=False):\n",
    "    updated_theta = {}\n",
    "    state_dicts = old.state_dict()\n",
    "    param_dicts = dict(old.named_parameters())\n",
    "\n",
    "    for i, (k, v) in enumerate(state_dicts.items()):\n",
    "        if k in param_dicts.keys() and param_dicts[k].grad is not None:\n",
    "            updated_theta[k] = param_dicts[k] - lr * param_dicts[k].grad\n",
    "        else:\n",
    "            updated_theta[k] = state_dicts[k]\n",
    "    if load:\n",
    "        new.load_state_dict(updated_theta)\n",
    "    else:\n",
    "        new = put_theta(new, updated_theta)\n",
    "    return new\n",
    "\n",
    "\n",
    "def put_theta(model, theta):\n",
    "    def k_param_fn(tmp_model, name=None):\n",
    "        if len(tmp_model._modules) != 0:\n",
    "            for k, v in tmp_model._modules.items():\n",
    "                if name is None:\n",
    "                    k_param_fn(v, name=str(k))\n",
    "                else:\n",
    "                    k_param_fn(v, name=str(name + \".\" + k))\n",
    "        else:\n",
    "            for k, v in tmp_model._parameters.items():\n",
    "                if not isinstance(v, torch.Tensor):\n",
    "                    continue\n",
    "                tmp_model._parameters[k] = theta[str(name + \".\" + k)]\n",
    "\n",
    "    k_param_fn(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MetaLog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLog:\n",
    "    def __init__(self, vocab, num_layer, hidden_size, drop_out, label2id):\n",
    "        self.label2id = label2id\n",
    "        self.vocab = vocab\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1024\n",
    "        self.drop_out = drop_out\n",
    "        self.model = AttGRUModel(vocab, self.num_layer, self.hidden_size, self.drop_out)\n",
    "        self.bk_model = AttGRUModel(\n",
    "            vocab, self.num_layer, self.hidden_size, self.drop_out, is_backup=True\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda(DEVICE)\n",
    "            self.bk_model = self.bk_model.cuda(DEVICE)\n",
    "        elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "            self.model = self.model.to(DEVICE)\n",
    "            self.bk_model = self.bk_model.to(DEVICE)\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        tag_logits = self.model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def bk_forward(self, inputs, targets):\n",
    "        tag_logits = self.bk_model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, threshold=None):\n",
    "        with torch.no_grad():\n",
    "            tag_logits = self.model(inputs)\n",
    "            tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        if threshold is not None:\n",
    "            probs = tag_logits.detach().cpu().numpy()\n",
    "            anomaly_id = self.label2id[\"Anomalous\"]\n",
    "            pred_tags = np.zeros(probs.shape[0])\n",
    "            for i, logits in enumerate(probs):\n",
    "                if logits[anomaly_id] >= threshold:\n",
    "                    pred_tags[i] = anomaly_id\n",
    "                else:\n",
    "                    pred_tags[i] = 1 - anomaly_id\n",
    "\n",
    "        else:\n",
    "            pred_tags = tag_logits.detach().max(1)[1].cpu()\n",
    "        return pred_tags, tag_logits\n",
    "\n",
    "    def evaluate(self, dataset, instances, threshold=0.5):\n",
    "        logger.info(f\"Start evaluating {dataset} by threshold {threshold}\")\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            globalBatchNum = 0\n",
    "            TP, TN, FP, FN = 0, 0, 0, 0\n",
    "            tag_correct, tag_total = 0, 0\n",
    "            for onebatch in data_iter(instances, self.test_batch_size, False):\n",
    "                tinst = generate_tinsts_binary_label(onebatch, vocab_BGL, False)\n",
    "                if torch.cuda.is_available():\n",
    "                    tinst.to_cuda(DEVICE)\n",
    "                elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                    tinst.to_mps(DEVICE)\n",
    "                self.model.eval()\n",
    "                pred_tags, tag_logits = self.predict(tinst.inputs, threshold)\n",
    "                for inst, bmatch in batch_variable_inst(\n",
    "                    onebatch, pred_tags, tag_logits, processor_BGL.id2tag\n",
    "                ):\n",
    "                    tag_total += 1\n",
    "                    if bmatch:\n",
    "                        tag_correct += 1\n",
    "                        if inst.label == \"Normal\":\n",
    "                            TN += 1\n",
    "                        else:\n",
    "                            TP += 1\n",
    "                    else:\n",
    "                        if inst.label == \"Normal\":\n",
    "                            FP += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                globalBatchNum += 1\n",
    "            if TP + FP != 0:\n",
    "                precision = 100 * TP / (TP + FP)\n",
    "                recall = 100 * TP / (TP + FN)\n",
    "                f1_score = 2 * precision * recall / (precision + recall)\n",
    "                # fpr = 100 * FP / (FP + TN)\n",
    "                logger.info(\n",
    "                    f\"{dataset}: F1 score = {f1_score} | Precision = {precision} | Recall = {recall}\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"{dataset}: F1 score = {0} | Precision = {0} | Recall = {0}\"\n",
    "                )\n",
    "                precision, recall, f1_score = 0, 0, 0\n",
    "        return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logging config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:56:33,042 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"MetaLog\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler(sys.stderr)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    "    )\n",
    ")\n",
    "file_handler = logging.FileHandler(os.path.join(LOG_ROOT, \"MetaLog.log\"))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    "    )\n",
    ")\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "logger.info(\n",
    "    f\"Construct logger for MetaLog succeeded, current working directory: {os.getcwd()}, logs will be written in {LOG_ROOT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Log custom params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:56:33,047 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Network params: lstm_hiddens = 128 | num_layer = 4 | drop_out = 0.5.\n",
      "2024-09-15 06:56:33,048 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Hyper-params: alpha = 0.002 | beta = 2 | gamma = 0.002 | word2vec_file = glove.42B.300d.txt.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Network params: lstm_hiddens = {lstm_hiddens} | num_layer = {num_layer} | drop_out = {drop_out}.\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Hyper-params: alpha = {alpha} | beta = {beta} | gamma = {gamma} | word2vec_file = {word2vec_file}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Import BGL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:56:33,054 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loading word2vec dict from glove.42B.300d.txt.\n",
      "100%|██████████| 1917494/1917494 [00:55<00:00, 34657.57it/s]\n",
      "2024-09-15 06:57:35,233 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Total 1917494 words in glove.42B.300d.txt dict.\n",
      "2024-09-15 06:57:35,264 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:57:35,272 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/BGL/raw_log_seqs.txt\n",
      "100%|██████████| 85577/85577 [00:00<00:00, 104884.56it/s]\n",
      "2024-09-15 06:57:36,261 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Extraction finished successfully.\n",
      "2024-09-15 06:57:36,262 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:57:36,263 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/BGL.ini\n",
      "2024-09-15 06:57:36,268 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/persistence\n",
      "2024-09-15 06:57:36,289 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Persistence file found, loading.\n",
      "2024-09-15 06:57:36,312 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loaded.\n",
      "2024-09-15 06:57:36,313 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_event_mapping.dict ... True\n",
      "2024-09-15 06:57:36,314 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_sequences.txt ... True\n",
      "2024-09-15 06:57:36,314 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start loading previous parsing results.\n",
      "2024-09-15 06:57:37,993 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loaded 4747963 log sequences and their mappings.\n",
      "2024-09-15 06:57:38,519 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loaded 85577 blocks\n",
      "2024-09-15 06:57:38,519 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Finished in 2.21\n",
      "2024-09-15 06:57:38,552 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Found 1121 tokens in 432 log templates\n",
      "2024-09-15 06:57:38,614 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: OOV Rate: 0.09636724681696064\n",
      "2024-09-15 06:57:38,666 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/templates.vec\n",
      "2024-09-15 06:57:38,667 - BGLLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: All data preparation finished in 2.35\n",
      "2024-09-15 06:57:38,667 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start preprocessing dataset BGL by parsing method IBM\n",
      "2024-09-15 06:57:38,667 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start generating instances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * ciod * * * * * * * * * * * * * * * * * * * * * * ciodb ciod * * * * * * * * ciod * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * fefff * fefff * * * * * * * * * * * * * ciod * * * * * ciod * * * * * * * * * ciod * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * regctl miscompare * * * * sernum * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * mpgood * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * bglmaster * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ciod * * * * * * * * * ciod * * * * * * * * * * * * * * * * sernum * * ciod * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * mpgood mpgood * * * * * * * * * * * mpgood mpgood ciod * * * * * * * * * * * * * * * * * * * * * * ciod * softheader * * ciod * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ciod * * * * * * * * * * * * * * fabd abdd fabdd fabda feaa * * feaa * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * mpgood mpgood * * * * * * * * * * * * * * * idoproxydb * * * * microloader ffffc * * idoproxydb idomarshalerio *& idoproxydb idomarshalerio * * * * * * * * * * * * * * * * x+ y+ z+ * * * * x+ y+ z+ * * * * * * * * x+ y+ z+ * * * * * * * * * * * * * * * * * * * x+ y+ z+ * * * * * * * x+ y+ z+ x+ softheader * pixf * * * * * softheader * * pixf softheader * * pixf * * * * * feaa feaa feaa * * feaa feaa cdadc cdadc baae * * * * * * * * * mpgood mpgood mpgood mpgood * feaa feaa * * * * `personality >version bglpersonality version' `void efcd efcd * * * * * softheader * prxf pixf * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * fpscr dbcr * * * * * * * * * * * * * ciod * * * * * * * * * * * * * bgldevices bgldevices \u0000 * ciod * * * * ciod * softheader * * ciod `vaddr `int "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85577/85577 [00:00<00:00, 176389.28it/s]\n",
      "2024-09-15 06:57:39,175 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: 16352 Normal, 110 Anomalous instances.\n",
      "2024-09-15 06:57:39,176 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: 32922 Normal, 26982 Anomalous instances.\n",
      "2024-09-15 06:57:39,511 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-15 06:57:39,590 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Embed size: 120 in pre dataset.\n",
      "2024-09-15 06:57:39,591 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-15 06:57:39,724 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Embed size: 401 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:57:42,330 - Solitary_HDBSCAN - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Solitary_HDBSCAN succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:57:42,331 - Prob_Label - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Probabilistic labeling succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:57:42,332 - Prob_Label - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Found previous labeled file, will load and continue to accelerate the process.\n",
      "2024-09-15 06:57:42,333 - Prob_Label - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start load previous clustered results from /Users/minhthienlongvo/research/MetaLog/outputs/results/MetaLog/BGL_IBM/prob_label_res/mcs-100_ms-100\n",
      "2024-09-15 06:57:42,333 - Prob_Label - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - WARNING: Please NOTE that this may cause some problem due to incomplete cluster settings.\n",
      "2024-09-15 06:57:42,348 - Vocab - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Total words: 436\n",
      "2024-09-15 06:57:42,349 - Vocab - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: The dim of pretrained embeddings: 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 1.08.\n"
     ]
    }
   ],
   "source": [
    "dataset = \"BGL\"\n",
    "\n",
    "# # Mark results saving directories.\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "prob_label_res_file_BGL = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\"\n",
    "    + dataset\n",
    "    + \"_\"\n",
    "    + parser\n",
    "    + \"/prob_label_res/mcs-\"\n",
    "    + str(min_cluster_size)\n",
    "    + \"_ms-\"\n",
    "    + str(min_samples),\n",
    ")\n",
    "rand_state_BGL = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\" + dataset + \"_\" + parser + \"/prob_label_res/random_state\",\n",
    ")\n",
    "\n",
    "output_model_dir = os.path.join(\n",
    "    save_dir, \"models/MetaLog/\" + dataset + \"_\" + parser + \"/model\"\n",
    ")\n",
    "output_res_dir = os.path.join(\n",
    "    save_dir, \"results/MetaLog/\" + dataset + \"_\" + parser + \"/detect_res\"\n",
    ")\n",
    "\n",
    "# Training, Validating and Testing instances.\n",
    "template_encoder_BGL = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    "    if dataset == \"NC\"\n",
    "    else Simple_template_TF_IDF(word2vec_file)\n",
    ")\n",
    "processor_BGL = Preprocessor()\n",
    "train_BGL, _, test_BGL = processor_BGL.process(\n",
    "    dataset=dataset,\n",
    "    parsing=parser,\n",
    "    cut_func=cut_by(0.3, 0., 0.01),\n",
    "    template_encoding=template_encoder_BGL.present,\n",
    ")\n",
    "\n",
    "# Log sequence representation.\n",
    "sequential_encoder_BGL = Sequential_TF(processor_BGL.embedding)\n",
    "train_reprs_BGL = sequential_encoder_BGL.present(train_BGL)\n",
    "for index, inst in enumerate(train_BGL):\n",
    "    inst.repr = train_reprs_BGL[index]\n",
    "test_reprs_BGL = sequential_encoder_BGL.present(test_BGL)\n",
    "for index, inst in enumerate(test_BGL):\n",
    "    inst.repr = test_reprs_BGL[index]\n",
    "\n",
    "# Dimension reduction if specified.\n",
    "transformer_BGL = None\n",
    "if reduce_dimension != -1:\n",
    "    start_time = time.time()\n",
    "    print(f\"Start FastICA, target dimension: {reduce_dimension}.\")\n",
    "    transformer_BGL = FastICA(n_components=reduce_dimension)\n",
    "    train_reprs_BGL = transformer_BGL.fit_transform(train_reprs_BGL)\n",
    "    for idx, inst in enumerate(train_BGL):\n",
    "        inst.repr = train_reprs_BGL[idx]\n",
    "    print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "\n",
    "# Probabilistic labeling.\n",
    "# Sample normal instances.\n",
    "train_normal_BGL = [x for x, inst in enumerate(train_BGL) if inst.label == \"Normal\"]\n",
    "normal_ids_BGL = train_normal_BGL[: int(0.5 * len(train_normal_BGL))]\n",
    "label_generator_BGL = Probabilistic_Labeling(\n",
    "    min_samples=min_samples,\n",
    "    min_clust_size=min_cluster_size,\n",
    "    res_file=prob_label_res_file_BGL,\n",
    "    rand_state_file=rand_state_BGL,\n",
    ")\n",
    "labeled_train_BGL = label_generator_BGL.auto_label(train_BGL, normal_ids_BGL)\n",
    "\n",
    "# Load Embeddings\n",
    "vocab_BGL = Vocab()\n",
    "vocab_BGL.load_from_dict(processor_BGL.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Import HDFS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:57:42,363 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loading word2vec dict from glove.42B.300d.txt.\n",
      "100%|██████████| 1917494/1917494 [00:56<00:00, 33911.56it/s]\n",
      "2024-09-15 06:58:47,051 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Total 1917494 words in glove.42B.300d.txt dict.\n",
      "2024-09-15 06:58:47,067 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:58:47,073 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/raw_log_seqs.txt\n",
      "100%|██████████| 575061/575061 [00:02<00:00, 244238.29it/s]\n",
      "2024-09-15 06:58:49,558 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Extraction finished successfully.\n",
      "2024-09-15 06:58:49,780 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:58:49,780 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:58:49,785 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-15 06:58:49,785 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-15 06:58:49,788 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-15 06:58:49,788 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-15 06:58:49,798 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Persistence file found, loading.\n",
      "2024-09-15 06:58:49,798 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Persistence file found, loading.\n",
      "2024-09-15 06:58:49,804 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loaded.\n",
      "2024-09-15 06:58:49,804 - drain - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loaded.\n",
      "2024-09-15 06:58:49,805 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_event_mapping.dict ... True\n",
      "2024-09-15 06:58:49,806 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_sequences.txt ... True\n",
      "2024-09-15 06:58:49,806 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start loading previous parsing results.\n",
      "2024-09-15 06:58:53,519 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loaded 11175629 log sequences and their mappings.\n",
      "2024-09-15 06:58:54,856 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loaded 575061 blocks\n",
      "2024-09-15 06:58:54,857 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Finished in 5.05\n",
      "2024-09-15 06:58:54,859 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Found 125 tokens in 46 log templates\n",
      "2024-09-15 06:58:54,869 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: OOV Rate: 0.09614966282358059\n",
      "2024-09-15 06:58:54,876 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/templates.vec\n",
      "2024-09-15 06:58:54,877 - HDFSLoader - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: All data preparation finished in 5.07\n",
      "2024-09-15 06:58:54,877 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start preprocessing dataset HDFS by parsing method IBM\n",
      "2024-09-15 06:58:54,878 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start generating instances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ipandport block* * * block* ipandport * ipandport ipandport ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport * * * block* ipandport * ipandport * ipandport * ipandport ipandport ipandport * ipandport * ipandport ipandport block* ipandport block* ipandport ipandport * * * * ipandport ipandport * ipandport ipandport block* * * "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575061/575061 [00:02<00:00, 275987.76it/s]\n",
      "2024-09-15 06:58:57,114 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: 165180 Normal, 7338 Anomalous instances.\n",
      "2024-09-15 06:58:57,114 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: 393043 Normal, 9500 Anomalous instances.\n",
      "2024-09-15 06:58:58,359 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-15 06:58:58,695 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Embed size: 41 in pre dataset.\n",
      "2024-09-15 06:58:58,696 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-15 06:58:59,095 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Embed size: 46 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n",
      "Finished at 18.7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = \"HDFS\"\n",
    "# Mark results saving directories.\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "prob_label_res_file_HDFS = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\"\n",
    "    + dataset\n",
    "    + \"_\"\n",
    "    + parser\n",
    "    + \"/prob_label_res/mcs-\"\n",
    "    + str(min_cluster_size)\n",
    "    + \"_ms-\"\n",
    "    + str(min_samples),\n",
    ")\n",
    "rand_state_HDFS = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\" + dataset + \"_\" + parser + \"/prob_label_res/random_state\",\n",
    ")\n",
    "\n",
    "# Training, Validating and Testing instances.\n",
    "template_encoder_HDFS = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    "    if dataset == \"NC\"\n",
    "    else Simple_template_TF_IDF(word2vec_file)\n",
    ")\n",
    "processor_HDFS = Preprocessor()\n",
    "train_HDFS, _, _ = processor_HDFS.process(\n",
    "    dataset=dataset,\n",
    "    parsing=parser,\n",
    "    cut_func=cut_by(0.3, 0.),\n",
    "    template_encoding=template_encoder_HDFS.present,\n",
    ")\n",
    "\n",
    "# Log sequence representation.\n",
    "sequential_encoder_HDFS = Sequential_TF(processor_HDFS.embedding)\n",
    "train_reprs_HDFS = sequential_encoder_HDFS.present(train_HDFS)\n",
    "for index, inst in enumerate(train_HDFS):\n",
    "    inst.repr = train_reprs_HDFS[index]\n",
    "\n",
    "# Dimension reduction if specified.\n",
    "transformer_HDFS = None\n",
    "if reduce_dimension != -1:\n",
    "    start_time = time.time()\n",
    "    print(f\"Start FastICA, target dimension: {reduce_dimension}.\")\n",
    "    transformer_HDFS = FastICA(n_components=reduce_dimension)\n",
    "    train_reprs_HDFS = transformer_HDFS.fit_transform(train_reprs_HDFS)\n",
    "    for idx, inst in enumerate(train_HDFS):\n",
    "        inst.repr = train_reprs_HDFS[idx]\n",
    "    print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "\n",
    "labeled_train_HDFS = train_HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Aggregate vocab and label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:59:20,367 - Vocab - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Total words: 50\n",
      "2024-09-15 06:59:20,368 - Vocab - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-15 06:59:20,376 - Vocab - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Total words: 482\n",
      "2024-09-15 06:59:20,377 - Vocab - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-15 06:59:20,426 - AttGRU - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: ==== Model Parameters ====\n",
      "2024-09-15 06:59:20,427 - AttGRU - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Input Dimension: 300\n",
      "2024-09-15 06:59:20,427 - AttGRU - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Hidden Size: 128\n",
      "2024-09-15 06:59:20,427 - AttGRU - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Num Layers: 4\n",
      "2024-09-15 06:59:20,428 - AttGRU - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Dropout: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 256\n",
      "Orthogonal pretrainer loss: 7.80e-06\n",
      "2 256\n",
      "Orthogonal pretrainer loss: 1.03e-05\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "new_embedding = {}\n",
    "for key in processor_BGL.embedding.keys():\n",
    "    new_embedding[key] = processor_BGL.embedding[key]\n",
    "for key in processor_HDFS.embedding.keys():\n",
    "    new_embedding[key + 432] = processor_HDFS.embedding[key]\n",
    "# Load Embeddings\n",
    "vocab_HDFS = Vocab()\n",
    "vocab_HDFS.load_from_dict(processor_HDFS.embedding)\n",
    "vocab.load_from_dict(new_embedding)\n",
    "\n",
    "metalog = MetaLog(\n",
    "    vocab=vocab,\n",
    "    num_layer=num_layer,\n",
    "    hidden_size=lstm_hiddens,\n",
    "    drop_out=drop_out,\n",
    "    label2id=processor_BGL.label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:59:21,163 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 0 | phase: train | start time: 06:59:21 | learning rate: [0.002].\n",
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:935: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n",
      "2024-09-15 07:11:12,692 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 500 | Epoch: 0 | Meta-train loss: 0.05241364985704422 | Meta-test loss: 0.38199684023857117.\n",
      "2024-09-15 07:23:08,205 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 1000 | Epoch: 0 | Meta-train loss: 0.06264618784189224 | Meta-test loss: 0.4360302984714508.\n",
      "2024-09-15 07:35:02,926 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 1500 | Epoch: 0 | Meta-train loss: 0.004530108068138361 | Meta-test loss: 0.29435890913009644.\n",
      "2024-09-15 07:40:20,678 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 07:41:44,468 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 95.10587934233254 | Precision = 90.83465721617989 | Recall = 99.79860365198711\n",
      "2024-09-15 07:41:44,478 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 07:42:24,791 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 41.47052036460136 | Precision = 33.94484582329886 | Recall = 53.283670595211625\n",
      "2024-09-15 07:42:25,321 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Exceed best F1 score: history = 0, current = 41.47052036460136.\n",
      "2024-09-15 07:42:25,368 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 0 finished.\n",
      "2024-09-15 07:42:25,390 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 1 | phase: train | start time: 07:42:25 | learning rate: [0.0015].\n",
      "2024-09-15 07:48:54,052 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 2000 | Epoch: 1 | Meta-train loss: 0.0881635919213295 | Meta-test loss: 0.3824729919433594.\n",
      "2024-09-15 08:00:42,799 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 2500 | Epoch: 1 | Meta-train loss: 0.036075931042432785 | Meta-test loss: 0.2885567545890808.\n",
      "2024-09-15 08:12:36,858 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 3000 | Epoch: 1 | Meta-train loss: 0.013645763508975506 | Meta-test loss: 0.23390845954418182.\n",
      "2024-09-15 08:23:23,872 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 08:24:46,855 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 89.04437268269346 | Precision = 80.27819711020057 | Recall = 99.95972073039742\n",
      "2024-09-15 08:24:46,858 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 08:25:25,736 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 55.0402576489533 | Precision = 53.20672478206725 | Recall = 57.00466977985324\n",
      "2024-09-15 08:25:25,738 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Exceed best F1 score: history = 41.47052036460136, current = 55.0402576489533.\n",
      "2024-09-15 08:25:25,766 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 1 finished.\n",
      "2024-09-15 08:25:25,792 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 2 | phase: train | start time: 08:25:25 | learning rate: [0.00084375].\n",
      "2024-09-15 08:26:33,230 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 3500 | Epoch: 2 | Meta-train loss: 0.01970989629626274 | Meta-test loss: 0.31651729345321655.\n",
      "2024-09-15 08:38:18,894 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 4000 | Epoch: 2 | Meta-train loss: 0.007016538642346859 | Meta-test loss: 0.3120041489601135.\n",
      "2024-09-15 08:50:10,350 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 4500 | Epoch: 2 | Meta-train loss: 0.034026261419057846 | Meta-test loss: 0.3602241575717926.\n",
      "2024-09-15 09:01:57,400 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 5000 | Epoch: 2 | Meta-train loss: 0.06299810111522675 | Meta-test loss: 0.3035317063331604.\n",
      "2024-09-15 09:06:11,655 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 09:07:36,907 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 89.97762051654267 | Precision = 81.87121629058888 | Recall = 99.86573576799141\n",
      "2024-09-15 09:07:36,911 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 09:08:16,485 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 49.5321975166041 | Precision = 40.571928662661435 | Recall = 63.57201097027648\n",
      "2024-09-15 09:08:16,487 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 2 finished.\n",
      "2024-09-15 09:08:16,517 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 3 | phase: train | start time: 09:08:16 | learning rate: [0.000474609375].\n",
      "2024-09-15 09:15:55,977 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 5500 | Epoch: 3 | Meta-train loss: 0.026342540979385376 | Meta-test loss: 0.32885417342185974.\n",
      "2024-09-15 09:27:54,765 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 6000 | Epoch: 3 | Meta-train loss: 0.00445936806499958 | Meta-test loss: 0.31190070509910583.\n",
      "2024-09-15 09:39:49,165 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 6500 | Epoch: 3 | Meta-train loss: 0.0042030285112559795 | Meta-test loss: 0.3712582290172577.\n",
      "2024-09-15 09:49:27,604 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 09:50:50,586 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 88.76042908224076 | Precision = 79.80068581225889 | Recall = 99.98657357679915\n",
      "2024-09-15 09:50:50,589 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 09:51:29,486 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 61.66500261493842 | Precision = 45.38932633420822 | Recall = 96.13816618486398\n",
      "2024-09-15 09:51:29,489 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Exceed best F1 score: history = 55.0402576489533, current = 61.66500261493842.\n",
      "2024-09-15 09:51:29,514 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 3 finished.\n",
      "2024-09-15 09:51:29,533 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 4 | phase: train | start time: 09:51:29 | learning rate: [0.00035595703125].\n",
      "2024-09-15 09:53:45,443 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 7000 | Epoch: 4 | Meta-train loss: 0.010430839844048023 | Meta-test loss: 0.2821393311023712.\n",
      "2024-09-15 10:05:44,094 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 7500 | Epoch: 4 | Meta-train loss: 0.03849983215332031 | Meta-test loss: 0.38923734426498413.\n",
      "2024-09-15 10:17:45,472 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 8000 | Epoch: 4 | Meta-train loss: 0.006962318439036608 | Meta-test loss: 0.32441145181655884.\n",
      "2024-09-15 10:29:45,163 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 8500 | Epoch: 4 | Meta-train loss: 0.05811932310461998 | Meta-test loss: 0.23059560358524323.\n",
      "2024-09-15 10:32:53,391 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 10:34:18,294 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 88.37844254510921 | Precision = 79.20212765957447 | Recall = 99.95972073039742\n",
      "2024-09-15 10:34:18,298 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 10:34:57,140 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 65.4245809304889 | Precision = 51.329916260625616 | Recall = 90.18975613371877\n",
      "2024-09-15 10:34:57,142 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Exceed best F1 score: history = 61.66500261493842, current = 65.4245809304889.\n",
      "2024-09-15 10:34:57,164 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 4 finished.\n",
      "2024-09-15 10:34:57,184 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 5 | phase: train | start time: 10:34:57 | learning rate: [0.000200225830078125].\n",
      "2024-09-15 10:43:53,532 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 9000 | Epoch: 5 | Meta-train loss: 0.008400929160416126 | Meta-test loss: 0.29897230863571167.\n",
      "2024-09-15 10:55:52,955 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 9500 | Epoch: 5 | Meta-train loss: 0.0029862301889806986 | Meta-test loss: 0.34398818016052246.\n",
      "2024-09-15 11:07:54,144 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 10000 | Epoch: 5 | Meta-train loss: 0.002641690894961357 | Meta-test loss: 0.3495486378669739.\n",
      "2024-09-15 11:16:25,768 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 11:17:53,308 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 88.22274881516587 | Precision = 78.94402035623409 | Recall = 99.97314715359828\n",
      "2024-09-15 11:17:53,315 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 11:18:33,258 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 67.89304700247382 | Precision = 52.63394132288734 | Recall = 95.61188940775332\n",
      "2024-09-15 11:18:33,259 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Exceed best F1 score: history = 65.4245809304889, current = 67.89304700247382.\n",
      "2024-09-15 11:18:33,277 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 5 finished.\n",
      "2024-09-15 11:18:33,298 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 6 | phase: train | start time: 11:18:33 | learning rate: [0.00011262702941894531].\n",
      "2024-09-15 11:21:59,034 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 10500 | Epoch: 6 | Meta-train loss: 0.011415013112127781 | Meta-test loss: 0.3410661220550537.\n",
      "2024-09-15 11:34:00,702 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 11000 | Epoch: 6 | Meta-train loss: 0.029482105746865273 | Meta-test loss: 0.2483247071504593.\n",
      "2024-09-15 11:45:59,482 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 11500 | Epoch: 6 | Meta-train loss: 0.003860475029796362 | Meta-test loss: 0.26928919553756714.\n",
      "2024-09-15 11:58:04,138 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 12000 | Epoch: 6 | Meta-train loss: 0.011177579872310162 | Meta-test loss: 0.2681845426559448.\n",
      "2024-09-15 12:00:02,133 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 12:01:26,902 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 90.29167424655873 | Precision = 82.32887316156143 | Recall = 99.95972073039742\n",
      "2024-09-15 12:01:26,906 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 12:02:06,749 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 65.09349039376559 | Precision = 49.84048838125246 | Recall = 93.79957008375955\n",
      "2024-09-15 12:02:06,751 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 6 finished.\n",
      "2024-09-15 12:02:06,774 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 7 | phase: train | start time: 12:02:06 | learning rate: [6.335270404815674e-05].\n",
      "2024-09-15 12:12:13,261 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 12500 | Epoch: 7 | Meta-train loss: 0.055820032954216 | Meta-test loss: 0.30850404500961304.\n",
      "2024-09-15 12:24:14,807 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 13000 | Epoch: 7 | Meta-train loss: 0.009643882513046265 | Meta-test loss: 0.27068644762039185.\n",
      "2024-09-15 12:36:09,042 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 13500 | Epoch: 7 | Meta-train loss: 0.09840872883796692 | Meta-test loss: 0.28780272603034973.\n",
      "2024-09-15 12:43:33,162 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 12:44:59,874 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 90.72190070057874 | Precision = 83.03780528604885 | Recall = 99.97314715359828\n",
      "2024-09-15 12:44:59,878 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 12:45:39,458 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 70.75422733357009 | Precision = 56.023966084097594 | Recall = 95.99362537988289\n",
      "2024-09-15 12:45:39,459 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Exceed best F1 score: history = 67.89304700247382, current = 70.75422733357009.\n",
      "2024-09-15 12:45:39,486 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 7 finished.\n",
      "2024-09-15 12:45:39,507 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 8 | phase: train | start time: 12:45:39 | learning rate: [4.7514528036117556e-05].\n",
      "2024-09-15 12:50:19,567 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 14000 | Epoch: 8 | Meta-train loss: 0.006842274684458971 | Meta-test loss: 0.34851473569869995.\n",
      "2024-09-15 13:02:17,433 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 14500 | Epoch: 8 | Meta-train loss: 0.003307557664811611 | Meta-test loss: 0.2841651737689972.\n",
      "2024-09-15 13:14:12,941 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 15000 | Epoch: 8 | Meta-train loss: 0.027675164863467216 | Meta-test loss: 0.39744848012924194.\n",
      "2024-09-15 13:26:12,750 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 15500 | Epoch: 8 | Meta-train loss: 0.04873485863208771 | Meta-test loss: 0.2670503556728363.\n",
      "2024-09-15 13:27:01,094 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 13:28:29,439 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 90.50890739952574 | Precision = 82.70918990999 | Recall = 99.9328678839957\n",
      "2024-09-15 13:28:29,443 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 13:29:09,409 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 68.64261395127873 | Precision = 53.29945077465366 | Recall = 96.39018604995923\n",
      "2024-09-15 13:29:09,411 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 8 finished.\n",
      "2024-09-15 13:29:09,453 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Starting epoch: 9 | phase: train | start time: 13:29:09 | learning rate: [2.6726922020316126e-05].\n",
      "2024-09-15 13:40:21,440 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 16000 | Epoch: 9 | Meta-train loss: 0.11026979237794876 | Meta-test loss: 0.35273051261901855.\n",
      "2024-09-15 13:52:24,679 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 16500 | Epoch: 9 | Meta-train loss: 0.0031917362939566374 | Meta-test loss: 0.2987715005874634.\n",
      "2024-09-15 14:04:19,248 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Step: 17000 | Epoch: 9 | Meta-train loss: 0.01105292048305273 | Meta-test loss: 0.3392741084098816.\n",
      "2024-09-15 14:10:33,528 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-15 14:12:00,594 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Train: F1 score = 90.09019916459835 | Precision = 82.03064711718663 | Recall = 99.90601503759399\n",
      "2024-09-15 14:12:00,598 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-15 14:12:40,452 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Test: F1 score = 68.6776113059186 | Precision = 53.519231565437 | Recall = 95.81572900452153\n",
      "2024-09-15 14:12:40,455 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Training epoch 9 finished.\n"
     ]
    }
   ],
   "source": [
    "# Meta-learning\n",
    "log = \"layer={}_hidden={}_epoch={}\".format(num_layer, lstm_hiddens, epochs)\n",
    "best_model_file = os.path.join(output_model_dir, log + \"_best.pt\")\n",
    "last_model_file = os.path.join(output_model_dir, log + \"_last.pt\")\n",
    "\n",
    "if not os.path.exists(output_model_dir):\n",
    "    os.makedirs(output_model_dir)\n",
    "if mode == \"train\":\n",
    "    # Train\n",
    "    optimizer = Optimizer(\n",
    "        filter(lambda p: p.requires_grad, metalog.model.parameters()), lr=gamma\n",
    "    )\n",
    "    global_step = 0\n",
    "    best_f1_score = 0\n",
    "    for epoch in range(epochs):\n",
    "        metalog.model.train()\n",
    "        metalog.bk_model.train()\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        logger.info(\n",
    "            f\"Starting epoch: {epoch} | phase: train | start time: {start} | learning rate: {optimizer.lr}.\"\n",
    "        )\n",
    "\n",
    "        batch_num = int(np.ceil(len(labeled_train_HDFS) / float(batch_size)))\n",
    "        batch_iter = 0\n",
    "        batch_num_test = int(np.ceil(len(labeled_train_BGL) / float(batch_size)))\n",
    "        batch_iter_test = 0\n",
    "        total_bn = max(batch_num, batch_num_test)\n",
    "        meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "        meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "\n",
    "        for i in range(total_bn):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Meta-train on HDFS and BGL\n",
    "            meta_train_batch = meta_train_loader.__next__()\n",
    "            meta_test_batch = meta_test_loader.__next__()\n",
    "            tinst_tr = generate_tinsts_binary_label(meta_train_batch, vocab_HDFS)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_tr.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_tr.to_mps(DEVICE)\n",
    "            loss = metalog.forward(tinst_tr.inputs, tinst_tr.targets)\n",
    "            loss_value = loss.data.cpu().numpy()\n",
    "            loss.backward(retain_graph=True)\n",
    "            batch_iter += 1\n",
    "            if torch.cuda.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .cuda()\n",
    "                )\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .to(DEVICE)\n",
    "                )\n",
    "            else:\n",
    "                metalog.bk_model = get_updated_network(\n",
    "                    metalog.model, metalog.bk_model, alpha\n",
    "                ).train()\n",
    "            \n",
    "            # Meta-test on BGL\n",
    "            tinst_test = generate_tinsts_binary_label(meta_test_batch, vocab_BGL)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_test.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_test.to_mps(DEVICE)\n",
    "            loss_te = beta * metalog.bk_forward(\n",
    "                tinst_test.inputs, tinst_test.targets\n",
    "            )\n",
    "            loss_value_te = loss_te.data.cpu().numpy() / beta\n",
    "            loss_te.backward()\n",
    "            batch_iter_test += 1\n",
    "            \n",
    "            # Aggregate the gradients and update the model.\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            if global_step % 500 == 0:\n",
    "                logger.info(\n",
    "                    f\"Step: {global_step} | Epoch: {epoch} | Meta-train loss: {loss_value} | Meta-test loss: {loss_value_te}.\"\n",
    "                )\n",
    "            if batch_iter == batch_num:\n",
    "                meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "                batch_iter = 0\n",
    "            if batch_iter_test == batch_num_test:\n",
    "                meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "                batch_iter_test = 0\n",
    "\n",
    "        if train_BGL:\n",
    "            metalog.evaluate(\"Train\", train_BGL + train_HDFS)\n",
    "            \n",
    "        if test_BGL:\n",
    "            _, _, f1_score = metalog.evaluate(\"Test\", test_BGL)\n",
    "\n",
    "            if f1_score > best_f1_score:\n",
    "                logger.info(\n",
    "                    f\"Exceed best F1 score: history = {best_f1_score}, current = {f1_score}.\"\n",
    "                )\n",
    "                torch.save(metalog.model.state_dict(), best_model_file)\n",
    "                best_f1_score = f1_score\n",
    "\n",
    "        logger.info(f\"Training epoch {epoch} finished.\")\n",
    "        torch.save(metalog.model.state_dict(), last_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 14:12:40,558 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: === Final Model ===\n",
      "/var/folders/45/0lfy6jf14g3cptljv3x_x53r0000gn/T/ipykernel_27840/2985776712.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metalog.model.load_state_dict(torch.load(last_model_file))\n",
      "2024-09-15 14:12:40,600 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Last model on Test BGL by threshold 0.5\n",
      "2024-09-15 14:13:20,535 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Last model on Test BGL: F1 score = 68.6776113059186 | Precision = 53.519231565437 | Recall = 95.81572900452153\n",
      "2024-09-15 14:13:20,538 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: === Best Model ===\n",
      "/var/folders/45/0lfy6jf14g3cptljv3x_x53r0000gn/T/ipykernel_27840/2985776712.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metalog.model.load_state_dict(torch.load(best_model_file))\n",
      "2024-09-15 14:13:20,562 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Start evaluating Best model on Test BGL by threshold 0.5\n",
      "2024-09-15 14:14:00,294 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Best model on Test BGL: F1 score = 70.75422733357009 | Precision = 56.023966084097594 | Recall = 95.99362537988289\n",
      "2024-09-15 14:14:00,296 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: All Finished!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(last_model_file):\n",
    "    logger.info(\"=== Final Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(last_model_file))\n",
    "    metalog.evaluate(\"Last model on Test BGL\", test_BGL)\n",
    "if os.path.exists(best_model_file):\n",
    "    logger.info(\"=== Best Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(best_model_file))\n",
    "    metalog.evaluate(\"Best model on Test BGL\", test_BGL)\n",
    "logger.info(\"All Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "STATISTICS_TEMPLATE_LOG_PATH = \"logs/Statistics_Template.log\"\n",
    "METALOG_LOG_PATH = \"logs/MetaLog.log\"\n",
    "\n",
    "with open(STATISTICS_TEMPLATE_LOG_PATH, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        regex = rf\"^.+ - Statistics_Template_Encoder - {SESSION} - INFO: Loading word2vec dict from (.+)\\.$\"\n",
    "        match = re.search(regex, line)\n",
    "\n",
    "        if match is not None:\n",
    "            word2vec_file = match.group().split()[-1]\n",
    "            break\n",
    "\n",
    "TITLE = f\"BILATERAL GENERALIZATION TRANSFERRING HDFS TO BGL\\n(using {word2vec_file})\\n\"\n",
    "\n",
    "train_f1_scores = []\n",
    "test_f1_scores = []\n",
    "meta_train_losses = []\n",
    "meta_test_losses = []\n",
    "with open(METALOG_LOG_PATH, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        regex = rf\"^.+ - MetaLog - {SESSION} - INFO: Train: F1 score = (.+) \\| Precision = (.+) \\| Recall = (.+)$\"\n",
    "        match = re.search(regex, line)\n",
    "\n",
    "        if match is not None:\n",
    "            params = match.group().split(\"|\")\n",
    "\n",
    "            f1_score = float(params[0].split()[-1])\n",
    "            train_f1_scores.append(f1_score)\n",
    "\n",
    "    for line in lines:\n",
    "        regex = rf\"^.+ - MetaLog - {SESSION} - INFO: Test: F1 score = (.+) \\| Precision = (.+) \\| Recall = (.+)$\"\n",
    "        match = re.search(regex, line)\n",
    "\n",
    "        if match is not None:\n",
    "            params = match.group().split(\"|\")\n",
    "\n",
    "            f1_score = float(params[0].split()[-1])\n",
    "            test_f1_scores.append(f1_score)\n",
    "\n",
    "    for line in lines:\n",
    "        regex = rf\"^.* - MetaLog - {SESSION} - INFO: Step: (.+) \\| Epoch: (.+) \\| Meta-train loss: (.+) \\| Meta-test loss: (.+)\\.$\"\n",
    "        match = re.search(regex, line)\n",
    "\n",
    "        if match is not None:\n",
    "            params = match.group().split(\"|\")\n",
    "\n",
    "            meta_train_loss = float(params[2].split()[-1].split()[-1])\n",
    "            meta_train_losses.append(meta_train_loss)\n",
    "\n",
    "            meta_test_loss = float(params[3].split()[-1].split()[-1][:-1])\n",
    "            meta_test_losses.append(meta_test_loss)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "num_epoches = [i for i in range(len(train_f1_scores))]\n",
    "num_validations = [i * 500 for i in range(len(meta_train_losses))]\n",
    "\n",
    "axs[0].set_ylim(0, 110)\n",
    "axs[0].plot(num_epoches, train_f1_scores, color=\"tab:blue\")\n",
    "axs[0].plot(num_epoches, test_f1_scores, color=\"tab:orange\")\n",
    "axs[0].legend([\"Train\", \"Test\"])\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"F1 Score\")\n",
    "\n",
    "max_test_f1_score = max(test_f1_scores)\n",
    "best_test_f1_score = 0\n",
    "\n",
    "for i in range(len(num_epoches)):\n",
    "    if test_f1_scores[i] == max_test_f1_score:\n",
    "        best_test_f1_score = i\n",
    "    axs[0].plot(num_epoches[i], train_f1_scores[i], \"o\", color=\"tab:blue\", zorder=10)\n",
    "    axs[0].text(\n",
    "        num_epoches[i],\n",
    "        train_f1_scores[i] + 5,\n",
    "        round(train_f1_scores[i], 2),\n",
    "        color=\"red\" if train_f1_scores[i] == train_f1_scores else \"black\",\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "    axs[0].plot(num_epoches[i], test_f1_scores[i], \"o\", color=\"tab:orange\", zorder=10)\n",
    "    axs[0].text(\n",
    "        num_epoches[i],\n",
    "        test_f1_scores[i] - 10,\n",
    "        round(test_f1_scores[i], 2),\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "axs[1].set_ylim(0, 1)\n",
    "axs[1].plot(num_validations, meta_train_losses, color=\"tab:blue\")\n",
    "axs[1].plot(num_validations, meta_test_losses, color=\"tab:orange\")\n",
    "axs[1].legend([\"Meta-train loss\", \"Meta-test loss\"])\n",
    "axs[1].set_xlabel(\"Step\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "LOSS_EPS = 0.05\n",
    "min_test_loss = min(meta_test_losses)\n",
    "for i in range(0, len(num_validations), 2):\n",
    "    axs[1].plot(\n",
    "        num_validations[i], meta_train_losses[i], \"o\", color=\"tab:blue\", zorder=10\n",
    "    )\n",
    "    axs[1].text(\n",
    "        num_validations[i],\n",
    "        meta_train_losses[i] + LOSS_EPS,\n",
    "        round(meta_train_losses[i], 4),\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "    axs[1].plot(\n",
    "        num_validations[i], meta_test_losses[i], \"o\", color=\"tab:orange\", zorder=10\n",
    "    )\n",
    "    axs[1].text(\n",
    "        num_validations[i],\n",
    "        meta_test_losses[i] + LOSS_EPS,\n",
    "        round(meta_test_losses[i], 4),\n",
    "        color=\"red\" if meta_test_losses[i] == min_test_loss else \"black\",\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"{TITLE}\\n\" + f\"Best model F1 Score = {test_f1_scores[best_test_f1_score]}\\n\"\n",
    ")\n",
    "\n",
    "fig.plot()\n",
    "fig.savefig(f\"visualization/graphs/{word2vec_file}-{SESSION}.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
