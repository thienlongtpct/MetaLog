{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:56:32,631 - AttGRU - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Attention-Based GRU succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:56:33,003 - Preprocessor - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:56:33,005 - StatisticsRepresentation. - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Statistics Representation succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:56:33,006 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Statistics Template Encoder succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-15 06:56:33,011 - Vocab - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for Vocab succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from CONSTANTS import DEVICE, LOG_ROOT, PROJECT_ROOT, SESSION\n",
    "from models.gru import AttGRUModel\n",
    "from module.Common import batch_variable_inst, data_iter, generate_tinsts_binary_label\n",
    "from module.Optimizer import Optimizer\n",
    "from preprocessing.AutoLabeling import Probabilistic_Labeling\n",
    "from preprocessing.datacutter.SimpleCutting import cut_by\n",
    "from preprocessing.Preprocess import Preprocessor\n",
    "from representations.sequences.statistics import Sequential_TF\n",
    "from representations.templates.statistics import (\n",
    "    Simple_template_TF_IDF,\n",
    "    Template_TF_IDF_without_clean,\n",
    ")\n",
    "from utils.Vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom params\n",
    "lstm_hiddens = 128\n",
    "num_layer = 4\n",
    "batch_size = 100\n",
    "drop_out = 0.5\n",
    "epochs = 10\n",
    "\n",
    "word2vec_file = \"glove.42B.300d.txt\"\n",
    "dim = 300\n",
    "alpha = 2e-3\n",
    "beta = 2\n",
    "gamma = 2e-3\n",
    "\n",
    "\n",
    "parser = \"IBM\"\n",
    "mode = \"train\"\n",
    "min_cluster_size = 100\n",
    "min_samples = 100\n",
    "reduce_dimension = 50\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function for updating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_network(old, new, lr, load=False):\n",
    "    updated_theta = {}\n",
    "    state_dicts = old.state_dict()\n",
    "    param_dicts = dict(old.named_parameters())\n",
    "\n",
    "    for i, (k, v) in enumerate(state_dicts.items()):\n",
    "        if k in param_dicts.keys() and param_dicts[k].grad is not None:\n",
    "            updated_theta[k] = param_dicts[k] - lr * param_dicts[k].grad\n",
    "        else:\n",
    "            updated_theta[k] = state_dicts[k]\n",
    "    if load:\n",
    "        new.load_state_dict(updated_theta)\n",
    "    else:\n",
    "        new = put_theta(new, updated_theta)\n",
    "    return new\n",
    "\n",
    "\n",
    "def put_theta(model, theta):\n",
    "    def k_param_fn(tmp_model, name=None):\n",
    "        if len(tmp_model._modules) != 0:\n",
    "            for k, v in tmp_model._modules.items():\n",
    "                if name is None:\n",
    "                    k_param_fn(v, name=str(k))\n",
    "                else:\n",
    "                    k_param_fn(v, name=str(name + \".\" + k))\n",
    "        else:\n",
    "            for k, v in tmp_model._parameters.items():\n",
    "                if not isinstance(v, torch.Tensor):\n",
    "                    continue\n",
    "                tmp_model._parameters[k] = theta[str(name + \".\" + k)]\n",
    "\n",
    "    k_param_fn(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MetaLog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLog:\n",
    "    def __init__(self, vocab, num_layer, hidden_size, drop_out, label2id):\n",
    "        self.label2id = label2id\n",
    "        self.vocab = vocab\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1024\n",
    "        self.drop_out = drop_out\n",
    "        self.model = AttGRUModel(vocab, self.num_layer, self.hidden_size, self.drop_out)\n",
    "        self.bk_model = AttGRUModel(\n",
    "            vocab, self.num_layer, self.hidden_size, self.drop_out, is_backup=True\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda(DEVICE)\n",
    "            self.bk_model = self.bk_model.cuda(DEVICE)\n",
    "        elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "            self.model = self.model.to(DEVICE)\n",
    "            self.bk_model = self.bk_model.to(DEVICE)\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        tag_logits = self.model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def bk_forward(self, inputs, targets):\n",
    "        tag_logits = self.bk_model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, threshold=None):\n",
    "        with torch.no_grad():\n",
    "            tag_logits = self.model(inputs)\n",
    "            tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        if threshold is not None:\n",
    "            probs = tag_logits.detach().cpu().numpy()\n",
    "            anomaly_id = self.label2id[\"Anomalous\"]\n",
    "            pred_tags = np.zeros(probs.shape[0])\n",
    "            for i, logits in enumerate(probs):\n",
    "                if logits[anomaly_id] >= threshold:\n",
    "                    pred_tags[i] = anomaly_id\n",
    "                else:\n",
    "                    pred_tags[i] = 1 - anomaly_id\n",
    "\n",
    "        else:\n",
    "            pred_tags = tag_logits.detach().max(1)[1].cpu()\n",
    "        return pred_tags, tag_logits\n",
    "\n",
    "    def evaluate(self, dataset, instances, threshold=0.5):\n",
    "        logger.info(f\"Start evaluating {dataset} by threshold {threshold}\")\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            globalBatchNum = 0\n",
    "            TP, TN, FP, FN = 0, 0, 0, 0\n",
    "            tag_correct, tag_total = 0, 0\n",
    "            for onebatch in data_iter(instances, self.test_batch_size, False):\n",
    "                tinst = generate_tinsts_binary_label(onebatch, vocab_BGL, False)\n",
    "                if torch.cuda.is_available():\n",
    "                    tinst.to_cuda(DEVICE)\n",
    "                elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                    tinst.to_mps(DEVICE)\n",
    "                self.model.eval()\n",
    "                pred_tags, tag_logits = self.predict(tinst.inputs, threshold)\n",
    "                for inst, bmatch in batch_variable_inst(\n",
    "                    onebatch, pred_tags, tag_logits, processor_BGL.id2tag\n",
    "                ):\n",
    "                    tag_total += 1\n",
    "                    if bmatch:\n",
    "                        tag_correct += 1\n",
    "                        if inst.label == \"Normal\":\n",
    "                            TN += 1\n",
    "                        else:\n",
    "                            TP += 1\n",
    "                    else:\n",
    "                        if inst.label == \"Normal\":\n",
    "                            FP += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                globalBatchNum += 1\n",
    "            if TP + FP != 0:\n",
    "                precision = 100 * TP / (TP + FP)\n",
    "                recall = 100 * TP / (TP + FN)\n",
    "                f1_score = 2 * precision * recall / (precision + recall)\n",
    "                # fpr = 100 * FP / (FP + TN)\n",
    "                logger.info(\n",
    "                    f\"{dataset}: F1 score = {f1_score} | Precision = {precision} | Recall = {recall}\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"{dataset}: F1 score = {0} | Precision = {0} | Recall = {0}\"\n",
    "                )\n",
    "                precision, recall, f1_score = 0, 0, 0\n",
    "        return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logging config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:56:33,042 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"MetaLog\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler(sys.stderr)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    "    )\n",
    ")\n",
    "file_handler = logging.FileHandler(os.path.join(LOG_ROOT, \"MetaLog.log\"))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(\n",
    "    logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    "    )\n",
    ")\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "logger.info(\n",
    "    f\"Construct logger for MetaLog succeeded, current working directory: {os.getcwd()}, logs will be written in {LOG_ROOT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Log custom params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:56:33,047 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Network params: lstm_hiddens = 128 | num_layer = 4 | drop_out = 0.5.\n",
      "2024-09-15 06:56:33,048 - MetaLog - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Hyper-params: alpha = 0.002 | beta = 2 | gamma = 0.002 | word2vec_file = glove.42B.300d.txt.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Network params: lstm_hiddens = {lstm_hiddens} | num_layer = {num_layer} | drop_out = {drop_out}.\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"Hyper-params: alpha = {alpha} | beta = {beta} | gamma = {gamma} | word2vec_file = {word2vec_file}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Import BGL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 06:56:33,054 - Statistics_Template_Encoder - SESSION_f56fe5747d539d46f028a40c6b1e8b17 - INFO: Loading word2vec dict from glove.42B.300d.txt.\n"
     ]
    }
   ],
   "source": [
    "dataset = \"BGL\"\n",
    "\n",
    "# # Mark results saving directories.\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "prob_label_res_file_BGL = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\"\n",
    "    + dataset\n",
    "    + \"_\"\n",
    "    + parser\n",
    "    + \"/prob_label_res/mcs-\"\n",
    "    + str(min_cluster_size)\n",
    "    + \"_ms-\"\n",
    "    + str(min_samples),\n",
    ")\n",
    "rand_state_BGL = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\" + dataset + \"_\" + parser + \"/prob_label_res/random_state\",\n",
    ")\n",
    "\n",
    "output_model_dir = os.path.join(\n",
    "    save_dir, \"models/MetaLog/\" + dataset + \"_\" + parser + \"/model\"\n",
    ")\n",
    "output_res_dir = os.path.join(\n",
    "    save_dir, \"results/MetaLog/\" + dataset + \"_\" + parser + \"/detect_res\"\n",
    ")\n",
    "\n",
    "# Training, Validating and Testing instances.\n",
    "template_encoder_BGL = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    "    if dataset == \"NC\"\n",
    "    else Simple_template_TF_IDF(word2vec_file)\n",
    ")\n",
    "processor_BGL = Preprocessor()\n",
    "train_BGL, _, test_BGL = processor_BGL.process(\n",
    "    dataset=dataset,\n",
    "    parsing=parser,\n",
    "    cut_func=cut_by(0.3, 0., 0.01),\n",
    "    template_encoding=template_encoder_BGL.present,\n",
    ")\n",
    "\n",
    "# Log sequence representation.\n",
    "sequential_encoder_BGL = Sequential_TF(processor_BGL.embedding)\n",
    "train_reprs_BGL = sequential_encoder_BGL.present(train_BGL)\n",
    "for index, inst in enumerate(train_BGL):\n",
    "    inst.repr = train_reprs_BGL[index]\n",
    "test_reprs_BGL = sequential_encoder_BGL.present(test_BGL)\n",
    "for index, inst in enumerate(test_BGL):\n",
    "    inst.repr = test_reprs_BGL[index]\n",
    "\n",
    "# Dimension reduction if specified.\n",
    "transformer_BGL = None\n",
    "if reduce_dimension != -1:\n",
    "    start_time = time.time()\n",
    "    print(f\"Start FastICA, target dimension: {reduce_dimension}.\")\n",
    "    transformer_BGL = FastICA(n_components=reduce_dimension)\n",
    "    train_reprs_BGL = transformer_BGL.fit_transform(train_reprs_BGL)\n",
    "    for idx, inst in enumerate(train_BGL):\n",
    "        inst.repr = train_reprs_BGL[idx]\n",
    "    print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "\n",
    "# Probabilistic labeling.\n",
    "# Sample normal instances.\n",
    "train_normal_BGL = [x for x, inst in enumerate(train_BGL) if inst.label == \"Normal\"]\n",
    "normal_ids_BGL = train_normal_BGL[: int(0.5 * len(train_normal_BGL))]\n",
    "label_generator_BGL = Probabilistic_Labeling(\n",
    "    min_samples=min_samples,\n",
    "    min_clust_size=min_cluster_size,\n",
    "    res_file=prob_label_res_file_BGL,\n",
    "    rand_state_file=rand_state_BGL,\n",
    ")\n",
    "labeled_train_BGL = label_generator_BGL.auto_label(train_BGL, normal_ids_BGL)\n",
    "\n",
    "# Load Embeddings\n",
    "vocab_BGL = Vocab()\n",
    "vocab_BGL.load_from_dict(processor_BGL.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Import HDFS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:16:25,782 - Statistics_Template_Encoder - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Loading word2vec dict from glove.42B.300d.txt.\n",
      "100%|██████████| 1917494/1917494 [01:24<00:00, 22601.49it/s]\n",
      "2024-09-14 13:18:13,070 - Statistics_Template_Encoder - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Total 1917494 words in glove.42B.300d.txt dict.\n",
      "2024-09-14 13:18:13,117 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 13:18:13,124 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/raw_log_seqs.txt\n",
      "100%|██████████| 575061/575061 [00:03<00:00, 183822.48it/s]\n",
      "2024-09-14 13:18:16,414 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Extraction finished successfully.\n",
      "2024-09-14 13:18:16,653 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 13:18:16,653 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 13:18:16,653 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-14 13:18:16,656 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-14 13:18:16,656 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-14 13:18:16,656 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-14 13:18:16,661 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-14 13:18:16,661 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-14 13:18:16,661 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-14 13:18:16,673 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Persistence file found, loading.\n",
      "2024-09-14 13:18:16,673 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Persistence file found, loading.\n",
      "2024-09-14 13:18:16,673 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Persistence file found, loading.\n",
      "2024-09-14 13:18:16,683 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Loaded.\n",
      "2024-09-14 13:18:16,683 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Loaded.\n",
      "2024-09-14 13:18:16,683 - drain - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Loaded.\n",
      "2024-09-14 13:18:16,685 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_event_mapping.dict ... True\n",
      "2024-09-14 13:18:16,685 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_sequences.txt ... True\n",
      "2024-09-14 13:18:16,685 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start loading previous parsing results.\n",
      "2024-09-14 13:18:20,603 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Loaded 11175629 log sequences and their mappings.\n",
      "2024-09-14 13:18:22,365 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Loaded 575061 blocks\n",
      "2024-09-14 13:18:22,366 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Finished in 5.68\n",
      "2024-09-14 13:18:22,369 - Statistics_Template_Encoder - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Found 125 tokens in 46 log templates\n",
      "2024-09-14 13:18:22,419 - Statistics_Template_Encoder - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: OOV Rate: 0.09625457617572515\n",
      "2024-09-14 13:18:22,429 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/templates.vec\n",
      "2024-09-14 13:18:22,429 - HDFSLoader - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: All data preparation finished in 5.75\n",
      "2024-09-14 13:18:22,430 - Preprocessor - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start preprocessing dataset HDFS by parsing method IBM\n",
      "2024-09-14 13:18:22,430 - Preprocessor - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start generating instances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ipandport block* * * block* ipandport * ipandport ipandport ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport * * * block* ipandport * ipandport * ipandport * ipandport ipandport ipandport * ipandport * ipandport ipandport block* ipandport block* ipandport ipandport * * * * ipandport ipandport * ipandport ipandport block* * * "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575061/575061 [00:02<00:00, 255770.11it/s]\n",
      "2024-09-14 13:18:24,860 - Preprocessor - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: 165180 Normal, 7338 Anomalous instances.\n",
      "2024-09-14 13:18:24,861 - Preprocessor - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: 393043 Normal, 9500 Anomalous instances.\n",
      "2024-09-14 13:18:26,233 - Preprocessor - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-14 13:18:26,575 - Preprocessor - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Embed size: 41 in pre dataset.\n",
      "2024-09-14 13:18:26,575 - Preprocessor - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-14 13:18:26,992 - Preprocessor - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Embed size: 46 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n",
      "Finished at 19.6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = \"HDFS\"\n",
    "# Mark results saving directories.\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "prob_label_res_file_HDFS = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\"\n",
    "    + dataset\n",
    "    + \"_\"\n",
    "    + parser\n",
    "    + \"/prob_label_res/mcs-\"\n",
    "    + str(min_cluster_size)\n",
    "    + \"_ms-\"\n",
    "    + str(min_samples),\n",
    ")\n",
    "rand_state_HDFS = os.path.join(\n",
    "    save_dir,\n",
    "    \"results/MetaLog/\" + dataset + \"_\" + parser + \"/prob_label_res/random_state\",\n",
    ")\n",
    "\n",
    "# Training, Validating and Testing instances.\n",
    "template_encoder_HDFS = (\n",
    "    Template_TF_IDF_without_clean(word2vec_file)\n",
    "    if dataset == \"NC\"\n",
    "    else Simple_template_TF_IDF(word2vec_file)\n",
    ")\n",
    "processor_HDFS = Preprocessor()\n",
    "train_HDFS, _, _ = processor_HDFS.process(\n",
    "    dataset=dataset,\n",
    "    parsing=parser,\n",
    "    cut_func=cut_by(0.3, 0.),\n",
    "    template_encoding=template_encoder_HDFS.present,\n",
    ")\n",
    "\n",
    "# Log sequence representation.\n",
    "sequential_encoder_HDFS = Sequential_TF(processor_HDFS.embedding)\n",
    "train_reprs_HDFS = sequential_encoder_HDFS.present(train_HDFS)\n",
    "for index, inst in enumerate(train_HDFS):\n",
    "    inst.repr = train_reprs_HDFS[index]\n",
    "\n",
    "# Dimension reduction if specified.\n",
    "transformer_HDFS = None\n",
    "if reduce_dimension != -1:\n",
    "    start_time = time.time()\n",
    "    print(f\"Start FastICA, target dimension: {reduce_dimension}.\")\n",
    "    transformer_HDFS = FastICA(n_components=reduce_dimension)\n",
    "    train_reprs_HDFS = transformer_HDFS.fit_transform(train_reprs_HDFS)\n",
    "    for idx, inst in enumerate(train_HDFS):\n",
    "        inst.repr = train_reprs_HDFS[idx]\n",
    "    print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "\n",
    "labeled_train_HDFS = train_HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Aggregate vocab and label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:19:19,945 - Vocab - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Total words: 50\n",
      "2024-09-14 13:19:19,948 - Vocab - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-14 13:19:19,957 - Vocab - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Total words: 482\n",
      "2024-09-14 13:19:19,958 - Vocab - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-14 13:19:19,979 - AttGRU - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: ==== Model Parameters ====\n",
      "2024-09-14 13:19:19,981 - AttGRU - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Input Dimension: 300\n",
      "2024-09-14 13:19:19,981 - AttGRU - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Hidden Size: 64\n",
      "2024-09-14 13:19:19,982 - AttGRU - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Num Layers: 4\n",
      "2024-09-14 13:19:19,982 - AttGRU - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Dropout: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 128\n",
      "Orthogonal pretrainer loss: 9.31e-03\n",
      "2 128\n",
      "Orthogonal pretrainer loss: 5.45e+07\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "new_embedding = {}\n",
    "for key in processor_BGL.embedding.keys():\n",
    "    new_embedding[key] = processor_BGL.embedding[key]\n",
    "for key in processor_HDFS.embedding.keys():\n",
    "    new_embedding[key + 432] = processor_HDFS.embedding[key]\n",
    "# Load Embeddings\n",
    "vocab_HDFS = Vocab()\n",
    "vocab_HDFS.load_from_dict(processor_HDFS.embedding)\n",
    "vocab.load_from_dict(new_embedding)\n",
    "\n",
    "metalog = MetaLog(\n",
    "    vocab=vocab,\n",
    "    num_layer=num_layer,\n",
    "    hidden_size=lstm_hiddens,\n",
    "    drop_out=drop_out,\n",
    "    label2id=processor_BGL.label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:19:25,868 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 0 | phase: train | start time: 13:19:25 | learning rate: [0.002].\n",
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:935: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n",
      "2024-09-14 13:31:46,294 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 500 | Epoch: 0 | Meta-train loss: 0.0019326373003423214 | Meta-test loss: 0.4342362582683563.\n",
      "2024-09-14 13:43:34,106 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 1000 | Epoch: 0 | Meta-train loss: 0.0013417900772765279 | Meta-test loss: 0.3380264937877655.\n",
      "2024-09-14 13:55:33,814 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 1500 | Epoch: 0 | Meta-train loss: 0.001428392599336803 | Meta-test loss: 0.2832505404949188.\n",
      "2024-09-14 14:01:01,641 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 14:02:23,590 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 93.79702813784381 | Precision = 88.39232511023715 | Recall = 99.90571120689656)\n",
      "2024-09-14 14:02:23,593 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 14:03:02,011 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 82.42778222131284 | Precision = 82.74020687852422 | Recall = 82.11770810169743)\n",
      "2024-09-14 14:03:02,126 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Exceed best F1 score: history = 0, current = 82.42778222131284.\n",
      "2024-09-14 14:03:02,142 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 0 finished.\n",
      "2024-09-14 14:03:02,157 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 1 | phase: train | start time: 14:03:02 | learning rate: [0.0015].\n",
      "2024-09-14 14:09:41,929 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 2000 | Epoch: 1 | Meta-train loss: 0.09409534186124802 | Meta-test loss: 0.38299238681793213.\n",
      "2024-09-14 14:21:55,532 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 2500 | Epoch: 1 | Meta-train loss: 0.0033691495191305876 | Meta-test loss: 0.30541250109672546.\n",
      "2024-09-14 14:34:04,055 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 3000 | Epoch: 1 | Meta-train loss: 0.002891355659812689 | Meta-test loss: 0.30579352378845215.\n",
      "2024-09-14 14:45:11,113 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 14:46:31,376 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 88.48839291042549 | Precision = 79.43855137683488 | Recall = 99.86530172413794)\n",
      "2024-09-14 14:46:31,381 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 14:47:09,482 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 91.0507714852168 | Precision = 85.12071688747059 | Recall = 97.86894967015047)\n",
      "2024-09-14 14:47:09,483 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Exceed best F1 score: history = 82.42778222131284, current = 91.0507714852168.\n",
      "2024-09-14 14:47:09,498 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 1 finished.\n",
      "2024-09-14 14:47:09,512 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 2 | phase: train | start time: 14:47:09 | learning rate: [0.00084375].\n",
      "2024-09-14 14:48:21,063 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 3500 | Epoch: 2 | Meta-train loss: 0.0011478669475764036 | Meta-test loss: 0.3232860565185547.\n",
      "2024-09-14 15:00:32,901 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 4000 | Epoch: 2 | Meta-train loss: 0.0014025364071130753 | Meta-test loss: 0.292306125164032.\n",
      "2024-09-14 15:12:46,344 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 4500 | Epoch: 2 | Meta-train loss: 0.0011092062341049314 | Meta-test loss: 0.2947928309440613.\n",
      "2024-09-14 15:24:55,708 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 5000 | Epoch: 2 | Meta-train loss: 0.0006043094326741993 | Meta-test loss: 0.316283643245697.\n",
      "2024-09-14 15:29:05,873 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 15:30:28,964 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 88.2563551736484 | Precision = 79.2264838225841 | Recall = 99.609375)\n",
      "2024-09-14 15:30:28,969 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 15:31:09,170 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 88.76694641704326 | Precision = 81.94792973651192 | Recall = 96.82380846490253)\n",
      "2024-09-14 15:31:09,173 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 2 finished.\n",
      "2024-09-14 15:31:09,196 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 3 | phase: train | start time: 15:31:09 | learning rate: [0.000474609375].\n",
      "2024-09-14 15:39:26,794 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 5500 | Epoch: 3 | Meta-train loss: 0.000742367934435606 | Meta-test loss: 0.25971847772598267.\n",
      "2024-09-14 15:51:21,008 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 6000 | Epoch: 3 | Meta-train loss: 0.04212849214673042 | Meta-test loss: 0.27752527594566345.\n",
      "2024-09-14 16:03:10,520 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 6500 | Epoch: 3 | Meta-train loss: 0.005164796020835638 | Meta-test loss: 0.38041597604751587.\n",
      "2024-09-14 16:12:49,042 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 16:14:10,432 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 87.88418179660613 | Precision = 78.53658536585365 | Recall = 99.75754310344827)\n",
      "2024-09-14 16:14:10,436 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 16:14:48,503 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 88.83697038988583 | Precision = 80.33504150558902 | Recall = 99.35141946482841)\n",
      "2024-09-14 16:14:48,504 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 3 finished.\n",
      "2024-09-14 16:14:48,521 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 4 | phase: train | start time: 16:14:48 | learning rate: [0.00035595703125].\n",
      "2024-09-14 16:17:07,847 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 7000 | Epoch: 4 | Meta-train loss: 0.0008642819593660533 | Meta-test loss: 0.23693543672561646.\n",
      "2024-09-14 16:29:18,591 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 7500 | Epoch: 4 | Meta-train loss: 0.0025141488295048475 | Meta-test loss: 0.3181082606315613.\n",
      "2024-09-14 16:41:22,082 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 8000 | Epoch: 4 | Meta-train loss: 0.06429988890886307 | Meta-test loss: 0.19892746210098267.\n",
      "2024-09-14 16:53:23,036 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 8500 | Epoch: 4 | Meta-train loss: 0.02840910665690899 | Meta-test loss: 0.3001430034637451.\n",
      "2024-09-14 16:56:29,986 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 16:57:52,990 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 88.03998571938594 | Precision = 78.85312300149222 | Recall = 99.64978448275862)\n",
      "2024-09-14 16:57:52,998 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 16:58:31,422 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 86.78929765886288 | Precision = 77.81795731169518 | Recall = 98.09873248832555)\n",
      "2024-09-14 16:58:31,423 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 4 finished.\n",
      "2024-09-14 16:58:31,438 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 5 | phase: train | start time: 16:58:31 | learning rate: [0.000200225830078125].\n",
      "2024-09-14 17:07:27,553 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 9000 | Epoch: 5 | Meta-train loss: 0.0013696003006771207 | Meta-test loss: 0.24116601049900055.\n",
      "2024-09-14 17:19:28,372 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 9500 | Epoch: 5 | Meta-train loss: 0.021353714168071747 | Meta-test loss: 0.25727444887161255.\n",
      "2024-09-14 17:31:19,035 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 10000 | Epoch: 5 | Meta-train loss: 0.037174150347709656 | Meta-test loss: 0.3085216283798218.\n",
      "2024-09-14 17:39:48,214 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 17:41:15,069 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 88.51007835396852 | Precision = 79.60193652501344 | Recall = 99.66325431034483)\n",
      "2024-09-14 17:41:15,077 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 17:41:57,926 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 86.40380140274364 | Precision = 77.01993095245003 | Recall = 98.39152027277444)\n",
      "2024-09-14 17:41:57,928 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 5 finished.\n",
      "2024-09-14 17:41:57,949 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 6 | phase: train | start time: 17:41:57 | learning rate: [0.00011262702941894531].\n",
      "2024-09-14 17:45:26,793 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 10500 | Epoch: 6 | Meta-train loss: 0.000987759791314602 | Meta-test loss: 0.33139175176620483.\n",
      "2024-09-14 17:57:31,216 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 11000 | Epoch: 6 | Meta-train loss: 0.000879055296536535 | Meta-test loss: 0.17516165971755981.\n",
      "2024-09-14 18:09:33,933 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 11500 | Epoch: 6 | Meta-train loss: 0.0021587528754025698 | Meta-test loss: 0.2613355815410614.\n",
      "2024-09-14 18:21:39,475 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 12000 | Epoch: 6 | Meta-train loss: 0.0016180295497179031 | Meta-test loss: 0.19906333088874817.\n",
      "2024-09-14 18:23:38,951 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 18:25:01,950 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 88.27142090513385 | Precision = 79.1911843372205 | Recall = 99.70366379310344)\n",
      "2024-09-14 18:25:01,955 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 18:25:40,387 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 86.22818879421301 | Precision = 76.93403494490799 | Recall = 98.07649544140538)\n",
      "2024-09-14 18:25:40,388 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 6 finished.\n",
      "2024-09-14 18:25:40,403 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 7 | phase: train | start time: 18:25:40 | learning rate: [6.335270404815674e-05].\n",
      "2024-09-14 18:35:49,727 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 12500 | Epoch: 7 | Meta-train loss: 0.0013759952271357179 | Meta-test loss: 0.31119483709335327.\n",
      "2024-09-14 18:47:56,445 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 13000 | Epoch: 7 | Meta-train loss: 0.0008754768641665578 | Meta-test loss: 0.23306907713413239.\n",
      "2024-09-14 19:00:00,077 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 13500 | Epoch: 7 | Meta-train loss: 0.005014500115066767 | Meta-test loss: 0.27441954612731934.\n",
      "2024-09-14 19:07:24,431 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 19:08:46,953 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 88.53948705685418 | Precision = 79.59797914651188 | Recall = 99.74407327586206)\n",
      "2024-09-14 19:08:46,961 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 19:09:25,416 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 86.13307304376119 | Precision = 76.7600324750638 | Recall = 98.11355718627233)\n",
      "2024-09-14 19:09:25,419 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 7 finished.\n",
      "2024-09-14 19:09:25,435 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 8 | phase: train | start time: 19:09:25 | learning rate: [4.7514528036117556e-05].\n",
      "2024-09-14 19:13:57,096 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 14000 | Epoch: 8 | Meta-train loss: 0.0007728002965450287 | Meta-test loss: 0.2465006560087204.\n",
      "2024-09-14 19:25:40,860 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 14500 | Epoch: 8 | Meta-train loss: 0.0010207595769315958 | Meta-test loss: 0.23407399654388428.\n",
      "2024-09-14 19:37:36,404 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 15000 | Epoch: 8 | Meta-train loss: 0.029308466240763664 | Meta-test loss: 0.22346080839633942.\n",
      "2024-09-14 19:49:33,880 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 15500 | Epoch: 8 | Meta-train loss: 0.0015968611696735024 | Meta-test loss: 0.2322230488061905.\n",
      "2024-09-14 19:50:22,023 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 19:51:45,625 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 88.03469581749049 | Precision = 78.75212585034014 | Recall = 99.79795258620689)\n",
      "2024-09-14 19:51:45,629 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 19:52:23,724 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 86.15949564539191 | Precision = 76.7114171633586 | Recall = 98.26180416574012)\n",
      "2024-09-14 19:52:23,726 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 8 finished.\n",
      "2024-09-14 19:52:23,745 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Starting epoch: 9 | phase: train | start time: 19:52:23 | learning rate: [2.6726922020316126e-05].\n",
      "2024-09-14 20:03:33,830 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 16000 | Epoch: 9 | Meta-train loss: 0.0012454497627913952 | Meta-test loss: 0.31363657116889954.\n",
      "2024-09-14 20:15:33,147 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 16500 | Epoch: 9 | Meta-train loss: 0.0012484891340136528 | Meta-test loss: 0.31766277551651.\n",
      "2024-09-14 20:27:35,519 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Step: 17000 | Epoch: 9 | Meta-train loss: 0.02041003666818142 | Meta-test loss: 0.2611187994480133.\n",
      "2024-09-14 20:33:55,346 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Train by threshold 0.5\n",
      "2024-09-14 20:35:19,853 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Train: F1 score = 88.24685768749629 | Precision = 79.10925985261134 | Recall = 99.77101293103448)\n",
      "2024-09-14 20:35:19,858 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating Test by threshold 0.5\n",
      "2024-09-14 20:35:58,692 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Test: F1 score = 86.23739837398374 | Precision = 76.82368619271105 | Recall = 98.2803350381736)\n",
      "2024-09-14 20:35:58,694 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Training epoch 9 finished.\n"
     ]
    }
   ],
   "source": [
    "# Meta-learning\n",
    "log = \"layer={}_hidden={}_epoch={}\".format(num_layer, lstm_hiddens, epochs)\n",
    "best_model_file = os.path.join(output_model_dir, log + \"_best.pt\")\n",
    "last_model_file = os.path.join(output_model_dir, log + \"_last.pt\")\n",
    "\n",
    "if not os.path.exists(output_model_dir):\n",
    "    os.makedirs(output_model_dir)\n",
    "if mode == \"train\":\n",
    "    # Train\n",
    "    optimizer = Optimizer(\n",
    "        filter(lambda p: p.requires_grad, metalog.model.parameters()), lr=gamma\n",
    "    )\n",
    "    global_step = 0\n",
    "    best_f1_score = 0\n",
    "    for epoch in range(epochs):\n",
    "        metalog.model.train()\n",
    "        metalog.bk_model.train()\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        logger.info(\n",
    "            f\"Starting epoch: {epoch} | phase: train | start time: {start} | learning rate: {optimizer.lr}.\"\n",
    "        )\n",
    "\n",
    "        batch_num = int(np.ceil(len(labeled_train_HDFS) / float(batch_size)))\n",
    "        batch_iter = 0\n",
    "        batch_num_test = int(np.ceil(len(labeled_train_BGL) / float(batch_size)))\n",
    "        batch_iter_test = 0\n",
    "        total_bn = max(batch_num, batch_num_test)\n",
    "        meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "        meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "\n",
    "        for i in range(total_bn):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Meta-train on HDFS and BGL\n",
    "            meta_train_batch = meta_train_loader.__next__()\n",
    "            meta_test_batch = meta_test_loader.__next__()\n",
    "            tinst_tr = generate_tinsts_binary_label(meta_train_batch, vocab_HDFS)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_tr.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_tr.to_mps(DEVICE)\n",
    "            loss = metalog.forward(tinst_tr.inputs, tinst_tr.targets)\n",
    "            loss_value = loss.data.cpu().numpy()\n",
    "            loss.backward(retain_graph=True)\n",
    "            batch_iter += 1\n",
    "            if torch.cuda.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .cuda()\n",
    "                )\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .to(DEVICE)\n",
    "                )\n",
    "            else:\n",
    "                metalog.bk_model = get_updated_network(\n",
    "                    metalog.model, metalog.bk_model, alpha\n",
    "                ).train()\n",
    "            \n",
    "            # Meta-test on BGL\n",
    "            tinst_test = generate_tinsts_binary_label(meta_test_batch, vocab_BGL)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_test.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_test.to_mps(DEVICE)\n",
    "            loss_te = beta * metalog.bk_forward(\n",
    "                tinst_test.inputs, tinst_test.targets\n",
    "            )\n",
    "            loss_value_te = loss_te.data.cpu().numpy() / beta\n",
    "            loss_te.backward()\n",
    "            batch_iter_test += 1\n",
    "            \n",
    "            # Aggregate the gradients and update the model.\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            if global_step % 500 == 0:\n",
    "                logger.info(\n",
    "                    f\"Step: {global_step} | Epoch: {epoch} | Meta-train loss: {loss_value} | Meta-test loss: {loss_value_te}.\"\n",
    "                )\n",
    "            if batch_iter == batch_num:\n",
    "                meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "                batch_iter = 0\n",
    "            if batch_iter_test == batch_num_test:\n",
    "                meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "                batch_iter_test = 0\n",
    "\n",
    "        if train_BGL:\n",
    "            metalog.evaluate(\"Train\", train_BGL + train_HDFS)\n",
    "            \n",
    "        if test_BGL:\n",
    "            _, _, f1_score = metalog.evaluate(\"Test\", test_BGL)\n",
    "\n",
    "            if f1_score > best_f1_score:\n",
    "                logger.info(\n",
    "                    f\"Exceed best F1 score: history = {best_f1_score}, current = {f1_score}.\"\n",
    "                )\n",
    "                torch.save(metalog.model.state_dict(), best_model_file)\n",
    "                best_f1_score = f1_score\n",
    "\n",
    "        logger.info(f\"Training epoch {epoch} finished.\")\n",
    "        torch.save(metalog.model.state_dict(), last_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 20:36:38,215 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: === Final Model ===\n",
      "/var/folders/45/0lfy6jf14g3cptljv3x_x53r0000gn/T/ipykernel_27776/2143043944.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metalog.model.load_state_dict(torch.load(last_model_file))\n",
      "2024-09-14 20:36:38,256 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: Start evaluating last model on Test BGL by threshold 0.5\n",
      "2024-09-14 20:37:16,608 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: last model on Test BGL: F1 score = 86.23739837398374 | Precision = 76.82368619271105 | Recall = 98.2803350381736)\n",
      "2024-09-14 20:37:16,609 - MetaLog - SESSION_e1dd795244c3980b7a9b81fcf515ade8 - INFO: === Best Model ===\n",
      "/var/folders/45/0lfy6jf14g3cptljv3x_x53r0000gn/T/ipykernel_27776/2143043944.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  metalog.model.load_state_dict(torch.load(best_model_file))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AttGRUModel:\n\tsize mismatch for word_embed.weight: copying a param with shape torch.Size([482, 200]) from checkpoint, the shape in current model is torch.Size([482, 300]).\n\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([192, 200]) from checkpoint, the shape in current model is torch.Size([192, 300]).\n\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([192, 200]) from checkpoint, the shape in current model is torch.Size([192, 300]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(best_model_file):\n\u001b[1;32m      6\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Best Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mmetalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     metalog\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest model on Test BGL\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_BGL)\n\u001b[1;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Finished!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/research/MetaLog/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AttGRUModel:\n\tsize mismatch for word_embed.weight: copying a param with shape torch.Size([482, 200]) from checkpoint, the shape in current model is torch.Size([482, 300]).\n\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([192, 200]) from checkpoint, the shape in current model is torch.Size([192, 300]).\n\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([192, 200]) from checkpoint, the shape in current model is torch.Size([192, 300])."
     ]
    }
   ],
   "source": [
    "if os.path.exists(last_model_file):\n",
    "    logger.info(\"=== Final Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(last_model_file))\n",
    "    metalog.evaluate(\"Last model on Test BGL\", test_BGL)\n",
    "if os.path.exists(best_model_file):\n",
    "    logger.info(\"=== Best Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(best_model_file))\n",
    "    metalog.evaluate(\"Best model on Test BGL\", test_BGL)\n",
    "logger.info(\"All Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
