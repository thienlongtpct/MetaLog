{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:39:19,300 - AttGRU - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct logger for Attention-Based GRU succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:39:19,358 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct logger for MetaLog succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:39:19,359 - StatisticsRepresentation. - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct logger for Statistics Representation succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:39:19,360 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct logger for Statistics Template Encoder succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:39:19,361 - Vocab - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct logger for Vocab succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "# External libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import FastICA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project specific imports\n",
    "from CONSTANTS import DEVICE, LOG_ROOT, PROJECT_ROOT, SESSION\n",
    "from models.gru import AttGRUModel\n",
    "from module.Common import batch_variable_inst, data_iter, generate_tinsts_binary_label\n",
    "from module.Optimizer import Optimizer\n",
    "from preprocessing.AutoLabeling import Probabilistic_Labeling\n",
    "from preprocessing.datacutter.SimpleCutting import cut_by\n",
    "from preprocessing.Preprocess import Preprocessor\n",
    "from representations.sequences.statistics import Sequential_TF\n",
    "from representations.templates.statistics import (\n",
    "    Simple_template_TF_IDF,\n",
    "    Template_TF_IDF_without_clean,\n",
    ")\n",
    "from utils.Vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom default params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the word2vec file and its dimensions\n",
    "word2vec_file = \"glove.42B.300d.txt\"\n",
    "dim = 300\n",
    "\n",
    "# Set the hyperparameters\n",
    "alpha = 0.002  # Learning rate for meta-train\n",
    "beta = 1       # Scaling factor for meta-test loss\n",
    "gamma = 0.002  # Learning rate for optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Network model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM hidden units\n",
    "lstm_hiddens = 64\n",
    "\n",
    "# Number of layers in the network\n",
    "num_layer = 4\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 100\n",
    "\n",
    "# Dropout rate\n",
    "drop_out = 0.2\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# Threshold for prediction\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Dataset params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parser type\n",
    "parser = \"IBM\"\n",
    "\n",
    "# Set the mode to 'train'\n",
    "mode = \"train\"\n",
    "\n",
    "# Parameters for clustering\n",
    "min_cluster_size = 100\n",
    "min_samples = 100\n",
    "\n",
    "# Dimension reduction target\n",
    "reduce_dimensions = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(dataset, parser, min_cluster_size, min_samples, project_root):\n",
    "    \"\"\"Generate file paths for probabilistic labeling, random state, model, and result directories.\"\"\"\n",
    "    save_dir = os.path.join(project_root, \"outputs\")\n",
    "    \n",
    "    prob_label_res_file = os.path.join(\n",
    "        save_dir,\n",
    "        f\"results/MetaLog/{dataset}_{parser}/prob_label_res/mcs-{min_cluster_size}_ms-{min_samples}\"\n",
    "    )\n",
    "    rand_state_file = os.path.join(\n",
    "        save_dir,\n",
    "        f\"results/MetaLog/{dataset}_{parser}/prob_label_res/random_state\"\n",
    "    )\n",
    "    \n",
    "    return prob_label_res_file, rand_state_file\n",
    "\n",
    "\n",
    "# Generate file paths for HDFS (source dataset)\n",
    "prob_label_res_file_HDFS, rand_state_file_HDFS = get_file_paths(\"HDFS\", parser, min_cluster_size, min_samples, PROJECT_ROOT)\n",
    "\n",
    "# Generate file paths for BGL (target dataset)\n",
    "prob_label_res_file_BGL, rand_state_file_BGL = get_file_paths(\"BGL\", parser, min_cluster_size, min_samples, PROJECT_ROOT)\n",
    "\n",
    "# Shared model and result directories\n",
    "output_model_dir = os.path.join(PROJECT_ROOT, f\"outputs/models/MetaLog/{parser}/model\")\n",
    "output_res_dir = os.path.join(PROJECT_ROOT, f\"outputs/results/MetaLog/{parser}/detect_res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Function for updating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_network(old, new, lr, load=False):\n",
    "    updated_theta = {}\n",
    "    state_dicts = old.state_dict()\n",
    "    param_dicts = dict(old.named_parameters())\n",
    "\n",
    "    for i, (k, v) in enumerate(state_dicts.items()):\n",
    "        if k in param_dicts.keys() and param_dicts[k].grad is not None:\n",
    "            updated_theta[k] = param_dicts[k] - lr * param_dicts[k].grad\n",
    "        else:\n",
    "            updated_theta[k] = state_dicts[k]\n",
    "    if load:\n",
    "        new.load_state_dict(updated_theta)\n",
    "    else:\n",
    "        new = put_theta(new, updated_theta)\n",
    "    return new\n",
    "\n",
    "\n",
    "def put_theta(model, theta):\n",
    "    def k_param_fn(tmp_model, name=None):\n",
    "        if len(tmp_model._modules) != 0:\n",
    "            for k, v in tmp_model._modules.items():\n",
    "                if name is None:\n",
    "                    k_param_fn(v, name=str(k))\n",
    "                else:\n",
    "                    k_param_fn(v, name=str(name + \".\" + k))\n",
    "        else:\n",
    "            for k, v in tmp_model._parameters.items():\n",
    "                if not isinstance(v, torch.Tensor):\n",
    "                    continue\n",
    "                tmp_model._parameters[k] = theta[str(name + \".\" + k)]\n",
    "\n",
    "    k_param_fn(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logging config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:39:31,840 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Logger for MetaLog constructed successfully. Current working directory: /Users/minhthienlongvo/research/MetaLog. Logs will be written in /Users/minhthienlongvo/research/MetaLog/logs.\n"
     ]
    }
   ],
   "source": [
    "# Log setup\n",
    "logger = logging.getLogger(\"MetaLog\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Formatter\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s - %(name)s - \" + SESSION + \" - %(levelname)s: %(message)s\"\n",
    ")\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler(sys.stderr)\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# File handler\n",
    "file_handler = logging.FileHandler(os.path.join(LOG_ROOT, \"MetaLog.log\"))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Successfully constructed\n",
    "logger.info(\n",
    "    f\"Logger for MetaLog constructed successfully. Current working directory: {os.getcwd()}. Logs will be written in {LOG_ROOT}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Log custom params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:39:31,846 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Network parameters:\n",
      "2024-09-16 16:39:31,846 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO:   - LSTM hidden units: 64\n",
      "2024-09-16 16:39:31,847 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO:   - Number of layers: 4\n",
      "2024-09-16 16:39:31,848 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO:   - Dropout rate: 0.2\n",
      "2024-09-16 16:39:31,848 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Hyperparameters:\n",
      "2024-09-16 16:39:31,849 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO:   - Alpha: 0.002\n",
      "2024-09-16 16:39:31,850 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO:   - Beta: 1\n",
      "2024-09-16 16:39:31,850 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO:   - Gamma: 0.002\n",
      "2024-09-16 16:39:31,851 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO:   - Word2Vec file: glove.42B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Network parameters:\")\n",
    "logger.info(f\"  - LSTM hidden units: {lstm_hiddens}\")\n",
    "logger.info(f\"  - Number of layers: {num_layer}\")\n",
    "logger.info(f\"  - Dropout rate: {drop_out}\")\n",
    "\n",
    "logger.info(f\"Hyperparameters:\")\n",
    "logger.info(f\"  - Alpha: {alpha}\")\n",
    "logger.info(f\"  - Beta: {beta}\")\n",
    "logger.info(f\"  - Gamma: {gamma}\")\n",
    "logger.info(f\"  - Word2Vec file: {word2vec_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, parser, cut_func, template_encoder, word2vec_file):\n",
    "    \"\"\"Preprocess the data and return train, validation, and test sets.\"\"\"\n",
    "    template_encoder = (\n",
    "        Template_TF_IDF_without_clean(word2vec_file)\n",
    "        if dataset == \"NC\"\n",
    "        else Simple_template_TF_IDF(word2vec_file)\n",
    "    )\n",
    "    processor = Preprocessor()\n",
    "    train_data, valid_data, test_data = processor.process(\n",
    "        dataset=dataset,\n",
    "        parsing=parser,\n",
    "        cut_func=cut_func,\n",
    "        template_encoding=template_encoder.present,\n",
    "    )\n",
    "    return train_data, valid_data, test_data, processor\n",
    "\n",
    "\n",
    "def encode_log_sequences(processor, train_data, test_data=None):\n",
    "    \"\"\"Encode log sequences using a sequential encoder and return updated instances.\"\"\"\n",
    "    sequential_encoder = Sequential_TF(processor.embedding)\n",
    "    \n",
    "    train_reprs = sequential_encoder.present(train_data)\n",
    "    for index, inst in enumerate(train_data):\n",
    "        inst.repr = train_reprs[index]\n",
    "    \n",
    "    if (test_data is not None):\n",
    "        test_reprs = sequential_encoder.present(test_data)\n",
    "        for index, inst in enumerate(test_data):\n",
    "            inst.repr = test_reprs[index]\n",
    "            \n",
    "        return train_data, test_data\n",
    "    \n",
    "    return train_data, None\n",
    "\n",
    "\n",
    "def reduce_dimension(train_data, reduce_dimensions):\n",
    "    \"\"\"Perform dimension reduction using FastICA and update the train data.\"\"\"\n",
    "    if reduce_dimensions != -1:\n",
    "        start_time = time.time()\n",
    "        print(f\"Start FastICA, target dimension: {reduce_dimensions}.\")\n",
    "        transformer = FastICA(n_components=reduce_dimensions)\n",
    "        train_reprs = np.array([inst.repr for inst in train_data])\n",
    "        train_reprs_transform = transformer.fit_transform(train_reprs)\n",
    "        for idx, inst in enumerate(train_data):\n",
    "            inst.repr = train_reprs_transform[idx]\n",
    "        print(f\"Finished at {round(time.time() - start_time, 2)}.\")\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def probabilistic_labeling(train_data, min_samples, min_cluster_size, prob_label_res_file, rand_state_file):\n",
    "    \"\"\"Perform probabilistic labeling and return labeled training data.\"\"\"\n",
    "    train_normal = [x for x, inst in enumerate(train_data) if inst.label == \"Normal\"]\n",
    "    normal_ids = train_normal[: int(0.5 * len(train_normal))]\n",
    "    \n",
    "    label_generator = Probabilistic_Labeling(\n",
    "        min_samples=min_samples,\n",
    "        min_clust_size=min_cluster_size,\n",
    "        res_file=prob_label_res_file,\n",
    "        rand_state_file=rand_state_file,\n",
    "    )\n",
    "    labeled_train_data = label_generator.auto_label(train_data, normal_ids)\n",
    "    return labeled_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Import BGL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:39:34,471 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loading word2vec dict from glove.42B.300d.txt.\n",
      "100%|██████████| 1917494/1917494 [00:56<00:00, 33885.11it/s]\n",
      "2024-09-16 16:40:37,238 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Total 1917494 words in glove.42B.300d.txt dict.\n",
      "2024-09-16 16:40:37,264 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:40:37,266 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/BGL/raw_log_seqs.txt\n",
      "100%|██████████| 85577/85577 [00:00<00:00, 95050.90it/s] \n",
      "2024-09-16 16:40:38,301 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Extraction finished successfully.\n",
      "2024-09-16 16:40:38,302 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:40:38,303 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/BGL.ini\n",
      "2024-09-16 16:40:38,305 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/persistence\n",
      "2024-09-16 16:40:38,319 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Persistence file found, loading.\n",
      "2024-09-16 16:40:38,343 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loaded.\n",
      "2024-09-16 16:40:38,344 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_event_mapping.dict ... True\n",
      "2024-09-16 16:40:38,344 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/log_sequences.txt ... True\n",
      "2024-09-16 16:40:38,344 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start loading previous parsing results.\n",
      "2024-09-16 16:40:39,977 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loaded 4747963 log sequences and their mappings.\n",
      "2024-09-16 16:40:40,522 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loaded 85577 blocks\n",
      "2024-09-16 16:40:40,522 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Finished in 2.18\n",
      "2024-09-16 16:40:40,557 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Found 1121 tokens in 432 log templates\n",
      "2024-09-16 16:40:40,622 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: OOV Rate: 0.09636724681696064\n",
      "2024-09-16 16:40:40,674 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/BGL/persistences/ibm_drain_depth-3_st-0.1/templates.vec\n",
      "2024-09-16 16:40:40,675 - BGLLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: All data preparation finished in 2.33\n",
      "2024-09-16 16:40:40,675 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start preprocessing dataset BGL by parsing method IBM\n",
      "2024-09-16 16:40:40,676 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start generating instances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * ciod * * * * * * * * * * * * * * * * * * * * * * ciodb ciod * * * * * * * * ciod * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * fefff * fefff * * * * * * * * * * * * * ciod * * * * * ciod * * * * * * * * * ciod * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * regctl miscompare * * * * sernum * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * mpgood * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * bglmaster * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ciod * * * * * * * * * ciod * * * * * * * * * * * * * * * * sernum * * ciod * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * mpgood mpgood * * * * * * * * * * * mpgood mpgood ciod * * * * * * * * * * * * * * * * * * * * * * ciod * softheader * * ciod * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ciod * * * * * * * * * * * * * * fabd abdd fabdd fabda feaa * * feaa * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * mpgood mpgood * * * * * * * * * * * * * * * idoproxydb * * * * microloader ffffc * * idoproxydb idomarshalerio *& idoproxydb idomarshalerio * * * * * * * * * * * * * * * * x+ y+ z+ * * * * x+ y+ z+ * * * * * * * * x+ y+ z+ * * * * * * * * * * * * * * * * * * * x+ y+ z+ * * * * * * * x+ y+ z+ x+ softheader * pixf * * * * * softheader * * pixf softheader * * pixf * * * * * feaa feaa feaa * * feaa feaa cdadc cdadc baae * * * * * * * * * mpgood mpgood mpgood mpgood * feaa feaa * * * * `personality >version bglpersonality version' `void efcd efcd * * * * * softheader * prxf pixf * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * fpscr dbcr * * * * * * * * * * * * * ciod * * * * * * * * * * * * * bgldevices bgldevices \u0000 * ciod * * * * ciod * softheader * * ciod `vaddr `int "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85577/85577 [00:00<00:00, 208336.18it/s]\n",
      "2024-09-16 16:40:41,109 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Train: 16352 Normal, 110 Anomalous instances.\n",
      "2024-09-16 16:40:41,110 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Test: 32922 Normal, 26982 Anomalous instances.\n",
      "2024-09-16 16:40:41,447 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-16 16:40:41,526 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Embed size: 120 in pre dataset.\n",
      "2024-09-16 16:40:41,527 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-16 16:40:41,660 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Embed size: 401 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:40:46,480 - Solitary_HDBSCAN - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct logger for Solitary_HDBSCAN succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:40:46,482 - Prob_Label - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct logger for Probabilistic labeling succeeded, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:40:46,489 - Solitary_HDBSCAN - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start training model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 1.07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/hdbscan/hdbscan_.py:1451: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  self._outlier_scores = outlier_scores(self._condensed_tree)\n",
      "2024-09-16 16:40:50,947 - Solitary_HDBSCAN - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Get Total 37 clusters in 4.46s\n",
      "2024-09-16 16:40:50,947 - Solitary_HDBSCAN - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Summarizing labeled normals and their reprs.\n",
      "2024-09-16 16:40:50,949 - Solitary_HDBSCAN - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Normal clusters are: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35}\n",
      "2024-09-16 16:40:50,951 - Solitary_HDBSCAN - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Shape of normal matrix: 8176 x 50\n",
      "2024-09-16 16:40:51,765 - Solitary_HDBSCAN - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Found 5864 normal, 0 anomalous by normal clusters\n",
      "2024-09-16 16:40:51,765 - Solitary_HDBSCAN - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Found 702 normal, 1720 anomalous by minimum distances\n",
      "2024-09-16 16:40:52,374 - Prob_Label - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: TP: 110. TN: 6566, FP: 1610. FN: 0\n",
      "2024-09-16 16:40:52,374 - Prob_Label - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Training set precision: 0.0640, recallL: 1.0000, F1: 0.1202.\n"
     ]
    }
   ],
   "source": [
    "# Define the anomaly rate for BGL dataset\n",
    "BGL_ANOMALY_RATE = 0.01  # 1% anomaly in BGL train dataset\n",
    "\n",
    "# Preprocess BGL data with the specified anomaly rate in the train set\n",
    "cut_func = cut_by(0.3, 0.0, BGL_ANOMALY_RATE)\n",
    "train_BGL, _, test_BGL, processor_BGL = preprocess_data(\"BGL\", parser, cut_func, None, word2vec_file)\n",
    "\n",
    "# Encode log sequences for the train and test sets\n",
    "encoded_train_BGL, encoded_test_BGL = encode_log_sequences(processor_BGL, train_BGL, test_BGL)\n",
    "\n",
    "# Reduce dimensions of the encoded train set if necessary\n",
    "reduced_train_BGL = reduce_dimension(encoded_train_BGL, reduce_dimensions)\n",
    "\n",
    "# Perform probabilistic labeling on the reduced train set\n",
    "labeled_train_BGL = probabilistic_labeling(reduced_train_BGL, min_samples, min_cluster_size, prob_label_res_file_BGL, rand_state_file_BGL)\n",
    "\n",
    "# Assign the final train and test sets\n",
    "final_train_BGL = reduced_train_BGL\n",
    "final_test_BGL = encoded_test_BGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Import HDFS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:40:52,402 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loading word2vec dict from glove.42B.300d.txt.\n",
      "100%|██████████| 1917494/1917494 [00:54<00:00, 35347.49it/s]\n",
      "2024-09-16 16:41:51,831 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Total 1917494 words in glove.42B.300d.txt dict.\n",
      "2024-09-16 16:41:51,842 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct self.logger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:41:51,848 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start load from previous extraction. File path /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/raw_log_seqs.txt\n",
      "100%|██████████| 575061/575061 [00:02<00:00, 254617.72it/s]\n",
      "2024-09-16 16:41:54,424 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Extraction finished successfully.\n",
      "2024-09-16 16:41:54,650 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:41:54,650 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Construct DrainLogger success, current working directory: /Users/minhthienlongvo/research/MetaLog, logs will be written in /Users/minhthienlongvo/research/MetaLog/logs\n",
      "2024-09-16 16:41:54,652 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-16 16:41:54,652 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Load Drain configuration from /Users/minhthienlongvo/research/MetaLog/conf/HDFS.ini\n",
      "2024-09-16 16:41:54,654 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-16 16:41:54,654 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Searching for target persistence file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/persistence\n",
      "2024-09-16 16:41:54,659 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Persistence file found, loading.\n",
      "2024-09-16 16:41:54,659 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Persistence file found, loading.\n",
      "2024-09-16 16:41:54,664 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loaded.\n",
      "2024-09-16 16:41:54,664 - drain - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loaded.\n",
      "2024-09-16 16:41:54,665 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_event_mapping.dict ... True\n",
      "2024-09-16 16:41:54,665 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: checking file /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/log_sequences.txt ... True\n",
      "2024-09-16 16:41:54,665 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start loading previous parsing results.\n",
      "2024-09-16 16:41:58,489 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loaded 11175629 log sequences and their mappings.\n",
      "2024-09-16 16:41:59,889 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Loaded 575061 blocks\n",
      "2024-09-16 16:41:59,889 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Finished in 5.22\n",
      "2024-09-16 16:41:59,891 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Found 125 tokens in 46 log templates\n",
      "2024-09-16 16:41:59,901 - Statistics_Template_Encoder - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: OOV Rate: 0.09614966282358059\n",
      "2024-09-16 16:41:59,908 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Finish calculating semantic representations, please found the vector file at /Users/minhthienlongvo/research/MetaLog/datasets/HDFS/persistences/ibm_drain_depth-4_st-0.5/templates.vec\n",
      "2024-09-16 16:41:59,908 - HDFSLoader - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: All data preparation finished in 5.24\n",
      "2024-09-16 16:41:59,908 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start preprocessing dataset HDFS by parsing method IBM\n",
      "2024-09-16 16:41:59,909 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Start generating instances.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ipandport block* * * block* ipandport * ipandport ipandport ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport block* ipandport ipandport ipandport ipandport * * * block* ipandport * ipandport * ipandport * ipandport ipandport ipandport * ipandport * ipandport ipandport block* ipandport block* ipandport ipandport * * * * ipandport ipandport * ipandport ipandport block* * * "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575061/575061 [00:02<00:00, 251755.35it/s]\n",
      "2024-09-16 16:42:02,345 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Train: 165180 Normal, 7338 Anomalous instances.\n",
      "2024-09-16 16:42:02,345 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Test: 393043 Normal, 9500 Anomalous instances.\n",
      "2024-09-16 16:42:03,575 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Update train instances' event-idx mapping.\n",
      "2024-09-16 16:42:03,914 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Embed size: 41 in pre dataset.\n",
      "2024-09-16 16:42:03,915 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Update test instances' event-idx mapping.\n",
      "2024-09-16 16:42:04,314 - Preprocessor - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Embed size: 46 in pre+post dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start FastICA, target dimension: 50.\n",
      "Finished at 18.38.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the anomaly rate for HDFS dataset\n",
    "HDFS_ANOMALY_RATE = 1.0  # 100% anomaly in HDFS train set\n",
    "\n",
    "# Preprocess HDFS data with the specified anomaly rate in the train set\n",
    "cut_func = cut_by(0.3, 0.0, HDFS_ANOMALY_RATE)\n",
    "train_HDFS, _, _, processor_HDFS = preprocess_data(\"HDFS\", parser, cut_func, None, word2vec_file)\n",
    "\n",
    "# Encode log sequences for the train set\n",
    "encoded_train_HDFS, _ = encode_log_sequences(processor_HDFS, train_HDFS)\n",
    "\n",
    "# Reduce dimensions of the encoded train set if necessary\n",
    "reduced_train_HDFS = reduce_dimension(encoded_train_HDFS, reduce_dimensions)\n",
    "\n",
    "# No probabilistic labeling for HDFS, use the reduced train set directly\n",
    "labeled_train_HDFS = reduced_train_HDFS\n",
    "\n",
    "# Assign the final train set for HDFS\n",
    "final_train_HDFS = reduced_train_HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Aggregate vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:42:27,319 - Vocab - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Total words: 436\n",
      "2024-09-16 16:42:27,320 - Vocab - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-16 16:42:27,322 - Vocab - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Total words: 50\n",
      "2024-09-16 16:42:27,322 - Vocab - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: The dim of pretrained embeddings: 300\n",
      "2024-09-16 16:42:27,322 - Vocab - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Total words: 482\n",
      "2024-09-16 16:42:27,323 - Vocab - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: The dim of pretrained embeddings: 300\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(processor):\n",
    "    \"\"\"Load embeddings from the given processor and return a Vocab object.\"\"\"\n",
    "    vocab = Vocab()\n",
    "    vocab.load_from_dict(processor.embedding)\n",
    "    return vocab\n",
    "\n",
    "def merge_embeddings(processor_BGL, processor_HDFS):\n",
    "    \"\"\"Merge embeddings from BGL and HDFS processors, handling padding.\"\"\"\n",
    "    new_embedding = processor_BGL.embedding.copy()\n",
    "    padding_offset = len(processor_BGL.embedding)  # Use the length of BGL embedding as padding offset\n",
    "    \n",
    "    for key, value in processor_HDFS.embedding.items():\n",
    "        new_key = key + padding_offset  # Append padding offset to key to avoid conflicts\n",
    "        new_embedding[new_key] = value\n",
    "    \n",
    "    return new_embedding\n",
    "\n",
    "# Load embeddings for BGL and HDFS\n",
    "vocab_BGL = load_embeddings(processor_BGL)\n",
    "vocab_HDFS = load_embeddings(processor_HDFS)\n",
    "\n",
    "# Merge embeddings from BGL and HDFS\n",
    "merged_embedding = merge_embeddings(processor_BGL, processor_HDFS)\n",
    "\n",
    "# Load merged embeddings into a new vocab object\n",
    "vocab = Vocab()\n",
    "vocab.load_from_dict(merged_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. MetaLog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:42:27,346 - AttGRU - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: ==== Model Parameters ====\n",
      "2024-09-16 16:42:27,347 - AttGRU - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Input Dimension: 300\n",
      "2024-09-16 16:42:27,347 - AttGRU - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Hidden Size: 64\n",
      "2024-09-16 16:42:27,347 - AttGRU - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Num Layers: 4\n",
      "2024-09-16 16:42:27,348 - AttGRU - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Dropout: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 128\n",
      "Orthogonal pretrainer loss: 2.99e-03\n",
      "2 128\n",
      "Orthogonal pretrainer loss: 2.13e+01\n"
     ]
    }
   ],
   "source": [
    "class MetaLog:\n",
    "    def __init__(self, vocab, num_layer, hidden_size, drop_out, label2id):\n",
    "        # Initialize MetaLog with given parameters\n",
    "        self.label2id = label2id\n",
    "        self.vocab = vocab\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1024\n",
    "        self.drop_out = drop_out\n",
    "        \n",
    "        # Initialize the main and backup models\n",
    "        self.model = AttGRUModel(vocab, num_layer, hidden_size, drop_out)\n",
    "        self.bk_model = AttGRUModel(vocab, num_layer, hidden_size, drop_out, is_backup=True)\n",
    "        \n",
    "        # Move models to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.cuda(DEVICE)\n",
    "            self.bk_model = self.bk_model.cuda(DEVICE)\n",
    "        elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "            self.model = self.model.to(DEVICE)\n",
    "            self.bk_model = self.bk_model.to(DEVICE)\n",
    "        \n",
    "        # Define the loss function\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Forward pass through the main model\n",
    "        tag_logits = self.model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def bk_forward(self, inputs, targets):\n",
    "        # Forward pass through the backup model\n",
    "        tag_logits = self.bk_model(inputs)\n",
    "        tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        loss = self.loss(tag_logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, threshold=None):\n",
    "        # Predict tags for given inputs\n",
    "        with torch.no_grad():\n",
    "            tag_logits = self.model(inputs)\n",
    "            tag_logits = F.softmax(tag_logits, dim=1)\n",
    "        \n",
    "        if threshold is not None:\n",
    "            # Apply threshold to determine anomalous tags\n",
    "            probs = tag_logits.detach().cpu().numpy()\n",
    "            anomaly_id = self.label2id[\"Anomalous\"]\n",
    "            pred_tags = np.zeros(probs.shape[0])\n",
    "            \n",
    "            for i, logits in enumerate(probs):\n",
    "                if logits[anomaly_id] >= threshold:\n",
    "                    pred_tags[i] = anomaly_id\n",
    "                else:\n",
    "                    pred_tags[i] = 1 - anomaly_id\n",
    "        else:\n",
    "            # Use max value to determine tags\n",
    "            pred_tags = tag_logits.detach().max(1)[1].cpu()\n",
    "        \n",
    "        return pred_tags, tag_logits\n",
    "\n",
    "    def evaluate(self, dataset, instances, threshold=0.5):\n",
    "        # Evaluate the model on the given dataset\n",
    "        logger.info(f\"Start evaluating {dataset} by threshold {threshold}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            globalBatchNum = 0\n",
    "            TP, TN, FP, FN = 0, 0, 0, 0\n",
    "            tag_correct, tag_total = 0, 0\n",
    "            \n",
    "            for onebatch in data_iter(instances, self.test_batch_size, False):\n",
    "                tinst = generate_tinsts_binary_label(onebatch, vocab_BGL, False)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    tinst.to_cuda(DEVICE)\n",
    "                elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                    tinst.to_mps(DEVICE)\n",
    "                \n",
    "                pred_tags, tag_logits = self.predict(tinst.inputs, threshold)\n",
    "                \n",
    "                for inst, bmatch in batch_variable_inst(onebatch, pred_tags, tag_logits, processor_BGL.id2tag):\n",
    "                    tag_total += 1\n",
    "                    if bmatch:\n",
    "                        tag_correct += 1\n",
    "                        if inst.label == \"Normal\":\n",
    "                            TN += 1\n",
    "                        else:\n",
    "                            TP += 1\n",
    "                    else:\n",
    "                        if inst.label == \"Normal\":\n",
    "                            FP += 1\n",
    "                        else:\n",
    "                            FN += 1\n",
    "                \n",
    "                globalBatchNum += 1\n",
    "            \n",
    "            if TP + FP != 0:\n",
    "                # Calculate precision, recall, and F1 score\n",
    "                precision = 100 * TP / (TP + FP)\n",
    "                recall = 100 * TP / (TP + FN)\n",
    "                f1_score = 2 * precision * recall / (precision + recall)\n",
    "                logger.info(f\"{dataset}: F1 score = {f1_score} | Precision = {precision} | Recall = {recall}\")\n",
    "            else:\n",
    "                logger.info(f\"{dataset}: F1 score = {0} | Precision = {0} | Recall = {0}\")\n",
    "                precision, recall, f1_score = 0, 0, 0\n",
    "        \n",
    "        return precision, recall, f1_score\n",
    "\n",
    "# Instantiate the MetaLog class with given parameters\n",
    "metalog = MetaLog(\n",
    "    vocab=vocab,\n",
    "    num_layer=num_layer,\n",
    "    hidden_size=lstm_hiddens,\n",
    "    drop_out=drop_out,\n",
    "    label2id=processor_BGL.label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2. Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the model output directory if it doesn't exist.\"\"\"\n",
    "if not os.path.exists(output_model_dir):\n",
    "    os.makedirs(output_model_dir)\n",
    "\n",
    "\"\"\"Construct and return the paths for the best and last model files.\"\"\"\n",
    "info = f\"layer={num_layer}_hidden={lstm_hiddens}_dropout_{drop_out}_epoch={epochs}\"\n",
    "\n",
    "best_model_file = os.path.join(output_model_dir, info + \"_best.pt\")\n",
    "last_model_file = os.path.join(output_model_dir, info + \"_last.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 16:47:16,520 - MetaLog - SESSION_6f1dc9aad937b2c770fc83d10f79c018 - INFO: Starting epoch: 0 | phase: train | start time: 16:47:16 | learning rate: [0.002].\n",
      "/Users/minhthienlongvo/research/MetaLog/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:935: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  param_grad = param.grad\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 59\u001b[0m\n\u001b[1;32m     53\u001b[0m     metalog\u001b[38;5;241m.\u001b[39mbk_model \u001b[38;5;241m=\u001b[39m get_updated_network(\n\u001b[1;32m     54\u001b[0m         metalog\u001b[38;5;241m.\u001b[39mmodel, metalog\u001b[38;5;241m.\u001b[39mbk_model, alpha\n\u001b[1;32m     55\u001b[0m     )\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Meta-test on BGL\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# meta_test_batch = next(meta_test_loader)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m meta_test_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_test_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m tinst_test \u001b[38;5;241m=\u001b[39m generate_tinsts_binary_label(meta_test_batch, vocab_BGL)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if mode == \"train\":\n",
    "    # Initialize optimizer\n",
    "    optimizer = Optimizer(\n",
    "        filter(lambda p: p.requires_grad, metalog.model.parameters()), lr=gamma\n",
    "    )\n",
    "    global_step = 0\n",
    "    best_f1_score = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        metalog.model.train()\n",
    "        metalog.bk_model.train()\n",
    "        start_time = time.strftime(\"%H:%M:%S\")\n",
    "        logger.info(\n",
    "            f\"Starting epoch: {epoch} | phase: train | start time: {start_time} | learning rate: {optimizer.lr}.\"\n",
    "        )\n",
    "\n",
    "        batch_num_train = int(np.ceil(len(labeled_train_HDFS) / float(batch_size)))\n",
    "        batch_num_test = int(np.ceil(len(labeled_train_BGL) / float(batch_size)))\n",
    "        total_batches = max(batch_num_train, batch_num_test)\n",
    "\n",
    "        meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "        meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "\n",
    "        for _ in range(total_batches):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Meta-train on HDFS\n",
    "            # meta_train_batch = next(meta_train_loader)\n",
    "            meta_train_batch = meta_train_loader.__next__()\n",
    "            tinst_train = generate_tinsts_binary_label(meta_train_batch, vocab)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_train.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_train.to_mps(DEVICE)\n",
    "            train_loss = metalog.forward(tinst_train.inputs, tinst_train.targets)\n",
    "            train_loss_value = train_loss.data.cpu().numpy()\n",
    "            train_loss.backward(retain_graph=True)\n",
    "\n",
    "            # Update backup model\n",
    "            if torch.cuda.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .cuda()\n",
    "                )\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                metalog.bk_model = (\n",
    "                    get_updated_network(metalog.model, metalog.bk_model, alpha)\n",
    "                    .train()\n",
    "                    .to(DEVICE)\n",
    "                )\n",
    "            else:\n",
    "                metalog.bk_model = get_updated_network(\n",
    "                    metalog.model, metalog.bk_model, alpha\n",
    "                ).train()\n",
    "\n",
    "            # Meta-test on BGL\n",
    "            # meta_test_batch = next(meta_test_loader)\n",
    "            meta_test_batch = meta_test_loader.__next__()\n",
    "            tinst_test = generate_tinsts_binary_label(meta_test_batch, vocab_BGL)\n",
    "            if torch.cuda.is_available():\n",
    "                tinst_test.to_cuda(DEVICE)\n",
    "            elif hasattr(torch.mps, \"is_available\") and torch.mps.is_available():\n",
    "                tinst_test.to_mps(DEVICE)\n",
    "            test_loss = beta * metalog.bk_forward(tinst_test.inputs, tinst_test.targets)\n",
    "            test_loss_value = test_loss.data.cpu().numpy() / beta\n",
    "            test_loss.backward()\n",
    "\n",
    "            # Update the model\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 500 == 0:\n",
    "                logger.info(\n",
    "                    f\"Step: {global_step} | Epoch: {epoch} | Meta-train loss: {train_loss_value} | Meta-test loss: {test_loss_value}.\"\n",
    "                )\n",
    "\n",
    "        # Reset data loaders if necessary\n",
    "        if meta_train_loader.is_empty():\n",
    "            meta_train_loader = data_iter(labeled_train_HDFS, batch_size, True)\n",
    "        if meta_test_loader.is_empty():\n",
    "            meta_test_loader = data_iter(labeled_train_BGL, batch_size, True)\n",
    "\n",
    "        # Evaluate and save the model\n",
    "        if final_train_BGL and final_train_HDFS:\n",
    "            metalog.evaluate(\"Train\", final_train_BGL + final_train_HDFS)\n",
    "\n",
    "        if final_test_BGL:\n",
    "            _, _, f1_score = metalog.evaluate(\"Test\", final_test_BGL)\n",
    "            if f1_score > best_f1_score:\n",
    "                logger.info(\n",
    "                    f\"Exceed best F1 score: history = {best_f1_score}, current = {f1_score}.\"\n",
    "                )\n",
    "                torch.save(metalog.model.state_dict(), best_model_file)\n",
    "                best_f1_score = f1_score\n",
    "\n",
    "        logger.info(f\"Training epoch {epoch} finished.\")\n",
    "        torch.save(metalog.model.state_dict(), last_model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the last model file exists and evaluate it\n",
    "if os.path.exists(last_model_file):\n",
    "    logger.info(\"=== Evaluating Final Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(last_model_file))\n",
    "    metalog.evaluate(\"Final Model on Test BGL\", final_test_BGL)\n",
    "\n",
    "# Check if the best model file exists and evaluate it\n",
    "if os.path.exists(best_model_file):\n",
    "    logger.info(\"=== Evaluating Best Model ===\")\n",
    "    metalog.model.load_state_dict(torch.load(best_model_file))\n",
    "    metalog.evaluate(\"Best Model on Test BGL\", final_test_BGL)\n",
    "\n",
    "logger.info(\"All evaluations finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export to graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Constanst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATISTICS_TEMPLATE_LOG_PATH = \"logs/Statistics_Template.log\"\n",
    "METALOG_LOG_PATH = \"logs/MetaLog.log\"\n",
    "LOSS_EPS = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word2vec_file(log_path, session):\n",
    "    \"\"\"Extract the word2vec file path from the statistics template log.\"\"\"\n",
    "    with open(log_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            match = re.search(rf\"^.+ - Statistics_Template_Encoder - {session} - INFO: Loading word2vec dict from (.+)\\.$\", line)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "def extract_f1_scores(log_path, session):\n",
    "    \"\"\"Extract train and test F1 scores from the MetaLog.\"\"\"\n",
    "    train_f1_scores, test_f1_scores = [], []\n",
    "    with open(log_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            train_match = re.search(rf\"^.+ - MetaLog - {session} - INFO: Train: F1 score = (.+) \\|\", line)\n",
    "            test_match = re.search(rf\"^.+ - MetaLog - {session} - INFO: Test: F1 score = (.+) \\|\", line)\n",
    "\n",
    "            if train_match:\n",
    "                train_f1_scores.append(float(train_match.group(1)))\n",
    "            if test_match:\n",
    "                test_f1_scores.append(float(test_match.group(1)))\n",
    "    return train_f1_scores, test_f1_scores\n",
    "\n",
    "def extract_meta_losses(log_path, session):\n",
    "    \"\"\"Extract meta-train and meta-test losses from the MetaLog.\"\"\"\n",
    "    meta_train_losses, meta_test_losses = [], []\n",
    "    with open(log_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            match = re.search(rf\"^.* - MetaLog - {session} - INFO: Step: .+ \\| Meta-train loss: (.+) \\| Meta-test loss: (.+)\\.$\", line)\n",
    "            if match:\n",
    "                meta_train_losses.append(float(match.group(1)))\n",
    "                meta_test_losses.append(float(match.group(2)))\n",
    "\n",
    "    return meta_train_losses, meta_test_losses\n",
    "\n",
    "# Extract the word2vec file path\n",
    "word2vec_file = extract_word2vec_file(STATISTICS_TEMPLATE_LOG_PATH, SESSION)\n",
    "title = f\"BILATERAL GENERALIZATION TRANSFERRING HDFS TO BGL\\n(using {word2vec_file})\\n\"\n",
    "\n",
    "# Extract F1 scores and losses\n",
    "train_f1_scores, test_f1_scores = extract_f1_scores(METALOG_LOG_PATH, SESSION)\n",
    "meta_train_losses, meta_test_losses = extract_meta_losses(METALOG_LOG_PATH, SESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_scores(ax, num_epochs, train_f1, test_f1):\n",
    "    \"\"\"Plot train and test F1 scores on the provided axis.\"\"\"\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.plot(num_epochs, train_f1, color=\"tab:blue\")\n",
    "    ax.plot(num_epochs, test_f1, color=\"tab:orange\")\n",
    "    ax.legend([\"Train\", \"Test\"])\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"F1 Score\")\n",
    "\n",
    "    for i in range(len(num_epochs)):\n",
    "        ax.plot(num_epochs[i], train_f1[i], \"o\", color=\"tab:blue\", zorder=10)\n",
    "        ax.text(num_epochs[i], train_f1[i] + 5, round(train_f1[i], 2), ha=\"center\")\n",
    "\n",
    "        ax.plot(num_epochs[i], test_f1[i], \"o\", color=\"tab:orange\", zorder=10)\n",
    "        ax.text(num_epochs[i], test_f1[i] - 10, round(test_f1[i], 2), ha=\"center\")\n",
    "\n",
    "def plot_meta_losses(ax, num_steps, meta_train_losses, meta_test_losses):\n",
    "    \"\"\"Plot meta-train and meta-test losses on the provided axis.\"\"\"\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.plot(num_steps, meta_train_losses, color=\"tab:blue\")\n",
    "    ax.plot(num_steps, meta_test_losses, color=\"tab:orange\")\n",
    "    ax.legend([\"Meta-train loss\", \"Meta-test loss\"])\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    min_test_loss = min(meta_test_losses)\n",
    "\n",
    "    for i in range(0, len(num_steps), 2):\n",
    "        ax.plot(num_steps[i], meta_train_losses[i], \"o\", color=\"tab:blue\", zorder=10)\n",
    "        ax.text(num_steps[i], meta_train_losses[i] + LOSS_EPS, round(meta_train_losses[i], 4), ha=\"center\")\n",
    "\n",
    "        ax.plot(num_steps[i], meta_test_losses[i], \"o\", color=\"tab:orange\", zorder=10)\n",
    "        ax.text(num_steps[i], meta_test_losses[i] + LOSS_EPS, round(meta_test_losses[i], 4), color=\"red\" if meta_test_losses[i] == min_test_loss else \"black\", ha=\"center\")\n",
    "\n",
    "# Plot F1 scores and losses\n",
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "num_epochs = list(range(len(train_f1_scores)))\n",
    "num_steps = [i * 500 for i in range(len(meta_train_losses))]\n",
    "\n",
    "plot_f1_scores(axs[0], num_epochs, train_f1_scores, test_f1_scores)\n",
    "plot_meta_losses(axs[1], num_steps, meta_train_losses, meta_test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4. Saving and exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the title for the plot\n",
    "best_test_f1_score = max(test_f1_scores)\n",
    "fig_title = f\"{title}\\nBest model F1 Score = {best_test_f1_score}\"\n",
    "fig.suptitle(fig_title)\n",
    "\n",
    "# Define the path to save the plot\n",
    "plot_dir = os.path.join(\"visualization\", \"graphs\")\n",
    "plot_filename = f\"{word2vec_file}-{SESSION}.png\"\n",
    "plot_path = os.path.join(plot_dir, plot_filename)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# Save the plot\n",
    "fig.savefig(plot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
